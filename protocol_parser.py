import re
from collections.__init__ import Counter
from typing import Generator

from numpy import ma as ma

from contract_agents import find_org_names, ORG_LEVELS_re
from contract_parser import extract_all_contraints_from_sr_2, find_value_sign_currency_attention
from hyperparams import HyperParameters
from legal_docs import BasicContractDocument, deprecated, LegalDocument, tokenize_doc_into_sentences_map, ContractValue
from ml_tools import *
from parsing import ParsingContext
from patterns import *
from structures import ORG_LEVELS_names
from text_normalize import r_group, ru_cap, r_quoted
from text_tools import *
from tf_support.embedder_elmo import ElmoEmbedder

something = r'(\s*.{1,100}\s*)'
itog1 = r_group(r_group(ru_cap('–∏—Ç–æ–≥–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è') + '|' + ru_cap('—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è')) + r"[:\n]?")

za = r_group(r_quoted('–∑–∞'))
pr = r_group(r_quoted('–ø—Ä–æ—Ç–∏–≤') + something)
vo = r_group(r_quoted('–≤–æ–∑–¥–µ—Ä–∂–∞–ª—Å—è') + something)

protocol_votes_ = r_group(itog1 + something) + r_group(za + something + pr + something + vo)
protocol_votes_re = re.compile(protocol_votes_, re.IGNORECASE | re.UNICODE)


class ProtocolDocument3(LegalDocument):

  def __init__(self, doc: LegalDocument):
    super().__init__('')
    if doc is not None:
      self.__dict__ = doc.__dict__

    self.sentence_map: TextMap = None
    self.sentences_embeddings = None

    self.distances_per_sentence_pattern_dict = {}

    self.agents_tags: [SemanticTag] = []
    self.org_level: [SemanticTag] = []
    self.agenda_questions: [SemanticTag] = []
    self.margin_values: [SemanticTag] = []

  def get_tags(self) -> [SemanticTag]:
    tags = []
    tags += self.agents_tags
    tags += self.org_level
    tags += self.agenda_questions
    tags += self.margin_values

    return tags


class ProtocolParser(ParsingContext):
  patterns_dict = [
    ['sum_max1', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –±–æ–ª–µ–µ 0 –º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'],

    # ['solution_1','—Ä–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],
    # ['solution_2','–ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –ø—Ä–∏–Ω—è—Ç—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è:'],

    ['not_value_1', '—Ä–∞–∑–º–µ—Ä —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ 0 —Ä—É–±–ª–µ–π'],
    ['not_value_2', '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è'],

    ['agenda_end_1', '–∫–≤–æ—Ä—É–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∑–∞—Å–µ–¥–∞–Ω–∏—è –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏–º–µ–µ—Ç—Å—è'],
    ['agenda_end_2', '–í–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['agenda_end_3', '–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ —Ä–µ—à–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è:'],

    ['agenda_start_1', '–ø–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['agenda_start_2', '–ü–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è'],

    ['deal_approval_1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_1.1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–¥–µ–ª–∫—É'],
    ['deal_approval_2', '–¥–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞'],
    ['deal_approval_3', '–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_3.1', '–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω–æ–π —Å–¥–µ–ª–∫–∏'],
    ['deal_approval_4', '–∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã'],

    ['question_1', '–ü–æ –≤–æ–ø—Ä–æ—Å—É ‚Ññ 0'],
    ['question_2', '–ü–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],
    ['question_3', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],
    ['question_4', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ 1 –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],

    ['footers_1', '–í—Ä–µ–º—è –ø–æ–¥–≤–µ–¥–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è'],
    ['footers_2', '–°–ø–∏—Å–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:'],
    ['footers_3', '–ü–æ–¥—Å—á–µ—Ç –≥–æ–ª–æ—Å–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª –°–µ–∫—Ä–µ—Ç–∞—Ä—å –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤'],
    ['footers_4', '–ü—Ä–æ—Ç–æ–∫–æ–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ 2-—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö']

  ]

  def __init__(self, embedder, elmo_embedder_default: ElmoEmbedder):
    ParsingContext.__init__(self, embedder)
    self.elmo_embedder_default = elmo_embedder_default
    self.protocols_factory: ProtocolPatternFactory = ProtocolPatternFactory(embedder)

    patterns_te = [p[1] for p in ProtocolParser.patterns_dict]
    self.patterns_embeddings = elmo_embedder_default.embedd_strings(patterns_te)

  def ebmedd(self, doc):
    doc.sentence_map = tokenize_doc_into_sentences_map(doc, 250)

    ### ‚öôÔ∏èüîÆ SENTENCES embedding
    doc.sentences_embeddings = self.elmo_embedder_default.embedd_strings(doc.sentence_map.tokens)

    ### ‚öôÔ∏èüîÆ WORDS Ebmedding
    doc.embedd(self.protocols_factory)

    doc.calculate_distances_per_pattern(self.protocols_factory)
    doc.distances_per_sentence_pattern_dict = calc_distances_per_pattern_dict(doc.sentences_embeddings,
                                                                              self.patterns_dict,
                                                                              self.patterns_embeddings)

  def analyse(self, doc: ProtocolDocument3):
    self.ebmedd(doc)
    self._analyse_embedded(doc)

  def _analyse_embedded(self, doc: ProtocolDocument3):
    doc.org_level = list(find_org_structural_level(doc))
    doc.agents_tags = list(find_protocol_org(doc))
    doc.agenda_questions = self.find_question_decision_sections(doc)

  def collect_spans_having_votes(self, segments, textmap):
    """
    search for votes in each document segment
    collect only
    :param segments:
    :param textmap:
    :return:  segments with votes
    """
    for span in segments:
      # print('=' * 50)
      subdoc = textmap.slice(span)
      protocol_votes = list(subdoc.finditer(protocol_votes_re))
      if protocol_votes:
        # print('-' * 50)
        # print(subdoc.text)
        # for c in protocol_votes:
        #   print('found:', subdoc.text_range(c))
        yield span
      # else:
      #   print('not found')

  def find_protocol_sections_edges(self, distances_per_pattern_dict):

    patterns = ['deal_approval_', 'footers_', 'question_']
    vv_ = []
    for p in patterns:
      v_ = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, p)
      v_ = relu(v_, 0.5)
      vv_.append(v_)

    v_sections_attention = sum_probabilities(vv_)

    v_sections_attention = relu(v_sections_attention, 0.7)
    return v_sections_attention

  def _get_value_attention_vector(self, doc):
    s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')
    s_value_attention_vector_neg = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_neg')
    s_value_attention_vector -= s_value_attention_vector_neg / 3
    s_value_attention_vector = relu(s_value_attention_vector, 0.25)
    return s_value_attention_vector

  def find_question_decision_sections(self, doc: ProtocolDocument3):
    wa = doc.distances_per_pattern_dict  # words attention
    v_sections_attention = self.find_protocol_sections_edges(doc.distances_per_sentence_pattern_dict)

    # --------------
    question_spans_sent = spans_between_non_zero_attention(v_sections_attention)
    question_spans_words = doc.sentence_map.remap_slices(question_spans_sent, doc.tokens_map)
    # --------------

    # *More* attention to spans having votes
    spans_having_votes = list(self.collect_spans_having_votes(question_spans_sent, doc.sentence_map))

    spans_having_votes_words = doc.sentence_map.remap_slices(spans_having_votes, doc.tokens_map)
    # questions_attention =  spans_to_attention(question_spans_words, len(doc))
    wa['bin_votes_attention'] = spans_to_attention(spans_having_votes_words, len(doc))

    # v_deal_approval_words = sentence_map.remap_spans(v_deal_approval,  doc.tokens_map )
    v_deal_approval = max_exclusive_pattern_by_prefix(doc.distances_per_sentence_pattern_dict, 'deal_approval_')
    _spans, v_deal_approval_words_attention = sentences_attention_to_words(v_deal_approval, doc.sentence_map,
                                                                           doc.tokens_map)

    ## value attention

    wa['relu_value_attention_vector'] = self._get_value_attention_vector(doc)
    wa['relu_deal_approval'] = relu(v_deal_approval_words_attention, 0.5)

    _value_attention_vector = sum_probabilities(
      [wa['relu_value_attention_vector'],
       wa['relu_deal_approval'],
       wa['bin_votes_attention'] / 3.0])

    wa['relu_value_attention_vector'] = relu(_value_attention_vector, 0.5)
    # // words_spans_having_votes = doc.sentence_map.remap_slices(spans_having_votes, doc.tokens_map)

    values: List[ContractValue] = find_value_sign_currency_attention(doc, wa['relu_value_attention_vector'])

    numbers_attention = np.zeros(len(doc.tokens_map))
    numbers_confidence = np.zeros(len(doc.tokens_map))
    for v in values:
      numbers_confidence[v.value.as_slice()] += v.value.confidence
      numbers_attention[v.value.as_slice()] = 1
      numbers_attention[v.currency.as_slice()] = 1
      numbers_attention[v.sign.as_slice()] = 1

    block_confidence = sum_probabilities([numbers_attention, wa['relu_deal_approval'], wa['bin_votes_attention'] / 5])

    return list(find_confident_spans(question_spans_words, block_confidence, 'agenda_item'))


class ProtocolPatternFactory(AbstractPatternFactory):
  def create_pattern(self, pattern_name, ppp):
    _ppp = (ppp[0].lower(), ppp[1].lower(), ppp[2].lower())
    fp = FuzzyPattern(_ppp, pattern_name)
    self.patterns.append(fp)
    return fp

  def __init__(self, embedder):
    AbstractPatternFactory.__init__(self, embedder)

    self._build_subject_pattern()
    self._build_sum_margin_extraction_patterns()
    self.embedd()

  def _build_sum_margin_extraction_patterns(self):
    suffix = '–º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'
    prefix = ''

    sum_comp_pat = CoumpoundFuzzyPattern()

    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_1', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ –±–æ–ª–µ–µ 0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_2', (prefix + '—Ü–µ–Ω–∞', '–Ω–µ –±–æ–ª—å—à–µ 0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_3', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å <', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_4', (prefix + '—Ü–µ–Ω–∞ –º–µ–Ω–µ–µ', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_5', (prefix + '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_6', (prefix + '–æ–±—â–∞—è —Å—É–º–º–∞ –º–æ–∂–µ—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_7', (prefix + '–ª–∏–º–∏—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏—è', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_8', (prefix + '–≤–µ—Ä—Ö–Ω–∏–π –ª–∏–º–∏—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏', '0', suffix)))
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_p_9', (prefix + '–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞', '0', suffix)))

    sum_comp_pat.add_pattern(
      self.create_pattern('sum_max_neg1', ('–µ–∂–µ–º–µ—Å—è—á–Ω–æ –Ω–µ –ø–æ–∑–¥–Ω–µ–µ', '0', '—á–∏—Å–ª–∞ –∫–∞–∂–¥–æ–≥–æ –º–µ—Å—è—Ü–∞')), -0.8)
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_neg2', ('–ø—Ä–∏–Ω—è–ª–∏ —É—á–∞—Å—Ç–∏–µ –≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏', '0', '—á–µ–ª–æ–≤–µ–∫')),
                             -0.8)
    sum_comp_pat.add_pattern(
      self.create_pattern('sum_max_neg3', ('—Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å', '0', '–º–µ—Å—è—Ü–µ–≤ —Å –¥–∞—Ç—ã –≤—ã–¥–∞—á–∏')), -0.8)
    sum_comp_pat.add_pattern(
      self.create_pattern('sum_max_neg4', ('–ø–æ–∑–¥–Ω–µ–µ —á–µ–º –∑–∞', '0', '–∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –¥–Ω–µ–π –¥–æ –¥–∞—Ç—ã –µ–≥–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è ')), -0.8)
    sum_comp_pat.add_pattern(self.create_pattern('sum_max_neg5', ('–æ–±—â–∞—è –ø–ª–æ—â–∞–¥—å', '0', '–∫–≤ . –º.')), -0.8)

    self.sum_pattern = sum_comp_pat

  def _build_subject_pattern(self):
    ep = ExclusivePattern()

    PRFX = "–ü–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è: \n"

    if True:
      ep.add_pattern(self.create_pattern('t_deal_1', (PRFX, '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –ø—Ä–æ–¥–∞–∂–µ–π')))
      ep.add_pattern(self.create_pattern('t_deal_2', (
        PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞', '—Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ–º –¥–æ–≥–æ–≤–æ—Ä–∞')))
      ep.add_pattern(self.create_pattern('t_deal_3', (
        PRFX + '–æ–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏', '–∫—Ä—É–ø–Ω–æ–π —Å–¥–µ–ª–∫–∏', '—Å–≤—è–∑–∞–Ω–Ω–æ–π —Å –ø—Ä–æ–¥–∞–∂–µ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞')))

      for p in ep.patterns:
        p.soft_sliding_window_borders = True

    if True:
      ep.add_pattern(self.create_pattern('t_org_1', (PRFX, '–û —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–ª–∏–∞–ª–∞', '–û–±—â–µ—Å—Ç–≤–∞')))
      ep.add_pattern(self.create_pattern('t_org_2', (PRFX, '–û–± —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –ü–æ–ª–æ–∂–µ–Ω–∏—è', '–æ —Ñ–∏–ª–∏–∞–ª–µ –û–±—â–µ—Å—Ç–≤–∞')))
      ep.add_pattern(self.create_pattern('t_org_3', (PRFX, '–û –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è', '—Ñ–∏–ª–∏–∞–ª–∞')))
      ep.add_pattern(self.create_pattern('t_org_4', (PRFX, '–û –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–º–æ—á–∏–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è', '—Ñ–∏–ª–∏–∞–ª–∞')))
      ep.add_pattern(self.create_pattern('t_org_5', (PRFX, '–û –≤–Ω–µ—Å–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π', '')))

    if True:
      ep.add_pattern(
        self.create_pattern('t_charity_1', (PRFX + '–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏')))
      ep.add_pattern(
        self.create_pattern('t_charity_2', (PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))
      ep.add_pattern(self.create_pattern('t_charity_3', (PRFX + '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))

      t_char_mix = CoumpoundFuzzyPattern()
      t_char_mix.name = "t_charity_mixed"

      t_char_mix.add_pattern(
        self.create_pattern('tm_charity_1', (PRFX + '–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–º–æ—â–∏', '')))
      t_char_mix.add_pattern(
        self.create_pattern('tm_charity_2', (PRFX + '–û —Å–æ–≥–ª–∞—Å–∏–∏ –Ω–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ', '—Å–¥–µ–ª–∫–∏ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))
      t_char_mix.add_pattern(self.create_pattern('tm_charity_3', (PRFX + '–û–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏', '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è', '')))

      ep.add_pattern(t_char_mix)

    self.subject_pattern = ep


class ProtocolDocument(BasicContractDocument):
  # TODO: use anothwer parent

  def __init__(self, original_text):
    LegalDocument.__init__(self, original_text)

    self.values: List[ProbableValue] = []
    self.section_indices: [int] = None

  def subject_weight_per_section(self, subj_pattern, paragraph_split_pattern):
    assert self.section_indices is not None

    distances_per_subj_pattern_, ranges_, winning_patterns = subj_pattern.calc_exclusive_distances(self.embeddings)

    ranges_global = [
      np.nanmin(distances_per_subj_pattern_),
      np.nanmax(distances_per_subj_pattern_)]

    section_names = [[paragraph_split_pattern.patterns[s[0]].name, s[1]] for s in self.section_indices]
    voting: List[str] = []
    for i in range(1, len(section_names)):
      p1 = section_names[i - 1]
      p2 = section_names[i]

      distances_per_pattern_t = distances_per_subj_pattern_[:, p1[1]:p2[1]]

      dist_per_pat = []
      for row in distances_per_pattern_t:
        dist_per_pat.append(np.nanmin(row))

      patindex = np.nanargmin(dist_per_pat)
      pat_prefix = subj_pattern.patterns[patindex].name[:5]
      #         print(patindex, pat_prefix)

      voting.append(pat_prefix)

      # TODO: HACK more attention to particular sections
      if p1[0] == 'p_agenda' or p1[0] == 'p_solution' or p1[0] == 'p_question':
        voting.append(pat_prefix)

    return Counter(voting), ranges_global, winning_patterns

  def get_found_sum(self) -> ProbableValue:

    print(f'deprecated: {self.get_found_sum}, use  .values')
    best_value: ProbableValue = max(self.values, key=lambda item: item.value.value)

    most_confident_value = max(self.values, key=lambda item: item.confidence)
    best_value = select_most_confident_if_almost_equal(best_value, most_confident_value)

    return best_value

  found_sum: ProbableValue = property(get_found_sum)

  def find_sections_indices(self, distances_per_section_pattern: FixedVector, min_section_size=20) -> [int]:
    x: FixedVector = distances_per_section_pattern
    pattern_to_best_index = np.array([[idx, np.argmin(ma.masked_invalid(row))] for idx, row in enumerate(x)])

    # replace best indices with sentence starts
    pattern_to_best_index[:, 1] = self.find_sentence_beginnings(pattern_to_best_index[:, 1])

    # sort by sentence start
    pattern_to_best_index = np.sort(pattern_to_best_index.view('i8,i8'), order=['f1'], axis=0).view(np.int)

    # remove "duplicated" indexes
    return self.remove_similar_indexes(pattern_to_best_index, 1, min_section_size)

  @deprecated
  def remove_similar_indexes(self, indices: [int], column: int, min_section_size: int = 20) -> [int]:
    warnings.warn("deprecated", DeprecationWarning)
    indices_zipped = [indices[0]]

    for i in range(1, len(indices)):
      if indices[i][column] - indices[i - 1][column] > min_section_size:
        pattern_to_token = indices[i]
        indices_zipped.append(pattern_to_token)

    return np.squeeze(indices_zipped)

  def split_text_into_sections(self, paragraph_split_pattern: ExclusivePattern, min_section_size=10):

    distances_per_section_pattern, _, __ = paragraph_split_pattern.calc_exclusive_distances(self.embeddings)

    # finding pattern positions

    self.section_indices = self.find_sections_indices(distances_per_section_pattern, min_section_size)

    return self.section_indices


class ProtocolAnlysingContext(ParsingContext):

  def __init__(self, embedder):
    ParsingContext.__init__(self, embedder)

    self.protocols_factory: ProtocolPatternFactory = None

    self.protocol: ProtocolDocument = None

  def process(self, text) -> ProtocolDocument:
    self._reset_context()

    if self.protocols_factory is None:
      self.protocols_factory = ProtocolPatternFactory(self.embedder)
      self._logstep("Pattern factory created, patterns embedded into ELMO space")

    # # ----
    # pnames = [p.name[0:5] for p in self.protocols_factory.subject_pattern.patterns]
    # c = Counter(pnames)
    # # ----

    self.protocol = ProtocolDocument(text)
    self.protocol.parse()
    self.protocol.embedd_tokens(self.protocols_factory.embedder)

    self.process_embedded_doc(self.protocol)
    return self.protocol

  def process_embedded_doc(self, doc: ProtocolDocument):

    section_indices = doc.split_text_into_sections(
      self.protocols_factory.paragraph_split_pattern)

    counter, ranges, winning_patterns = doc.subject_weight_per_section(self.protocols_factory.subject_pattern,
                                                                       self.protocols_factory.paragraph_split_pattern)

    section_names = [self.protocols_factory.paragraph_split_pattern.patterns[s[0]].name for s in doc.section_indices]
    sidx = section_names.index('p_solution')
    if sidx < 0:
      sidx = section_names.index('p_agenda')
    if sidx < 0:
      sidx = section_names.index('p_question')

    if sidx < 0:
      sidx = 0

    #   html += winning_patterns_to_html(
    #       doc.tokens, ranges,
    #       winning_patterns,
    #       range(section_indices[sidx][1], section_indices[sidx+1][1]),
    #       colormaps=subject_colormaps )

    doc.values = self.find_values_2(doc)
    doc.per_subject_distances = counter  # Hack

    # self.renderer.print_results(doc)
    # self.renderer.render_subject(counter)

  def find_values_2(self, value_section: LegalDocument) -> List[ProbableValue]:

    value_attention_vector = 1.0 - self.protocols_factory.sum_pattern._find_patterns(value_section.embeddings)
    value_section.distances_per_pattern_dict['value_attention_vector_tuned'] = value_attention_vector
    values: List[ProbableValue] = extract_all_contraints_from_sr_2(value_section, value_attention_vector)
    return values

  def get_value(self):
    return self.protocol.values

  values = property(get_value)


def find_protocol_org(protocol: ProtocolDocument3) -> List[SemanticTag]:
  ret = []
  x: List[SemanticTag] = find_org_names(protocol[0:HyperParameters.protocol_caption_max_size_words])
  nm = SemanticTag.find_by_kind(x, 'org.1.name')
  if nm is not None:
    ret.append(nm)

  tp = SemanticTag.find_by_kind(x, 'org.1.type')
  if tp is not None:
    ret.append(tp)

  protocol.agents_tags = ret
  return ret


import re

from pyjarowinkler import distance


def closest_name(pattern: str, knowns: [str]) -> (str, int):
  #
  min_distance = 0
  found = None
  for b in knowns:
    d = distance.get_jaro_distance(pattern, b, winkler=True, scaling=0.1)
    if d > min_distance:
      found = b
      min_distance = d

  return found, min_distance


def find_org_structural_level(doc: LegalDocument) -> Generator:
  compiled_re = re.compile(ORG_LEVELS_re, re.MULTILINE | re.IGNORECASE | re.UNICODE)

  entity_type = 'org_structural_level'
  for m in re.finditer(compiled_re, doc.text):

    char_span = m.span(entity_type)
    span = doc.tokens_map.token_indices_by_char_range_2(char_span)
    val = doc.tokens_map.text_range(span)

    val, conf = closest_name(val, ORG_LEVELS_names)

    confidence = conf * (1.0 - (span[0] / len(doc)))  # relative distance from the beginning of the document
    if span_len(char_span) > 1 and is_long_enough(val):

      if confidence > HyperParameters.org_level_min_confidence:
        tag = SemanticTag(entity_type, val, span)
        tag.confidence = confidence

        yield tag


def find_confident_spans(slices, block_confidence, tag_name):
  k = 0
  for _slice in slices:
    k += 1
    pv = block_confidence[_slice[0]:_slice[1]]
    confidence = estimate_confidence_by_mean_top_non_zeros(pv, 5)
    print('-' * 100)
    print(_slice, confidence)
    if confidence > 0.6:
      st = SemanticTag(f"{tag_name}_{k}", None, _slice)
      st.confidence = confidence
      yield (st)
