{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev: Parse ALL contracts and save all to JSONs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "al6xZ5y6FBU1",
        "T9CRJ19ZAPyG",
        "B8DgCjtPAqKT"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbsxFAqC6pjQ",
        "colab_type": "text"
      },
      "source": [
        "# Import code from gitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F89NLdN6A8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "_git_branch = 'document-parser-lib'\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from documents import TextMap\n",
        "from renderer import HtmlRenderer\n",
        "from legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7yvxWVk6xJD",
        "colab_type": "text"
      },
      "source": [
        "# Find contract Subject "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLjOzkOATgk",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-embedded test doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCfuE2dw6wO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import unittest\n",
        "import warnings\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from documents import TextMap\n",
        "from legal_docs import LegalDocument\n",
        "from ml_tools import SemanticTag, filter_values_by_key_prefix\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        " \n",
        "#LOAD DOC \n",
        "with open('/content/nlp_tools/tests/2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx.pickle', 'rb') as handle:\n",
        "  doc = pickle.load(handle)\n",
        "\n",
        "with open('/content/nlp_tools/tests/contract_pattern_factory.pickle', 'rb') as handle:\n",
        "  factory = pickle.load(handle)\n",
        "\n",
        "\n",
        "ctx = ContractAnlysingContext(embedder={}, renderer=None, pattern_factory=factory)\n",
        "ctx.verbosity_level = 3\n",
        "\n",
        "#Find sections\n",
        "ctx.sections_finder.find_sections(doc, \n",
        "                                  ctx.pattern_factory, \n",
        "                                  ctx.pattern_factory.headlines,\n",
        "                                  headline_patterns_prefix='headline.')\n",
        "\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q4SyyIW8XLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (doc.paragraphs[1].header.value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al6xZ5y6FBU1",
        "colab_type": "text"
      },
      "source": [
        "### make_subject_attention_vector_3\n",
        "–Ω–∞–π–¥–µ–Ω–∞ —Å–µ–∫—Ü–∏—è \"–ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞\" (subj)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXkQVpxyE7kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ml_tools import SemanticTag, FixedVector, filter_values_by_key_prefix, rectifyed_sum\n",
        "from structures import ContractSubject\n",
        "\n",
        "section = doc.sections['subj'].body\n",
        " \n",
        "section.calculate_distances_per_pattern(ctx.pattern_factory, merge=True, pattern_prefix='x_ContractSubject')\n",
        "\n",
        "all_subjects_vectors = filter_values_by_key_prefix(section.distances_per_pattern_dict, 'x_ContractSubject')\n",
        "all_subjects_mean: FixedVector = rectifyed_sum(all_subjects_vectors)\n",
        "\n",
        "subject_kind =  ContractSubject.Charity\n",
        "subject_attention_vector: FixedVector = ctx.make_subject_attention_vector_3(section, subject_kind,\n",
        "                                                                                   all_subjects_mean)\n",
        "\n",
        "renderer_.render_color_text(section.tokens, subject_attention_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CRJ19ZAPyG",
        "colab_type": "text"
      },
      "source": [
        "### make_subject_attention_vector_3\n",
        "–ø–µ—Ä–≤—ã–µ 1500 —Å–ª–æ–≤, –∫–æ–≥–¥–∞ —Å–µ–∫—Ü–∏—è \"–ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞\" –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqarSmmYVCp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ml_tools import SemanticTag, FixedVector, filter_values_by_key_prefix, rectifyed_sum\n",
        "from structures import ContractSubject\n",
        "\n",
        "#Finding subject in first 1500 words\n",
        "section = doc.subdoc_slice(slice(0, 1500))\n",
        "\n",
        "section.calculate_distances_per_pattern(ctx.pattern_factory, merge=True, pattern_prefix='x_ContractSubject')\n",
        "\n",
        "all_subjects_vectors = filter_values_by_key_prefix(section.distances_per_pattern_dict, 'x_ContractSubject')\n",
        "all_subjects_mean: FixedVector = rectifyed_sum(all_subjects_vectors)\n",
        "renderer_.render_color_text(section.tokens, all_subjects_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_5sC3la9QQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "subject_kind =  ContractSubject.Charity\n",
        "subject_attention_vector: FixedVector = ctx.make_subject_attention_vector_3(section, subject_kind,\n",
        "                                                                                   all_subjects_mean)\n",
        "\n",
        "\n",
        "renderer_.render_color_text(section.tokens, subject_attention_vector)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8DgCjtPAqKT",
        "colab_type": "text"
      },
      "source": [
        "## Find subject sentence (span, region, paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6zU5zyzAshy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#ACHTUNG: this code moved to gitHub codebase, see _find_most_relevant_paragraph\n",
        "\n",
        "attention_vector = subject_attention_vector\n",
        "paragraph_attention_vector = np.zeros_like (attention_vector)\n",
        "top_index=0\n",
        "for i in np.nonzero(attention_vector)[0]:\n",
        "  par = section.tokens_map.sentence_at_index(i)\n",
        "  paragraph_len = par[1]-par[0]\n",
        "  if paragraph_len:\n",
        "    # TODO: Next line is weird\n",
        "    # Calculate density of the matches per paragraph:\n",
        "    density = attention_vector[i] / paragraph_len\n",
        "    paragraph_attention_vector[par[0]: par[1]] += attention_vector[i] + density\n",
        "\n",
        "    if paragraph_attention_vector[par[0]] > paragraph_attention_vector[top_index]:\n",
        "      top_index = par[0]\n",
        "\n",
        "par = section.tokens_map.sentence_at_index(top_index)\n",
        "renderer_.render_color_text(section.tokens_cc, paragraph_attention_vector)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsL3ksfNw-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc.tokens_map.text_range(par)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9wt2lg2jyXw",
        "colab_type": "text"
      },
      "source": [
        "# export JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMp2NnSruIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from legal_docs import DocumentJson\n",
        "\n",
        "ctx.analyze_contract_doc(doc)\n",
        "\n",
        "_path = '____.json'\n",
        "with open(_path, 'w') as file:\n",
        "  jjj = DocumentJson(doc)\n",
        "  _j= json.dumps(jjj.__dict__, indent=4, ensure_ascii=False, default=lambda o: '<not serializable>')\n",
        "  file.write(_j)\n",
        "  print(f'saved file to {_path}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esxRGxnjidln",
        "colab_type": "text"
      },
      "source": [
        "### Render single test Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iceqc8qUjX8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_semantic_tag(tag: SemanticTag, map: TextMap):\n",
        "  print('-->', f'{tag.kind} \\t\\t [{tag.value}]\\t [{map.text_range(tag.span)}]')\n",
        "\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        " \n",
        "  for tag in cd.tags:\n",
        "    if tag.kind !='headline':\n",
        "     print_semantic_tag(tag, wordsmap)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX4o_zu4icFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      markup_vector[span[0]:span[1]] += 0.5\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector, _range=(0, 1))\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "def load_and_show_json(filename):\n",
        "  with open(filename, 'r') as json_file:\n",
        "    data_str = json_file.read()\n",
        "    # print(str(data_str))\n",
        "    json_obj:DocumentJson = DocumentJson.from_json( data_str)\n",
        "    html = json2html(json_obj)\n",
        "    display(HTML(html))\n",
        "\n",
        "load_and_show_json(_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfhy1BhMNfm",
        "colab_type": "text"
      },
      "source": [
        "## Export all contracts to JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7_rZoyzMRzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpHstfXIOVqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from embedding_tools import ElmoEmbedder\n",
        "elmo_embedder = ElmoEmbedder(module_url = 'https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz')\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from contract_parser import ContractAnlysingContext\n",
        "contractAnlysingContext = ContractAnlysingContext(elmo_embedder, renderer_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/nemoware/document-parser/releases/download/1.0.5/document-parser-1.0.5-distribution.zip\n",
        "!unzip document-parser-1.0.5-distribution.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107CPsZglZ7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ ['documentparser']='/content/document-parser-1.0.5'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUASEb5sMxJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "from legal_docs import DocumentJson\n",
        "\n",
        "wp = WordDocParser()\n",
        "_out = '/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/JSONs/contracts-19.09.26/'\n",
        "contracts_filename_prefix = '/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "\n",
        "filenames = wp.list_filenames(contracts_filename_prefix)\n",
        "print(filenames)\n",
        "\n",
        "cnt = 0\n",
        "for fn in filenames:\n",
        "  cnt += 1\n",
        "  print(f'reading:\"{fn}\"')\n",
        "  short_fn = fn.split('/')[-1]\n",
        "\n",
        "  try:\n",
        "    # ------------------------\n",
        "    contract = join_paragraphs(wp.read_doc(fn), fn)\n",
        "    # ------------------------\n",
        "\n",
        "    contractAnlysingContext.analyze_contract_doc(contract)\n",
        "    contract = contractAnlysingContext.contract\n",
        "\n",
        "    _path = _out + f'{short_fn}.json'\n",
        "    with open(_path, 'w') as file:\n",
        "      # \n",
        "      jjj = DocumentJson(contract)\n",
        "      file.write(jjj.dumps())\n",
        "      print(f'saved file to {_path}')\n",
        "  except Exception as e:\n",
        "    print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aZ4HQRstCLH",
        "colab_type": "text"
      },
      "source": [
        "### Verify random json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIHEi_QqtGS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_and_show_json(_out+filenames[3].split('/')[-1]+'.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19diMBJejNZ",
        "colab_type": "text"
      },
      "source": [
        "#Parse 2K sectioned contracts (Achtung, dis takes time)\n",
        "- upload zipped results of document-parser first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxlAyKnbh4O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzNVR7Kee5mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/jsons_phase0.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1d3bop-fyqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "filenames = [file for file in glob.glob( \"/content/jsons_phase0/**/*.json\", recursive=True)] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkY14L71ieD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import csv, json\n",
        "from bson import json_util\n",
        "\n",
        "\n",
        "from contract_parser import ContractAnlysingContext\n",
        "from integration.word_document_parser import join_paragraphs\n",
        "\n",
        "\n",
        "def export_csv(rows, headline=['1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
        "  with open(f'contracts-stats.csv', mode='w') as csv_file:\n",
        "    _writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    _writer.writerow(headline)\n",
        "    for l in rows:\n",
        "      _writer.writerow(l)\n",
        "\n",
        "\n",
        "def _parse_contract(res, doc_id, row, ctx: ContractAnlysingContext):\n",
        "  contract = join_paragraphs(res, doc_id)\n",
        "  ctx.analyze_contract_doc(contract)\n",
        "\n",
        "  row[4:8] = [contract.tag_value('org.1.name'),\n",
        "              contract.tag_value('org.1.alias'),\n",
        "              contract.tag_value('org.2.name'),\n",
        "              contract.tag_value('org.2.alias')]\n",
        "  return contract\n",
        "\n",
        " \n",
        "rows=[]\n",
        "cnt=0\n",
        "\n",
        "for fn in filenames[0:10]:\n",
        "  with open(fn, 'r') as file:\n",
        "    json_string = file.read()\n",
        "    res = json.loads(json_string, object_hook=json_util.object_hook)\n",
        "\n",
        "  if 'CONTRACT' == res[\"documentType\"]:\n",
        "    row = [cnt, '', None, None, None, None, None, None, fn, None]\n",
        "    row[2:4] = [res[\"documentType\"], res[\"documentDate\"]]\n",
        "    _parse_contract(res, fn,  row, contractAnlysingContext)\n",
        "\n",
        "    rows.append(row)\n",
        "    if cnt % 5 == 0: #save every 5 rows\n",
        "      export_csv(rows)\n",
        "\n",
        "    cnt+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qln4vihrY9gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def zipfolder(foldername, target_dir):            \n",
        "  zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  rootlen = len(target_dir) + 1\n",
        "  for base, dirs, files in os.walk(target_dir):\n",
        "    for file in files:\n",
        "      fn = os.path.join(base, file)\n",
        "      zipobj.write(fn, fn[rootlen:])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}