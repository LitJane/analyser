{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Batch test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LfLfXF13M80m"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/tokenization-improve/notebooks/Batch_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGoExQvowHLP",
        "colab_type": "text"
      },
      "source": [
        "# BATCH processing\n",
        "- processing all files and saving results to scv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "_git_branch = \"tokenization-improve\" #@param {type:\"string\"}\n",
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSb3bCJFxVp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Select sections to run: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown 1. ### Charters\n",
        "batch_charters = True #@param {type:\"boolean\"}\n",
        "batch_charters_contents = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown 2. ### Protocols\n",
        "batch_protocols = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown 3. ### Contracts\n",
        "\n",
        "batch_contract_contents = True #@param {type:\"boolean\"}\n",
        "batch_contract_find_sections = True #@param {type:\"boolean\"}\n",
        "batch_contracts = True #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9nfwicwxBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "   \n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(module_url = 'https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz')\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "def _init_the_code():\n",
        "  if '_init_the_code' in GLOBALS__:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import SilentRenderer\n",
        "\n",
        "  class RendererForBatch(SilentRenderer):\n",
        "    pass\n",
        "\n",
        "  GLOBALS__['renderer'] = RendererForBatch()\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "  def read_doc(fn):\n",
        "    import docx2txt, sys, os\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text\n",
        "\n",
        "  GLOBALS__['read_doc'] = read_doc\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "# AZ:---------------------------------------------------------------------------END OF THE THE CODE, See you later\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKaWFEoWcK_u",
        "colab_type": "code",
        "outputId": "476a8f1f-d635-4f4d-912c-4b98c715674a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "## do preparation here   \n",
        "    \n",
        "#1.\n",
        "_init_import_code_from_gh()\n",
        "#2.\n",
        "_init_embedder()\n",
        "#3.\n",
        "_init_the_code()\n",
        "#4. \n",
        "# if batch_charters:\n",
        "#   _init_charters()\n",
        "if batch_contracts:\n",
        "  _init_contracts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fetching code from GitHub.....tokenization-improve\n",
            "\n",
            "\n",
            "ü¶ä GIT revision:\n",
            "510\n",
            "* tokenization-improve\n",
            "Even less verbose\n",
            "\n",
            "Logging achtungs\n",
            "\n",
            "Even less verbose\n",
            "\n",
            "\n",
            "installing antiword...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "antiword is already the newest version (0.37-11build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "\n",
            "installing docx2txt...\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "\n",
            "‚ù§Ô∏è DONE importing Code fro GitHub\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 10:21:44.282207 140173885994880 deprecation_wrapper.py:119] From nlp_tools/embedding_tools.py:132: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0802 10:21:45.560985 140173885994880 deprecation_wrapper.py:119] From nlp_tools/embedding_tools.py:143: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "W0802 10:21:45.564817 140173885994880 deprecation_wrapper.py:119] From nlp_tools/embedding_tools.py:143: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "W0802 10:21:45.570451 140173885994880 deprecation_wrapper.py:119] From nlp_tools/embedding_tools.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "‚ù§Ô∏è DONE creating words embedding model\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x7f7c6b279d68>\n",
            "loading word cases stats model /content/nlp_tools/vocab/word_cases_stats.pickle\n",
            "‚ù§Ô∏è DONE initializing the code\n",
            "‚ù§Ô∏è DONE initing Contracts-related tools and models \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZTskr7JU2E5"
      },
      "source": [
        "# BATCH\n",
        "–ø–∞–∫–µ—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —É—Å—Ç–∞–≤–æ–≤, –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKvUAZujWSPp",
        "colab_type": "text"
      },
      "source": [
        "## Authenticate on Google and mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ENUGvcVCrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b2309d6-22f0-4426-dca0-284ea3a38895"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch4V2kirjnte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "def read_documents(filename_prefix):\n",
        "  filenames = []\n",
        "  filenames += [file for file in glob.glob(filename_prefix + \"*.docx\", recursive=True)]\n",
        "  filenames += [file for file in glob.glob(filename_prefix + \"*.doc\", recursive=True)]\n",
        "\n",
        "\n",
        "  texts = {}\n",
        "  for file in filenames:\n",
        "    try:\n",
        "      text = GLOBALS__['read_doc'](file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.doc FILE!!', file)\n",
        "\n",
        "  return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3NFgCm8BEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------\n",
        "import csv\n",
        "def _trim_and_pad(l):\n",
        "  return (l + [''] * 40)[0:10] # padding: make all linese 10 columns-long\n",
        "\n",
        "def export_csv(filename, lines, headline=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']):\n",
        "  with open(f'/content/gdrive/My Drive/GazpromOil/–í—Å—è—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (shared)/{filename}', mode='w') as csv_file:\n",
        "    _writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    _writer.writerow(_trim_and_pad(headline) )\n",
        "    for l in lines:\n",
        "      ll = _trim_and_pad(l)\n",
        "      # print(ll)\n",
        "      _writer.writerow(ll)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhUn2IWFjtCo",
        "colab_type": "text"
      },
      "source": [
        "# CHARTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycf8gUg-Erq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from charter_parser import CharterDocumentParser\n",
        "from charter_patterns import CharterPatternFactory\n",
        "from embedding_tools import ElmoEmbedder\n",
        "\n",
        "\n",
        "CH_PF = CharterPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "CH_CTX = CharterDocumentParser(CH_PF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLfXF13M80m",
        "colab_type": "text"
      },
      "source": [
        "### üè∫Micro-self-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tTLwiVK5hMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59625509-67fe-4cc5-a877-3443e602d27f"
      },
      "source": [
        "microsample1=\"—Å–ª–æ–≤–æ\"\n",
        "microsample2 = \"\"\"\n",
        "–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è\n",
        "\n",
        "1.1. –í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –Ω–∞—Å—Ç–æ—è—â–∏–º –î–æ–≥–æ–≤–æ—Ä–æ–º –ñ–µ—Ä—Ç–≤–æ–≤–∞—Ç–µ–ª—å –æ–±—è–∑—É–µ—Ç—Å—è –±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ü–æ–ª—É—á–∞—Ç–µ–ª—é –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –≤ —Ä–∞–∑–º–µ—Ä–µ 30 000 (–¢—Ä–∏–¥—Ü–∞—Ç—å —Ç—ã—Å—è—á) —Ä—É–±–ª–µ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "CH_CTX.analyze_charter(microsample1)\n",
        "CH_CTX.analyze_charter(microsample2)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ö†Ô∏è WARNING: - –°–µ–∫—Ü–∏—è –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–Ω–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n",
            "‚ö†Ô∏è WARNING: - –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –∏—Å–∫–∞—Ç—å –ø—Ä–æ—Å—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ üöë\n",
            "----WARNING!: function make_soft_attention_vector is deprecated\n",
            "----ERROR: make_soft_attention_vector: too few tokens —Å–ª–æ–≤–æ \n",
            "----WARNING!: function subdoc is deprecated\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "Recent parsing warnings:\n",
            "\t\t ‚ö†Ô∏è WARNING: - –°–µ–∫—Ü–∏—è –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–æ–º–ø–Ω–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\n",
            "\t\t ‚ö†Ô∏è WARNING: - –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –∏—Å–∫–∞—Ç—å –ø—Ä–æ—Å—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ üöë\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "----WARNING!: function rectifyed_sum_by_pattern_prefix is deprecated\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_vector': array([0.15856819, 0.20308314, 0.44035047, 0.46422809, 0.45892921,\n",
              "         0.22366183, 0.10593686, 0.08844268, 0.0897323 , 0.05845033,\n",
              "         0.10999921, 0.21157591, 0.09394148, 0.07869598, 0.04399219,\n",
              "         0.11458343, 0.05021568, 0.08370542, 0.07552669, 0.05326365,\n",
              "         0.12868634, 0.12525885, 0.07050131, 0.02920814, 0.10561333,\n",
              "         0.18129593, 0.06377464, 0.0612906 , 0.14995377, 0.16842198,\n",
              "         0.46758541, 0.48259509]),\n",
              "  'name': '',\n",
              "  'tokens': ['–û–±—â–∏–µ',\n",
              "   '–ø–æ–ª–æ–∂–µ–Ω–∏—è',\n",
              "   '\\n',\n",
              "   '\\n',\n",
              "   '1.1',\n",
              "   '.',\n",
              "   '–í',\n",
              "   '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏',\n",
              "   '—Å',\n",
              "   '–Ω–∞—Å—Ç–æ—è—â–∏–º',\n",
              "   '–î–æ–≥–æ–≤–æ—Ä–æ–º',\n",
              "   '–ñ–µ—Ä—Ç–≤–æ–≤–∞—Ç–µ–ª—å',\n",
              "   '–æ–±—è–∑—É–µ—Ç—Å—è',\n",
              "   '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ',\n",
              "   '–ø–µ—Ä–µ–¥–∞—Ç—å',\n",
              "   '–ü–æ–ª—É—á–∞—Ç–µ–ª—é',\n",
              "   '–¥–µ–Ω–µ–∂–Ω—ã–µ',\n",
              "   '—Å—Ä–µ–¥—Å—Ç–≤–∞',\n",
              "   '–≤',\n",
              "   '—Ä–∞–∑–º–µ—Ä–µ',\n",
              "   '30000',\n",
              "   '(',\n",
              "   '–¢—Ä–∏–¥—Ü–∞—Ç—å',\n",
              "   '—Ç—ã—Å—è—á',\n",
              "   ')',\n",
              "   '—Ä—É–±–ª–µ–π',\n",
              "   '–≤',\n",
              "   '–∫–∞—á–µ—Å—Ç–≤–µ',\n",
              "   '–ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è',\n",
              "   '.',\n",
              "   '\\n',\n",
              "   '\\n'],\n",
              "  'type': 'org_unknown',\n",
              "  'type_name': 'undefined'},\n",
              " {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POrjmZ2Sx8Z_",
        "colab_type": "text"
      },
      "source": [
        "### üè∫ Test parse single charter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEq5r32Vx6Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  CH_CTX.analyze_charter(charters['/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ - –ì–ü–ù-–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–ì–û–°–ê-2018.docx'], verbosity=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgEuLsK4kVHX",
        "colab_type": "text"
      },
      "source": [
        "## Read all the charters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDj8yBOJ8qh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "18d72654-9079-4b81-b4ae-6209ae0f3f44"
      },
      "source": [
        "if batch_charters or batch_charters_contents:\n",
        "  charters_filename_prefix='/content/gdrive/My Drive/GazpromOil/Charters/'\n",
        "  charters = read_documents(charters_filename_prefix)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ - –ì–ü–ù-–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–ì–û–°–ê-2018.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤_–†–µ–≥. –ø—Ä–æ–¥–∞–∂–∏ –∞–≤–≥ 2018.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/6.1.1(a) Project Tri-Neft - Sunrise Charter.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ 2.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–ï–Æ –£—Å—Ç–∞–≤.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–ú–ù–ü–ó –£—Å—Ç–∞–≤.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–ì–ü–ù –£—Å—Ç–∞–≤.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/—è–ú–µ—Å—Å–æ—è—Ö–∞ –£—Å—Ç–∞–≤–∞.docx\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–ú–ù–ì –£—Å—Ç–∞–≤.docx\n",
            "Unexpected error: (<class 'KeyError'>, KeyError(\"There is no item named 'word/document.xml' in the archive\",), <traceback object at 0x7f7c1a356888>)\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –ì–ü–ù-–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫ (–ù–ë)_—Å –°–î.doc\n",
            "Unexpected error: (<class 'KeyError'>, KeyError(\"There is no item named 'word/document.xml' in the archive\",), <traceback object at 0x7f7c1a356688>)\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –ì–ü–ù-–ö–ü_–ì–û–°–£-2018_—Å–µ–Ω—Ç—è–±—Ä—å end.doc\n",
            "Unexpected error: (<class 'KeyError'>, KeyError(\"There is no item named 'word/document.xml' in the archive\",), <traceback object at 0x7f7c1a356108>)\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–ù–æ–≤–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è –£—Å—Ç–∞–≤–∞.doc\n",
            "Unexpected error: (<class 'KeyError'>, KeyError(\"There is no item named 'word/document.xml' in the archive\",), <traceback object at 0x7f7c1a356b08>)\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤_–û–û–û –Æ–ü –ì–ü–ó_—Ä–µ–¥ 5.doc\n",
            "Unexpected error: (<class 'KeyError'>, KeyError(\"There is no item named 'word/document.xml' in the archive\",), <traceback object at 0x7f7c1a356808>)\n",
            "good: /content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –û–û–û.doc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Uax-wv4_8f",
        "colab_type": "text"
      },
      "source": [
        "##  Util/export methods "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdakIJA4-qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "from legal_docs import LegalDocument, CharterDocument\n",
        "from patterns import PatternSearchResult\n",
        "from renderer import org_level_dict\n",
        "from structures import OrgStructuralLevel\n",
        "from transaction_values import ValueConstraint\n",
        "\n",
        "ggg = {\n",
        "  'RealEstate': 'üêå –ù–µ–ø–æ–¥–≤–∏–∂–Ω–æ—Å—Ç—å',\n",
        "  'Charity': 'üôè –ë–ª–∞–≥-–æ—Å—Ç—å',\n",
        "  'Lawsuit': 'üè¶ –°—É–¥–µ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω–∞—è',\n",
        "  'Other': 'üëΩ–Ω–µ –∏–∑–≤–µ—Å—Ç–Ω—è–∫ (other)'\n",
        "}\n",
        "\n",
        "\n",
        "def _populate_structure( doc: LegalDocument):\n",
        "  lines=[]\n",
        "\n",
        "  for n in range(len(doc.structure.headline_indexes) - 1):\n",
        "    i = doc.structure.headline_indexes[n]\n",
        "    i_next = doc.structure.headline_indexes[n + 1]\n",
        "\n",
        "    sline = doc.structure.structure[i]\n",
        "    sline_next = doc.structure.structure[i_next]\n",
        "\n",
        "    line = [\n",
        "      '',#B erase\n",
        "      '',#C\n",
        "      '',#D\n",
        "      str(i),#E\n",
        "      sline.to_string(doc.tokens_cc),#F\n",
        "      sline_next.span[0]-sline.span[1],\n",
        "      sline.line_number,\n",
        "      sline.minor_number,\n",
        "      str(sline.number),\n",
        "      sline.level,\n",
        "      sline._possible_levels[0],\n",
        "      sline.span[1]\n",
        "      \n",
        "    ]\n",
        "    \n",
        "    lines.append(line)\n",
        "  \n",
        "  return lines\n",
        "\n",
        "def _populate_org(orginfo) -> int:\n",
        "  line = [\n",
        "    orginfo['name'],\n",
        "    orginfo['type_name'],\n",
        "    ''\n",
        "  ]\n",
        "  # print(orginfo)\n",
        "  #   from text_tools import untokenize\n",
        "  #   worksheet.update_cell(the_row, col , orginfo['name'])\n",
        "  #   worksheet.update_cell(the_row, col + 1, orginfo['type_name'])\n",
        "  # #   worksheet.update_cell(the_row, col + 2, orginfo['type'])\n",
        "  #   worksheet.update_cell(the_row, col + 2, untokenize(orginfo['tokens'])[0:300] )\n",
        "\n",
        "  return [line]\n",
        "\n",
        "\n",
        "def _populate_rz(charter):\n",
        "  lines = [ [charter.filename, '?'] ]\n",
        "   \n",
        "  lines += _populate_org(charter.org)\n",
        "\n",
        "  for level in OrgStructuralLevel:\n",
        "    constraint_search_results: List[PatternSearchResult] = charter.constraints_by_org_level(level)\n",
        "\n",
        "    line = [\n",
        "      '', '',\n",
        "      org_level_dict[level].upper()\n",
        "    ]\n",
        "\n",
        "    lines.append(line)\n",
        "    # renderered=0\n",
        "    for sentence in constraint_search_results:\n",
        "      rr = populate_constraints(sentence)\n",
        "      lines += rr\n",
        "    # if renderered==0:\n",
        "    #   r+=1\n",
        "    #   _clean(ws, r)\n",
        "    #   ws.update_acell(f'G{r}', '–ø—É—Å—Ç–æ—Ç–∞ –ø—É—Å—Ç–æ—Ç—ã –ù–ê–ô–î–ï–ù–ê ‡•ê ‡§ì‡§ô‡•ç‡§ï‡§æ‡§∞ üêº –æ–º —à–∞–Ω—Ç–∏ —à–∞–Ω—Ç–∏  ‡•ê' )\n",
        "\n",
        "    #   r+=1\n",
        "\n",
        "  return lines\n",
        "\n",
        "\n",
        "def populate_constraints(sentence: LegalDocument):\n",
        "  lines = []\n",
        "  subj_type = sentence.subject_mapping[\"subj\"]\n",
        "\n",
        "  constraints: List[ValueConstraint] = sentence.constraints\n",
        "  sname = ggg[subj_type.name]\n",
        "\n",
        "  line = [\n",
        "    '', '', \" \".join(sentence.tokens[0:200]),\n",
        "    sname\n",
        "  ]\n",
        "\n",
        "  lines.append(line)\n",
        "\n",
        "  if len(constraints) > 0:\n",
        "    for pv in constraints:\n",
        "      c = pv.value\n",
        "      if c.value is not None and c.sign is not None:\n",
        "        sign_ = \">\" if c.sign > 0 else '<'\n",
        "        line = [\n",
        "          '', '', '', '', sign_, c.value, c.currency\n",
        "        ]\n",
        "        lines.append(line)\n",
        "\n",
        "  return lines\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ab4gPIyQVJ",
        "colab_type": "text"
      },
      "source": [
        "## Parse all charters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6qtgta-5DHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ee7a2f6-af07-4f0e-931e-211caeee10be"
      },
      "source": [
        "lines=[]\n",
        "cnt =0\n",
        "for fn in charters.keys():\n",
        "  cnt+=1\n",
        "  print(cnt, \"=\"*100)\n",
        "  print(fn)\n",
        "  CH_CTX.analyze_charter(charters[fn], verbosity=0)\n",
        "  charter: CharterDocument = CH_CTX.charter\n",
        "  \n",
        "  lines+=[[fn]]\n",
        "  lines += _populate_rz(charter)\n",
        "  lines+=[['']]\n",
        "  export_csv('charters_data.csv', lines)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ - –ì–ü–ù-–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç_–ì–û–°–ê-2018.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "2 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤_–†–µ–≥. –ø—Ä–æ–¥–∞–∂–∏ –∞–≤–≥ 2018.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "3 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/6.1.1(a) Project Tri-Neft - Sunrise Charter.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "4 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ 2.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "5 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–ï–Æ –£—Å—Ç–∞–≤.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "6 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–ú–ù–ü–ó –£—Å—Ç–∞–≤.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "7 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–ì–ü–ù –£—Å—Ç–∞–≤.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "8 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/—è–ú–µ—Å—Å–æ—è—Ö–∞ –£—Å—Ç–∞–≤–∞.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "9 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–ú–ù–ì –£—Å—Ç–∞–≤.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "10 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –ì–ü–ù-–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫ (–ù–ë)_—Å –°–î.doc\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "11 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –ì–ü–ù-–ö–ü_–ì–û–°–£-2018_—Å–µ–Ω—Ç—è–±—Ä—å end.doc\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "12 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–ù–æ–≤–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è –£—Å—Ç–∞–≤–∞.doc\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "13 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤_–û–û–û –Æ–ü –ì–ü–ó_—Ä–µ–¥ 5.doc\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n",
            "14 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Charters/–£—Å—Ç–∞–≤ –û–û–û.doc\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t Splitting Document into sections ‚úÇÔ∏è üìÉ -> üìÑüìÑüìÑ\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 1.\t extracting NERs (named entities üè¶ üè® üèõ)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeSkOj16jF7O",
        "colab_type": "text"
      },
      "source": [
        "# CONTRACTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZEaU9oR_kk2r"
      },
      "source": [
        "#### Rread all Contracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m0prkxt5kk2s",
        "colab": {}
      },
      "source": [
        "if batch_contracts or batch_contract_contents or batch_contract_find_sections:\n",
        "  contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "  contracts = read_documents(contracts_filename_prefix)\n",
        "print(f'Total CONTRACTS: {len(contracts)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jRKo-uhDkk2t",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "from contract_parser import ContractDocument\n",
        "from ml_tools import ProbableValue\n",
        "from structures import ContractSubject\n",
        "\n",
        "known_subjects_dict = {\n",
        "  ContractSubject.Charity: 'üôè –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',\n",
        "  ContractSubject.RealEstate: \"üêå –°–¥–µ–ª–∫–∏ —Å –∏–º—É—â–µ—Å—Ç–≤–æ–º\",\n",
        "  ContractSubject.Lawsuit: \"üè¶–°—É–¥–µ–±–Ω—ã–µ —Å–ø–æ—Ä—ã\",\n",
        "  ContractSubject.Deal: \"üëΩ–°–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏\",\n",
        "  ContractSubject.Other: \"üëΩ–ü—Ä–æ—á–µ–µ\"\n",
        "}\n",
        "\n",
        "\n",
        "def render_subj(contract: ContractDocument):\n",
        "  subjects: List[ProbableValue] = contract.subjects\n",
        "  confidence = 0\n",
        "  if len(subjects) > 0:\n",
        "    sorted_ = [y for y in sorted(subjects, key=lambda x: -x.confidence)]\n",
        "    subject_kind = sorted_[0].value\n",
        "    confidence = sorted_[0].confidence\n",
        "  else:\n",
        "    subject_kind = ContractSubject.Other\n",
        "\n",
        "  if subject_kind in known_subjects_dict:\n",
        "    rendering_name = known_subjects_dict[subject_kind]\n",
        "  else:\n",
        "    rendering_name = '–ø—Ä–æ—á–µ–µ'\n",
        "\n",
        "  return rendering_name, subject_kind, confidence\n",
        "\n",
        "\n",
        "def _populate_contract(contract: ContractDocument, filename):\n",
        "  lines = []\n",
        "  lines += [[filename]]\n",
        "\n",
        "  doc = contract\n",
        "  values = contract.contract_values\n",
        "\n",
        "  line1=['‚ö†Ô∏è –ö–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç 1 –Ω–µ –≤—ã—è–≤–ª–µ–Ω','‚ö†Ô∏è –§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –º—É—Ç–Ω–∞—è']\n",
        "  line2=['‚ö†Ô∏è –ö–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç 2 –Ω–µ –≤—ã—è–≤–ª–µ–Ω','‚ö†Ô∏è –§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –º—É—Ç–Ω–∞—è']\n",
        "  \n",
        "  for tag in contract.agents_tags:\n",
        "    if tag['kind']=='org.1.name':\n",
        "      line1[0]=tag['value']\n",
        "    if tag['kind']=='org.2.name':\n",
        "      line2[0]=tag['value']\n",
        "    if tag['kind']=='org.1.type':\n",
        "      line1[1]=tag['value']\n",
        "    if tag['kind']=='org.2.type':\n",
        "      line2[1]=tag['value']\n",
        "\n",
        "  lines.append(line1)\n",
        "  lines.append(line2)\n",
        "  if 'subj' in doc.sections:\n",
        "    section = doc.sections['subj']\n",
        "    body = section.body.text[:1000]\n",
        "    headline = section.subdoc.text[:500]\n",
        "\n",
        "    quote = headline + \"\\n\" + body\n",
        "  else:\n",
        "    quote = '‚ö†Ô∏è –†–∞–∑–¥–µ–ª –æ –ø—Ä–µ–¥–º–µ—Ç–µ (–ª—é–±–≤–∏) –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω'\n",
        "\n",
        "  rendering_name, subject_kind, confidence = render_subj(doc)\n",
        "  lines.append(['', '', rendering_name, '', '', '', f'{confidence:.4g}', quote])\n",
        "\n",
        "  if len(values) > 0:\n",
        "\n",
        "    for pc in values:\n",
        "      c = pc.value\n",
        "      line = ['','', '', c.sign, c.value, c.currency, f'{pc.confidence:.4g}', ' '.join(c.context.tokens)[:500]]\n",
        "      lines.append(line)\n",
        "\n",
        "  else:\n",
        "    lines.append(['','', '', \"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –Ω–∏—á—Ç–æ!\"])\n",
        "\n",
        "  # q+='\\n\\n'+CTX.get_warings()\n",
        "  # worksheet.update_cell(r, pricequote_col, q)  \n",
        "\n",
        "  return lines\n",
        "\n",
        "#-----------------------\n",
        "\n",
        "CO_CTX = GLOBALS__['ContractAnlysingContext']\n",
        "lines = _populate_contract(CO_CTX.contract, filename)\n",
        "# export_csv('contracts.csv', lines)\n",
        "export_csv('contract___test.csv', lines, ['', '1', '2', 'sign', 'value', 'currency', 'confidence', 'quote', '8', '9'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU_9Js-cU2XS",
        "colab_type": "text"
      },
      "source": [
        "## Parse all Contracts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsYe6y5QGchb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5352d699-47bc-4ec4-c9ce-f9ba74c3e23f"
      },
      "source": [
        "lines=[]\n",
        "cnt =0\n",
        "for fn in contracts.keys() :\n",
        "  cnt+=1\n",
        "  print(cnt, \"=\"*100)\n",
        "  print(fn)\n",
        "  \n",
        "  CO_CTX.analyze_contract(contracts[fn])  \n",
        "  lines += _populate_contract(CO_CTX.contract, fn)\n",
        "\n",
        "  lines+=[['']]\n",
        "  export_csv('contracts_data.csv', lines, ['', '1', '2', 'sign', 'value', 'currency', 'confidence', 'quote', '8', '9'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ====================================================================================================\n",
            "/content/gdrive/My Drive/GazpromOil/Contracts/–î–æ–≥–æ–≤–æ—Ä_–û–û–û –ó–æ–¥—á–∏–∏ÃÜ_25 –º–ª–Ω.$.docx\n",
            "‚ù§Ô∏è ACCOMPLISHED: \t 0.\t parsing document üëû and detecting document high-level structure\n",
            "WARNING: Document is too large for embedding: 19232 tokens. Splitting into 3 windows overlapping with 100 tokens \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}