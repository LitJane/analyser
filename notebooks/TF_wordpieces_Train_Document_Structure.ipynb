{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF: wordpieces Train Document Structure.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "03-DoDRNznUm",
        "ghkcbyjWC8iF",
        "ch0yAjrKpyGG",
        "e2BkMulqQxJu",
        "nR9cnbYg-6xu"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/tensorflow-model-tokens/notebooks/TF_wordpieces_Train_Document_Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbe5_adm0LeZ",
        "colab_type": "text"
      },
      "source": [
        "# Document structure detector model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"tensorflow-model-tokens\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "def read_doc(fn):\n",
        "  import docx2txt, sys, os\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    text = docx2txt.process(fn)\n",
        "\n",
        "  except:\n",
        "    print(\"Unexpected error:\", sys.exc_info())\n",
        "    os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "    with open(fn + '.txt') as f:\n",
        "      text = f.read()\n",
        "\n",
        "  return text\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "03-DoDRNznUm"
      },
      "source": [
        "### Rendering utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqlvJBC6znUm",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "_c_cyan1=(0, 1, 0.6)\n",
        "_c_orange1=(1, 0.7, 0)\n",
        "_c_green1=(0.3, 1, 0)\n",
        "_c_pink1=(0.3, 0, 1)\n",
        "\n",
        "_c_cyan2=(0, 0.8, 0.6)\n",
        "_c_orange2=(1, 0.6, 0)\n",
        "_c_green2=(0.4, 1, 0)\n",
        "_c_pink2=(0.4, 0, 1)\n",
        "\n",
        "token_colors=[\n",
        "    (0.5, 0.5, 0.5),#undefined\n",
        "    \n",
        "    _c_cyan1,\n",
        "    _c_orange1,\n",
        "    _c_green1,\n",
        "    _c_pink1,\n",
        "    _c_pink1,\n",
        "    \n",
        "    _c_cyan2,\n",
        "    _c_orange2,\n",
        "    _c_green2,\n",
        "    _c_pink2,\n",
        "    _c_pink2,\n",
        "    \n",
        "    (0.1, 0.3, 1)#headlines\n",
        "]\n",
        "\n",
        "_colormap={}\n",
        "for k in range(20):\n",
        "  key = f'c{k}'\n",
        "  _colormap[key] = token_colors[ k % len(token_colors) ]\n",
        "    \n",
        "    \n",
        "def color_matrix(matrix, tokens):\n",
        "    \n",
        "  mt = matrix.T\n",
        "  vectors = {}\n",
        "\n",
        "  for k in range( int(matrix.shape[1])):\n",
        "\n",
        "    key = f'c{k}'\n",
        "    _colormap[key] = token_colors[ k % len(token_colors) ]\n",
        "    vectors[key] = mt[k]\n",
        "  \n",
        " \n",
        "  GLOBALS__['renderer'].render_multicolor_text(tokens, vectors, _colormap, min_color=(0.4, 0.4, 0.4) )\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "##------------\n",
        "def plot_matrix(matrix):\n",
        "    \n",
        "  mt = matrix.T\n",
        "  \n",
        "  with sns.axes_style(\"white\"):\n",
        "    plt.figure(figsize=(25, 3))\n",
        "    ax = sns.heatmap(mt, square=False,  cmap=\"YlGnBu\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0zwssE3ugsi",
        "colab_type": "text"
      },
      "source": [
        "## Read train set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zYVejKzZznUk"
      },
      "source": [
        "### üìÇüë§Load files from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tGCs9IjyznUk",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        " \n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "_path = '/content/gdrive/My Drive/GazpromOil/'\n",
        "\n",
        "with open(_path+'doc_structure_trainset.pickle', 'rb') as handle:\n",
        "  TRAINSET_read = pickle.load(handle)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYQ-CmVg0ybh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in TRAINSET_read[0:2]:\n",
        "  GLOBALS__['renderer'].render_color_text(a, b, _range=(0,1))\n",
        "  plot_matrix( b.reshape((1024, 1)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa2uKeMQ4gL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "TOKENS=[]\n",
        "LABELS=[]\n",
        "\n",
        "for a, b in TRAINSET_read:\n",
        "  TOKENS.append(a)\n",
        "\n",
        "  mx = np.zeros( (b.shape[0], 2)  )\n",
        "\n",
        "  mx[:,1] = b\n",
        "  mx[:,0] = 1-b\n",
        "#   plot_matrix(mx)\n",
        "  LABELS.append( mx)\n",
        "  \n",
        "LABELS=np.array(LABELS)\n",
        "                \n",
        "LABELS.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoJVlRBD5NPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from documents import TOKENIZER_DEFAULT\n",
        "from documents import SpmGTokenizer, Tokens\n",
        "\n",
        "\n",
        "tkz = SpmGTokenizer()\n",
        "DICT_SIZE=tkz.sp.get_piece_size()\n",
        "DICT_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTCmEpUtefEx",
        "colab_type": "text"
      },
      "source": [
        "### –ù–∞–±–æ—Ä –¥–ª—è  –æ—Ä–≥–∞–Ω–æ–ª–µ–ø—Ç–∏—á–µ—Å–∫–æ–π (–≥–ª–∞–∑–æ-—Ä—É—á–Ω–æ–π) –æ—Ü–µ–Ω–∫–∏ üò±—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
        "–≠—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –º–æ–¥–µ–ª—å (–º–∞–¥–∞–º) –Ω–µ –¥–æ–ª–∂–Ω–∞ –≤–∏–¥–µ—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvFnuSWrbdDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install rstr\n",
        "\n",
        "import glob\n",
        "from contract_parser import ContractDocument3\n",
        "from trainset_builder import add_padding_to_max\n",
        "\n",
        "\n",
        "def read_documents(filename_prefix):\n",
        "  texts = {}\n",
        "  for file in glob.glob(filename_prefix+\"*.doc\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.doc FILE!!', file)\n",
        "\n",
        "  for file in glob.glob(filename_prefix+\"*.docx\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.docx FILE!!', file)\n",
        "      \n",
        "  return texts\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "contracts = read_documents(contracts_filename_prefix)\n",
        "\n",
        "\n",
        "assert len(contracts) > 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DOCS=[]\n",
        "for c in contracts.values():\n",
        "  doc = ContractDocument3(c)\n",
        "  doc.parse()\n",
        "  DOCS.append(doc)\n",
        "#   print(doc.get_len())\n",
        "  \n",
        "def prepare_validation_data(docs):\n",
        "  tokenss=[]\n",
        "  for doc in docs:\n",
        "    tokenss.append(doc.tokens_cc)\n",
        "\n",
        "  tokenss, lens, _ = add_padding_to_max(tokenss)\n",
        "  return  tokenss, lens \n",
        "\n",
        "\n",
        "VAL_TOKENS, VAL_LENS = prepare_validation_data(DOCS)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G10qzXLlxvL7",
        "colab_type": "text"
      },
      "source": [
        "# MODELS (120 EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-wza-rhYkNE",
        "colab_type": "text"
      },
      "source": [
        "## init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7q4jKnnfoCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from keras.activations import relu\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Concatenate, Input, Flatten, Conv1D, MaxPooling1D, Lambda, Add, BatchNormalization\n",
        "from keras.layers import Dropout, concatenate\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from IPython.display import SVG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVoctjjxshst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 120\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qCs8t3fXw_",
        "colab_type": "text"
      },
      "source": [
        "####  support  methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUV6l725oU7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_training_history(history):\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "HISTORIES={}\n",
        "\n",
        "def plot_compare_models():\n",
        "  for key in HISTORIES:\n",
        "    history=HISTORIES[key]\n",
        "#     plt.plot(history.history['loss'], label=f'{key} loss')\n",
        "    plt.plot(history.history['val_loss'], label=f'{key} val_loss')\n",
        "  \n",
        "  plt.title('Models loss')   \n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwidnmibS0ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback \n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.objectives import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM, Flatten\n",
        "\n",
        "model_checkpoint_path=\"/content/gdrive/My Drive/Colab Notebooks/checkpoints/\"\n",
        "from documents import TOKENIZER_DEFAULT\n",
        "\n",
        "\n",
        "def custom_categorical_crossentropy(x, y):\n",
        "    x = K.flatten(x)\n",
        "    y = K.flatten(y)\n",
        "    return categorical_crossentropy(x, y)\n",
        "\n",
        "\n",
        "def to_hashes(tokens):\n",
        "  return np.array( [TOKENIZER_DEFAULT.sp.piece_to_id(w) for w in tokens], dtype=np.int32)\n",
        "\n",
        " \n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We8lKK8LBPxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫=True\n",
        "–ù–µ–°–µ–π—á–∞—Å=False\n",
        "\n",
        "\n",
        "def validate_model_4(model, _LABELS_TRAIN,  train=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫, predict_unseen=–ù–µ–°–µ–π—á–∞—Å):\n",
        "  demo_set = 8\n",
        "  \n",
        "  texts = np.array([ to_hashes(t) for t in TOKENS]) \n",
        "  val_texts = np.array([ to_hashes(t) for t in VAL_TOKENS], dtype=np.int32) \n",
        "   \n",
        "  \n",
        " \n",
        "  try:\n",
        "    model = load_model(model_checkpoint_path+model.name)\n",
        "    print('==== LOADED: ', model_checkpoint_path+model.name)\n",
        "  except:\n",
        "    print('cannot load ', model_checkpoint_path+model.name)\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, min_lr=1E-6, verbose=1)\n",
        "  checkpoint = ModelCheckpoint(model_checkpoint_path+model.name, monitor='val_loss', verbose=1, save_best_only=True, mode='min') \n",
        "\n",
        "  if train:\n",
        "    history = model.fit(texts[demo_set:], _LABELS_TRAIN[demo_set:], validation_split=0.25, epochs=EPOCHS, batch_size=BATCH_SIZE,  callbacks=[reduce_lr, checkpoint])\n",
        "\n",
        "\n",
        "    HISTORIES[model.name]=history\n",
        "    plot_training_history(history)\n",
        "    plot_compare_models()\n",
        "\n",
        "  if predict_unseen:\n",
        "    pred = model.predict(val_texts[3:7])  \n",
        "    _val_texts=VAL_TOKENS\n",
        "  else:\n",
        "    pred = model.predict( texts[0:demo_set])\n",
        "    _val_texts=TOKENS\n",
        "    \n",
        "  for p in range(len(pred)):\n",
        "    print(\"*\"*150)\n",
        " \n",
        "    plot_matrix(pred[p])\n",
        "    color_matrix(pred[p], _val_texts[p])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57S2izbCorfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " raise 'stop here please, it is going to train all models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ghkcbyjWC8iF"
      },
      "source": [
        "## 4: ü•®‚òπÔ∏è Detect structure:  Emb:  3000 x 8;  val_acc: 0.9745\n",
        "- ### loss: loss: 0.0768 - acc: 0.9754 - val_loss: 0.0804 - val_acc: 0.9745\n",
        "- –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–æ–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–∫ —Å–µ–±–µ. –°–æ–≤—Å–µ–º –Ω–∏–∫–∞–∫ \n",
        "- –ø–æ—Å–ª–µ 120 —ç–ø–æ—Ö –µ—Å—Ç—å –ø–æ—Ç–µ—Ç–µ–Ω—Ü–∏–∞–ª –∫ –æ–±—É—á–µ–Ω–∏—é, -- –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ –ø–ª–∞—Ç–æ\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2eWXAlxDtHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cp_name=\"residual-inputs B wordpieces 2 X bi-GRU emb=8x2000 SpatialDropout1D=0.05 l=10\"\n",
        "# base_model = load_model(model_checkpoint_path + cp_name)\n",
        "# base_model.summary()\n",
        "from keras.layers import GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Conv1D, Concatenate, SpatialDropout1D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "EMB=8\n",
        "\n",
        "def build_structure_detect_model (features): \n",
        "  _dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(DICT_SIZE, EMB))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "\n",
        "  model.add( Conv1D( filters = features*4, kernel_size=(3), padding='same', activation='relu' ))\n",
        "#  model.add( Conv1D( filters = features, kernel_size=(3), padding='same', activation='relu' ))\n",
        "#   model.add( Bidirectional (GRU(features, return_sequences=True)))\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        " \n",
        "  model.add( TimeDistributed(Dense(features, activation='softmax')))\n",
        " \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  model.name=f'structure_detector2 bi-GRU emb={EMB}x{DICT_SIZE} dropout={_dropout} l={len(model.layers)}'\n",
        "  return model\n",
        "\n",
        " \n",
        "model = build_structure_detect_model( LABELS.shape[-1] )\n",
        "model.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lME0-dk9EdLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVG(model_to_dot(model, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ayRc_HGUC8ia",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# validate_model_4(model, LABELS, train=–ù–µ–°–µ–π—á–∞—Å, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gLGEjKDlC8ih"
      },
      "source": [
        "### Train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5vFvgNW8C8ii",
        "outputId": "ccec8aa0-9015-4bda-a53e-17123acc9e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "EPOCHS=120\n",
        "BATCH_SIZE=762\n",
        "\n",
        "validate_model_4(model, LABELS, train=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ch0yAjrKpyGG"
      },
      "source": [
        "## 7: ü•®Detect structure: binary_crossentropy\n",
        "- ### loss: 0.0689 - acc: 0.9787 - val_loss: 0.0690 - val_acc: 0.9779\n",
        "- –µ—Å—Ç—å –ø–æ—Ç–∞–Ω—Ü–µ–≤–∞–ª –∫ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é \n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pUrnuJ-2w4Ip",
        "colab": {}
      },
      "source": [
        "# cp_name=\"residual-inputs B wordpieces 2 X bi-GRU emb=8x2000 SpatialDropout1D=0.05 l=10\"\n",
        "# base_model = load_model(model_checkpoint_path + cp_name)\n",
        "# base_model.summary()\n",
        "from keras.layers import GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Conv1D, Concatenate, SpatialDropout1D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "EMB=8\n",
        "\n",
        "def build_structure_detect_model (features): \n",
        "  _dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(DICT_SIZE, EMB))\n",
        "  \n",
        "\n",
        "  model.add( Conv1D( filters = features*4, kernel_size=(5), padding='same', activation='relu' ))\n",
        "#  model.add( Conv1D( filters = features, kernel_size=(3), padding='same', activation='relu' ))\n",
        "#   model.add( Bidirectional (GRU(features, return_sequences=True)))\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        " \n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  model.add( TimeDistributed(Dense(features, activation='softmax')))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  model.name=f'structure_detector-7 bi-GRU emb={EMB}x{DICT_SIZE} dropout={_dropout} l={len(model.layers)}'\n",
        "  return model\n",
        "\n",
        " \n",
        "model7 = build_structure_detect_model( LABELS.shape[-1] )\n",
        "model7.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c94b1cc2-069f-46cc-b2b9-301ee49179fe",
        "id": "0G3O2dsLw4IQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=120\n",
        "BATCH_SIZE=762\n",
        "\n",
        "validate_model_4(model7, LABELS, train=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e2BkMulqQxJu"
      },
      "source": [
        "## 8: ü•®binary_crossentropy, 3 bi-GRU, dropouts, CNN 5x8, EMB=8x3000\n",
        "- ### loss 240 epochs: loss: 0.0304 - acc: 0.9903 - val_loss: 0.0455 - val_acc: 0.9858\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oO2HPnImQxJ1",
        "colab": {}
      },
      "source": [
        "# cp_name=\"residual-inputs B wordpieces 2 X bi-GRU emb=8x2000 SpatialDropout1D=0.05 l=10\"\n",
        "# base_model = load_model(model_checkpoint_path + cp_name)\n",
        "# base_model.summary()\n",
        "from keras.layers import GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Conv1D, Concatenate, SpatialDropout1D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "EMB=8\n",
        "\n",
        "def build_structure_detect_model (features): \n",
        "  _dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(DICT_SIZE, EMB))\n",
        "  \n",
        "\n",
        "  model.add( Conv1D( filters = features*4, kernel_size=(5), padding='same', activation='relu' ))\n",
        "#  model.add( Conv1D( filters = features, kernel_size=(3), padding='same', activation='relu' ))\n",
        "#   model.add( Bidirectional (GRU(features, return_sequences=True)))\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "#   model.add(SpatialDropout1D(_dropout) )\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        " \n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  model.add( TimeDistributed(Dense(features, activation='softmax')))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  model.name=f'structure_detector-8 3 bi-GRU emb={EMB}x{DICT_SIZE} dropout={_dropout} l={len(model.layers)}'\n",
        "  return model\n",
        "\n",
        " \n",
        "model8 = build_structure_detect_model( LABELS.shape[-1] )\n",
        "model8.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "429880a9-56d2-48c1-c2d0-94836b2bae4d",
        "id": "kb8SomI0QxJ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=1\n",
        "BATCH_SIZE=1530\n",
        "\n",
        "validate_model_4(model8, LABELS, train=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lptA6gdN0kiG"
      },
      "source": [
        "## 9: ü•®binary_crossentropy, 4 bi-GRU, dropouts, CNN 3x8, EMB=10x3000\n",
        "- ### Total params: 34,698\n",
        "- ### loss 120 epochs: loss: 0.2610 - acc: 0.8688 - val_loss: 0.2578 - val_acc: 0.8748\n",
        "- ### loss 240 epochs: loss: 0.1941 - acc: 0.9187 - val_loss: 0.1933 - val_acc: 0.9208\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqXcWBKR0kiJ",
        "colab": {}
      },
      "source": [
        "# cp_name=\"residual-inputs B wordpieces 2 X bi-GRU emb=8x2000 SpatialDropout1D=0.05 l=10\"\n",
        "# base_model = load_model(model_checkpoint_path + cp_name)\n",
        "# base_model.summary()\n",
        "from keras.layers import GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Conv1D, Concatenate, SpatialDropout1D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "EMB=10\n",
        "\n",
        "def build_structure_detect_model (features): \n",
        "  _dropout = 0.1\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(DICT_SIZE, EMB))\n",
        "  \n",
        "\n",
        "  model.add( Conv1D( filters = features*4, kernel_size=(3), padding='same', activation='relu' ))\n",
        "\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  \n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "  \n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True)))\n",
        "  model.add(SpatialDropout1D(_dropout) )\n",
        "\n",
        "  model.add( Bidirectional (GRU(features*4, return_sequences=True))) \n",
        "  \n",
        "  model.add( TimeDistributed(Dense(features, activation='softmax')))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  model.name=f'structure_detector-9 4 bi-GRU emb={EMB}x{DICT_SIZE} dropout={_dropout} l={len(model.layers)}'\n",
        "  return model\n",
        "\n",
        " \n",
        "model9 = build_structure_detect_model( LABELS.shape[-1] )\n",
        "model9.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1N9TQJcL0kiR",
        "colab": {}
      },
      "source": [
        "EPOCHS=120\n",
        "BATCH_SIZE=1530\n",
        "\n",
        "validate_model_4(model9, LABELS, train=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nR9cnbYg-6xu"
      },
      "source": [
        "## 0: Baseline model (12K weights)\n",
        "- ### 120 epochs : loss: 0.1623 - acc: 0.9495 - val_loss: 0.1659 - val_acc: 0.9494\n",
        "- ### 250 epochs : loss: 0.0748 - acc: 0.9750 - val_loss: 0.0839 - val_acc: 0.9719\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UorSOvdR-6xy",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.layers import GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Conv1D, Concatenate, SpatialDropout1D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "EMB=4\n",
        "\n",
        "def build_structure_detect_model (features): \n",
        "  \n",
        "   \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(DICT_SIZE, EMB))\n",
        "  \n",
        "\n",
        "  model.add( Conv1D( filters = 6, kernel_size=(5), padding='same', activation='relu' ))\n",
        "  model.add( Bidirectional (GRU(6, return_sequences=True)))\n",
        "  \n",
        "  model.add( TimeDistributed(Dense(features, activation='softmax')))\n",
        " \n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  model.name=f'structure_detector-0 baseline emb={EMB}x{DICT_SIZE} l={len(model.layers)}'\n",
        "  return model\n",
        "\n",
        " \n",
        "model0 = build_structure_detect_model( LABELS.shape[-1] )\n",
        "model0.name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5zaNkb1_dKj",
        "colab_type": "code",
        "outputId": "93475a48-a9b8-4f39-a02b-62dc0260a5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=1200\n",
        "BATCH_SIZE=762\n",
        "\n",
        "validate_model_4(model0, LABELS, train=False, predict_unseen=–í–æ–∏—Å—Ç–∏–Ω—É–¢–∞–∫)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}