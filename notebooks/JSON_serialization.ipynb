{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JSON serialization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/protocols-parser/notebooks/JSON_serialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxw4t56_uSZ",
        "colab_type": "text"
      },
      "source": [
        "#JSON serialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "–ì—Ä–µ–±–∞–Ω–æ–µ–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"protocols-parser\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "def read_doc(fn):\n",
        "  import docx2txt, sys, os\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    text = docx2txt.process(fn)\n",
        "\n",
        "  except:\n",
        "    print(\"Unexpected error:\", sys.exc_info())\n",
        "    os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "    with open(fn + '.txt') as f:\n",
        "      text = f.read()\n",
        "\n",
        "  return text\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyxetjX_-yr",
        "colab_type": "text"
      },
      "source": [
        "## Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tGCs9IjyznUk",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        " \n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zYVejKzZznUk"
      },
      "source": [
        "### Read a doc from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSYNFuDAQZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import glob\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "doc_x_filenames=sorted(glob.glob(contracts_filename_prefix+\"*.docx\"))\n",
        "\n",
        "_filename = doc_x_filenames[3]\n",
        "print(_filename)\n",
        "\n",
        "contracts = {}\n",
        "try:\n",
        "  text = read_doc(_filename)\n",
        "  contracts[_filename] = text\n",
        "except:\n",
        "  print('WRONG *.docx FILE!!', file)\n",
        "  raise 'cannot continue blah blah'\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0jICoNUOcjV",
        "colab_type": "text"
      },
      "source": [
        "### Contract: parse and serialize  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45A2wSkmODqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "from contract_parser import ContractDocument3\n",
        "from text_tools import untokenize\n",
        "\n",
        "\n",
        "class ContractDocument4(ContractDocument3):\n",
        "  def __init__(self, contract_text):\n",
        "    super().__init__(contract_text)\n",
        "\n",
        "  def to_json(self) -> str:\n",
        "    \"\"\"\n",
        "    XXX: move this to GitHub code\n",
        "    \"\"\"\n",
        "    j = ContractDocumentJson(self)\n",
        "    return json.dumps(j.__dict__, indent=4, ensure_ascii=False, default=lambda o: '<not serializable>')\n",
        "\n",
        "\n",
        "class ContractDocumentJson:\n",
        "\n",
        "  def from_json(jsondata):\n",
        "    c = ContractDocumentJson(None)\n",
        "    c.__dict__ = jsondata\n",
        "    return c\n",
        "\n",
        "  def __init__(self, doc: ContractDocument4):\n",
        "    self.ID = –ì—Ä–µ–±–∞–Ω–æ–µ–ù–∏—á—Ç–æ\n",
        "    self.filename = –ì—Ä–µ–±–∞–Ω–æ–µ–ù–∏—á—Ç–æ\n",
        "    self.original_text = –ì—Ä–µ–±–∞–Ω–æ–µ–ù–∏—á—Ç–æ\n",
        "    self.normal_text = –ì—Ä–µ–±–∞–Ω–æ–µ–ù–∏—á—Ç–æ\n",
        "\n",
        "    self.import_timestamp = time.time()\n",
        "    self.analyze_timestamp = time.time()\n",
        "\n",
        "    if doc is None:\n",
        "      return\n",
        "\n",
        "    self.checksum = hash(doc.normal_text)\n",
        "\n",
        "    self.tags = []\n",
        "    self.tokenization_maps = {}\n",
        "\n",
        "    self.tokenization_maps['$words'] = doc.tokens_map.map\n",
        "\n",
        "    for field in doc.__dict__:\n",
        "      if field in self.__dict__:\n",
        "        self.__dict__[field] = doc.__dict__[field]\n",
        "\n",
        "    for hi in doc.structure.headline_indexes:\n",
        "      s = doc.structure.structure[hi]\n",
        "      _t = {\n",
        "        \"kind\": \"headline\",\n",
        "        \"value\": untokenize(doc.tokens_cc[slice(s.span[0], s.span[1])]),\n",
        "        \"span\": [\n",
        "          s.span[0],\n",
        "          s.span[1]\n",
        "        ],\n",
        "        \"span_map\": '$words'\n",
        "      }\n",
        "\n",
        "      self.tags.append(_t)\n",
        "\n",
        "    # self._agent_infos_to_tags(doc)\n",
        "    for tag in doc.agents_tags:\n",
        "      self.tags.append(tag.__dict__)\n",
        "\n",
        "\n",
        "### test    ===============\n",
        "contract_text = contracts[_filename]\n",
        "doc = ContractDocument4(contract_text)\n",
        "doc.filename = _filename\n",
        "doc.parse()\n",
        "\n",
        "## WRITE IT\n",
        "_path = 'contract_info_almost_real.json'\n",
        "with open(_path, 'w') as file:\n",
        "  file.write(doc.to_json())\n",
        "  print(f'saved file to {_path}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFJxWFlJMZb",
        "colab_type": "text"
      },
      "source": [
        "#### read json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-wuxzNJL0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## RE√ÖD it\n",
        "\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def text_range_by_map(text, _map, span):\n",
        "  start = _map[span[0]][0]\n",
        "  stop = _map[span[1]][1]\n",
        "  # assume map is ordered\n",
        "  return text[start: stop]\n",
        "\n",
        "\n",
        "def tokens_in_range(text, _map, span):\n",
        "  tokens_i = _map[span[0]:span[1]]\n",
        "  return [\n",
        "    text[tr[0]:tr[1]] for tr in tokens_i\n",
        "  ]\n",
        "\n",
        "\n",
        "with open(_path) as json_file:\n",
        "  data = json.load(json_file)\n",
        "  cd = ContractDocumentJson.from_json(data)\n",
        "\n",
        "  wordsmap = cd.tokenization_maps['$words']\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        "  tokens = tokens_in_range(cd.normal_text, wordsmap, [0, len(wordsmap)])\n",
        "\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  #   print(len(cd.normal_text))\n",
        "  print('Listing all tags:', '*' * 80)\n",
        "  for tag in cd.tags:\n",
        "    span = tag['span']\n",
        "    _map = cd.tokenization_maps[tag['span_map']]\n",
        "    print(tag['kind'], tokens_in_range(cd.normal_text, _map, span))\n",
        "    markup_vector[span[0]:span[1]] += 1\n",
        "    \n",
        "    \n",
        "  markup_vector[0:500]+=1\n",
        "  GLOBALS__['renderer'].render_color_text(tokens, markup_vector)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}