{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev Protocols.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "2SBxBrBkAzGt",
        "MlhlHnoYQyqM",
        "q58BgdvHanlN",
        "Ipr_V7-HsUhm",
        "OQFmD7SV3yV4",
        "RsdMcThYjWTJ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEjJV3Elb8t",
        "colab_type": "text"
      },
      "source": [
        "## pull code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFNIoGZL198",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_git_branch = 'protocols-4'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from documents import TextMap\n",
        "from colab_support.renderer import HtmlRenderer\n",
        "from legal_docs import DocumentJson\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range, separator=separator)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mlpzzdNAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyjarowinkler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lib_version = '1.1.9'\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmYb03dFcJ9",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fbSX2h2MedM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from legal_docs import LegalDocument\n",
        " \n",
        "from ml_tools import *\n",
        "\n",
        "# from headers_detector import doc_features, load_model, make_headline_attention_vector\n",
        "from hyperparams import HyperParameters\n",
        "from protocol_parser import protocol_votes_re\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCveVZqvNhsr",
        "colab_type": "text"
      },
      "source": [
        "## üíÖ Init Embedder(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3k194xUFy20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from protocol_parser import ProtocolAnlysingContext, ProtocolPatternFactory\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "# from contract_patterns import ContractPatternFactory\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        "\n",
        "protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "# contracts_factory = ContractPatternFactory(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GfkasHwTw6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#TODO: move to git hb\n",
        "def calc_distances_to_pattern(sentences_embeddings_, pattern_embedding, dist_func=distance.cosine):\n",
        "  assert len(pattern_embedding.shape) == 1\n",
        "  assert len(sentences_embeddings_.shape) == 2\n",
        "  assert sentences_embeddings_.shape[1] == pattern_embedding.shape[0] \n",
        "\n",
        "  _distances = np.ones(len(sentences_embeddings_))\n",
        "  for word_index in range(0, len(sentences_embeddings_)):\n",
        "    _distances[word_index] = max(0, min(1, 1.0 - dist_func(sentences_embeddings_[word_index], pattern_embedding)))\n",
        "\n",
        "  return _distances\n",
        "\n",
        "def calc_distances_per_pattern_dict( sentences_embeddings_, patterns_dict, patterns_embeddings):\n",
        "  \n",
        "  # TODO: see https://keras.io/layers/merge/#dot\n",
        "\n",
        "  distances_per_pattern_dict = {}\n",
        "  for i in range(len(patterns_dict)):\n",
        "    pattern = patterns_dict[i]\n",
        "    _distances = calc_distances_to_pattern(sentences_embeddings_, patterns_embeddings[i])\n",
        "\n",
        "    distances_per_pattern_dict[pattern[0]] = _distances\n",
        "  return distances_per_pattern_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SBxBrBkAzGt",
        "colab_type": "text"
      },
      "source": [
        "## üë®üèø‚ÄçüöÄ Test sentence embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmJqPEmsDrMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import re\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def plot_similarity(labels, features, rotation):\n",
        "#   corr = np.inner(features, features)\n",
        "  corr = cosine_similarity(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      annot=True, \n",
        "      cmap=\"GnBu\" #\"YlOrRd\"\n",
        "  )\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoTmnoghC8By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages_ = ['–º–∞–º–∞ —Ä–∞–º—É –º—ã–ª–∞', \n",
        "             '–º—ã–ª–∞ –±–æ–¥—Ä–æ —Ä–∞–º—É –º–∞–º–∞', \n",
        "             '–ê–Ω—Ç–æ–Ω —Ö–º—É—Ä–∏–ª –±—Ä–æ–≤–∏', \n",
        "             '–ê–Ω—Ç–æ–Ω —Å–∫–æ—Ä–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç –º–∏—Ä', \n",
        "             '–ê–Ω—Ç–æ–Ω –Ω–µ —Å–∫–æ—Ä–æ –∑–∞—Ö–≤–∞—Ç–∏—Ç –º–∏—Ä', \n",
        "             '–®–ª–∞ –°–∞—à–∞ –º—ã—Ç—å –†–∞–º—É',\n",
        "             '–®–ª–∞ –°–∞—à–∞ –º—ã—Ç—å —Ä–∞–º—É –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–¥—Ä–æ']\n",
        "message_embeddings_ = elmo_embedder_default.embedd_strings(messages_)\n",
        "plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7P7uX1FvzP",
        "colab_type": "text"
      },
      "source": [
        "## Embedd patterns as sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlhlHnoYQyqM",
        "colab_type": "text"
      },
      "source": [
        "### print patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7P_pzKnJOAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patterns = [ ' '.join(p.prefix_pattern_suffix_tuple) for p in protocols_factory.patterns]\n",
        "patterns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmwED6axQamQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in  protocols_factory.patterns:\n",
        "  pn = '===='.join(p.prefix_pattern_suffix_tuple)\n",
        "  print(p.name, p.prefix_pattern_suffix_tuple[1], pn)\n",
        "  print('--')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRIF48ks3OV",
        "colab_type": "text"
      },
      "source": [
        "### Read doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVpcnaLgEJ",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title ‚úÖPre-embedded or Upload?\n",
        "upload_files = True #@param {type:\"boolean\"}\n",
        "filename = '/content/\\u041F\\u0440\\u043E\\u0442\\u043E\\u043A\\u043E\\u043B_\\u0413\\u041F\\u041D-\\u041A\\u041F_\\u0417\\u0435\\u043B\\u0435\\u043D\\u044B\\u0435.doc' #@param {type:\"string\"}\n",
        "if not upload_files:\n",
        "  with open('/content/nlp_tools/tests/–ü—Ä–æ—Ç–æ–∫–æ–ª_–°–î_ 3.docx.pickle', 'rb') as handle:\n",
        "    doc = pickle.load(handle)\n",
        "\n",
        "if upload_files:\n",
        "  results = wp.read_doc(filename)\n",
        "  for doc in results['documents'][:1]:  # XXX\n",
        "    if 'PROTOCOL' == doc['documentType']:    \n",
        "      doc = join_paragraphs(doc, 'no_id')\n",
        "\n",
        "\n",
        "from legal_docs import tokenize_doc_into_sentences_map\n",
        "sentence_map = tokenize_doc_into_sentences_map(doc, 250)\n",
        " \n",
        "for token in sentence_map.tokens[0:30]:\n",
        "  print('-->', token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6wYijTKSYMp",
        "colab_type": "text"
      },
      "source": [
        "## define patterns dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXTDRGZDal37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patterns_dict=[   \n",
        "    ['sum_max1', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ –±–æ–ª–µ–µ 0 –º–ª–Ω. —Ç—ã—Å. –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π –¥–æ–ª–ª–∞—Ä–æ–≤ –∫–æ–ø–µ–µ–∫ –µ–≤—Ä–æ'],\n",
        "\n",
        "    # ['solution_1','—Ä–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],\n",
        "    # ['solution_2','–ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –ø—Ä–∏–Ω—è—Ç—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è:'],\n",
        "    \n",
        "    ['not_value_1', '—Ä–∞–∑–º–µ—Ä —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ 0 —Ä—É–±–ª–µ–π'],\n",
        "    ['not_value_2', '–ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è'],\n",
        "   \n",
        "    ['agenda_end_1','–∫–≤–æ—Ä—É–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∑–∞—Å–µ–¥–∞–Ω–∏—è –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏–º–µ–µ—Ç—Å—è'],\n",
        "    ['agenda_end_2','–í–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],\n",
        "    ['agenda_end_3','–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ —Ä–µ—à–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è:'],\n",
        "    \n",
        "    ['agenda_start_1','–ø–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],\n",
        "    ['agenda_start_2','–ü–æ–≤–µ—Å—Ç–∫–∞ –¥–Ω—è'],\n",
        "    \n",
        "    ['deal_approval_1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Å–¥–µ–ª–∫–∏'],\n",
        "    ['deal_approval_1.1', '–æ–¥–æ–±—Ä–∏—Ç—å —Å–¥–µ–ª–∫—É'],\n",
        "    ['deal_approval_2','–¥–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞'],\n",
        "    ['deal_approval_3','–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–¥–µ–ª–∫–∏'],\n",
        "    ['deal_approval_3.1','–ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–æ–≤–µ—Ä—à–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω–æ–π —Å–¥–µ–ª–∫–∏'],\n",
        "    ['deal_approval_4','–∑–∞–∫–ª—é—á–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã'],\n",
        "    \n",
        "    ['question_1','–ü–æ –≤–æ–ø—Ä–æ—Å—É ‚Ññ 0'],\n",
        "    ['question_2', '–ü–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è –∑–∞—Å–µ–¥–∞–Ω–∏—è'],\n",
        "    ['question_3', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],\n",
        "    ['question_4', '–†–µ—à–µ–Ω–∏–µ, –ø—Ä–∏–Ω—è—Ç–æ–µ –ø–æ 1 –≤–æ–ø—Ä–æ—Å—É –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è:'],\n",
        "\n",
        "    ['footers_1', '–í—Ä–µ–º—è –ø–æ–¥–≤–µ–¥–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è'],\n",
        "    ['footers_2', '–°–ø–∏—Å–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:'],\n",
        "    ['footers_3', '–ü–æ–¥—Å—á–µ—Ç –≥–æ–ª–æ—Å–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª –°–µ–∫—Ä–µ—Ç–∞—Ä—å –°–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤'],\n",
        "    ['footers_4', '–ü—Ä–æ—Ç–æ–∫–æ–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ 2-—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö']\n",
        "    \n",
        "    \n",
        "    \n",
        "]\n",
        "\n",
        "patterns_te=[ p[1] for p in patterns_dict ]\n",
        "patterns_embeddings_ = elmo_embedder_default.embedd_strings(patterns_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5eYIEFaP0gA",
        "colab_type": "text"
      },
      "source": [
        "### ‚öôÔ∏èüîÆSENTENCES embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBpMe6g_P3tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_embeddings_ = elmo_embedder_default.embedd_strings(sentence_map.tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7wSkDaBPcFC",
        "colab_type": "text"
      },
      "source": [
        "### ‚öôÔ∏èüîÆ WORDS Ebmedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X4WuM6PYMYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "doc.embedd(protocols_factory)\n",
        "doc.calculate_distances_per_pattern(protocols_factory)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58BgdvHanlN",
        "colab_type": "text"
      },
      "source": [
        "### üë®üèø‚ÄçüöÄTry find subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLXPnhB9Sa75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subject = '''–ø—Ä–æ–¥–∞–∂–µ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–µ–≥–æ –û–±—â–µ—Å—Ç–≤—É –Ω–∞ –ø—Ä–∞–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏'''\n",
        "\n",
        "# subject = '''–¥–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è'''\n",
        "\n",
        "subject = subject[:200]\n",
        "subject_embeddings = elmo_embedder_default.embedd_strings([subject])\n",
        "\n",
        "import scipy.spatial.distance as distance\n",
        "\n",
        "\n",
        "\n",
        "contract_subj_attention = calc_distances_to_pattern(sentences_embeddings_, subject_embeddings[0])\n",
        "contract_subj_attention = relu(contract_subj_attention, 0.5)\n",
        "\n",
        "\n",
        "\n",
        "renderer_.render_color_text(sentence_map.tokens, contract_subj_attention, _range=(0,1), separator='<br>')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIPteR9xPllB",
        "colab_type": "text"
      },
      "source": [
        "## Split text by agenda questions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoQff4M9UTtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_spans_having_votes(segments, textmap):\n",
        "  \"\"\"\n",
        "  search for votes in each document segment\n",
        "  collect only\n",
        "  :param segments: \n",
        "  :param textmap: \n",
        "  :return:  segments with votes\n",
        "  \"\"\"\n",
        "  for span in segments:\n",
        "    # print('=' * 50)\n",
        "    subdoc = textmap.slice(span)\n",
        "    protocol_votes = list(subdoc.finditer(protocol_votes_re))\n",
        "    if protocol_votes:\n",
        "      # print('-' * 50)\n",
        "      # print(subdoc.text)\n",
        "      # for c in protocol_votes:\n",
        "      #   print('found:', subdoc.text_range(c))\n",
        "      yield span\n",
        "    # else:\n",
        "    #   print('not found')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgpcI4fLMsWl",
        "colab_type": "text"
      },
      "source": [
        "### üìù Sections attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31WDPHa5YqDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_protocol_sections_edges(distances_per_pattern_dict):\n",
        "\n",
        "  patterns = ['deal_approval_', 'footers_', 'question_']\n",
        "  vv_=[]\n",
        "  for p in patterns:\n",
        "    v_ = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, p)\n",
        "    v_ = relu(v_, 0.5)\n",
        "    vv_.append(v_)\n",
        "  \n",
        "  v_sections_attention = sum_probabilities(vv_)\n",
        "\n",
        "  # v_deal_approval = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'deal_approval_')\n",
        "  # v_deal_approval = relu(v_deal_approval, 0.5)\n",
        "\n",
        "  # v_question = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'question_')\n",
        "  # v_question = relu(v_question, 0.5)\n",
        "\n",
        "  # v_sections_attention = sum_probabilities([v_deal_approval, v_question])\n",
        "  v_sections_attention = relu(v_sections_attention, 0.7)\n",
        "  return v_sections_attention\n",
        "\n",
        "\n",
        "distances_per_pattern_dict = calc_distances_per_pattern_dict(sentences_embeddings_, patterns_dict, patterns_embeddings_)\n",
        "v_sections_attention = find_protocol_sections_edges(distances_per_pattern_dict)\n",
        "\n",
        "# renderer_.render_color_text(sentence_map.tokens , v_question+v_deal_approval, _range=(0,1), separator='<br>')\n",
        "renderer_.render_color_text(sentence_map.tokens,  v_sections_attention, _range=(0,1), separator='<br>')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W1nF8aLUWrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_spans_sent = spans_between_non_zero_attention(v_sections_attention)\n",
        "print('question_spans_sent', question_spans_sent)\n",
        "\n",
        "question_spans_words =  sentence_map.remap_slices(question_spans_sent,  doc.tokens_map )\n",
        "print('question_spans_words', question_spans_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewa65VrR0i0",
        "colab_type": "text"
      },
      "source": [
        "More attention to spans having votes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUhUfyGPwnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spans_having_votes = list(collect_spans_having_votes(question_spans_sent, sentence_map))\n",
        "\n",
        "\n",
        "spans_having_votes_words =  sentence_map.remap_slices(spans_having_votes,  doc.tokens_map )\n",
        "# questions_attention =  spans_to_attention(question_spans_words, len(doc))\n",
        "votes_attention =  spans_to_attention(spans_having_votes_words, len(doc))\n",
        "\n",
        "# v_deal_approval_words = sentence_map.remap_spans(v_deal_approval,  doc.tokens_map )\n",
        "v_deal_approval = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'deal_approval_')\n",
        "_spans, v_deal_approval_words_attention =  sentences_attention_to_words(v_deal_approval, sentence_map,  doc.tokens_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### debug -------------votes_attention\n",
        "\n",
        "renderer_.render_color_text(doc.tokens_map.tokens , relu(v_deal_approval_words_attention, 0.3)   , _range=(0,1))\n",
        "\n",
        "for span in spans_having_votes:\n",
        "  print(span, '=' * 50)\n",
        "  subdoc = sentence_map.slice(span)\n",
        "  print(subdoc.text)\n",
        "  print(sentence_map.char_range( [span.start, span.stop] ))\n",
        "  print('^' * 50)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURFn02TYCJB",
        "colab_type": "text"
      },
      "source": [
        "### Value attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBPCKXeSpPnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')\n",
        "s_value_attention_vector_neg = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_neg')\n",
        "s_value_attention_vector -=   s_value_attention_vector_neg/3\n",
        "s_value_attention_vector = relu(s_value_attention_vector, 0.25)\n",
        "\n",
        "v_deal_approval_words_attention = relu (v_deal_approval_words_attention, 0.5)\n",
        "value_attention_vector = sum_probabilities ( [s_value_attention_vector, v_deal_approval_words_attention, votes_attention/3.0] )\n",
        "# value_attention_vector = sum_probabilities ( [ v_deal_approval_words_attention, votes_attention/10.0] )\n",
        "value_attention_vector = relu(value_attention_vector, 0.5)\n",
        " \n",
        "renderer_.render_color_text(doc.tokens_map.tokens, value_attention_vector  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuduhrZNgIFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWh7gxN8om2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentences to words\n",
        "from contract_parser import find_value_sign_currency_attention \n",
        "from legal_docs import ContractValue\n",
        "\n",
        "words_spans_having_votes =  sentence_map.remap_slices (spans_having_votes,  doc.tokens_map )\n",
        "\n",
        "\n",
        "#--------------\n",
        "#color things\n",
        "print('spans_having_votes', list(spans_having_votes))\n",
        "print('words_spans_having_votes', list(words_spans_having_votes))\n",
        "\n",
        "#\n",
        "\n",
        "# for span in list(words_spans_having_votes):\n",
        "#   print(span)\n",
        "#   selection [ span[0]:span[1]+1  ] += 0.4\n",
        "#--------------\n",
        "\n",
        "#find contract values\n",
        "# for span in words_spans_having_votes:\n",
        "#   subdoc = doc.subdoc_slice(slice(span[0],span[1]) )\n",
        "values:List[ContractValue] = find_value_sign_currency_attention(doc, value_attention_vector)\n",
        "\n",
        "#color \n",
        "numbers_attention = np.zeros(len(doc.tokens_map))\n",
        "numbers_confidence = np.zeros(len(doc.tokens_map))\n",
        "for v in values:\n",
        "  numbers_confidence [ v.value.as_slice() ] +=v.value.confidence\n",
        "  numbers_attention  [ v.value.as_slice() ] = 1\n",
        "  numbers_attention  [ v.currency.as_slice() ] = 1\n",
        "  numbers_attention  [ v.sign.as_slice() ] = 1\n",
        "\n",
        "block_confidence = sum_probabilities ( [numbers_attention, v_deal_approval_words_attention, votes_attention/5] )\n",
        "\n",
        "# value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, '_phrase')\n",
        "renderer_.render_color_text(doc.tokens_map.tokens, numbers_attention, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JdE_vopgImk",
        "colab_type": "text"
      },
      "source": [
        "## Collect only meaningful sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozb9qbYQqvL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block_confidence = sum_probabilities ( [ numbers_attention, value_attention_vector, v_deal_approval_words_attention] )\n",
        "\n",
        "renderer_.render_color_text(doc.tokens_map.tokens, block_confidence, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGhXXSFZbWDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_confident_spans(slices, block_confidence, tag_name):\n",
        "  k = 0\n",
        "  for _slice in slices:\n",
        "    k+=1\n",
        "    pv = block_confidence[_slice[0]:_slice[1]]\n",
        "    confidence = estimate_confidence_by_mean_top_non_zeros(pv, 5)\n",
        "    print('-' * 100)\n",
        "    print(_slice, confidence)\n",
        "    if confidence > 0.6:\n",
        "      st = SemanticTag(f\"{tag_name}_{k}\", None, _slice)\n",
        "      st.confidence = confidence\n",
        "      yield (st)\n",
        "\n",
        "\n",
        "tags = find_confident_spans(question_spans_words, block_confidence, 'agenda_item' )\n",
        "for tag in tags:\n",
        "  print(tag.kind)\n",
        "  print(doc.substr(tag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS4gLoAuAZxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QknxIdlTZoN9",
        "colab_type": "text"
      },
      "source": [
        "# MISCL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYfcp9PZBc_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_agenda(distances_per_pattern_dict):\n",
        "  v1 = max_exclusive_pattern_by_prefix(distances_per_pattern_dict,  'agenda_start_' )\n",
        "  agenda_index_start = np.argmax(v1)\n",
        "  # v =max_exclusive_pattern_by_prefix( [distances_per_pattern_dict['deal_approval_1'], distances_per_pattern_dict['deal_approval_2']]  )\n",
        "  v2 = max_exclusive_pattern_by_prefix(distances_per_pattern_dict,  'agenda_end_' )\n",
        "  agenda_index_end = np.argmax(v2)\n",
        "\n",
        "  v2[:agenda_index_start] = 0\n",
        "  v2[agenda_index_end:] = 0\n",
        "\n",
        "\n",
        "  return None, v2\n",
        "  pass\n",
        "\n",
        "questions, attention_v = detect_agenda(distances_per_pattern_dict)\n",
        "print(questions)\n",
        "renderer_.render_color_text(spans_map.tokens , attention_v, _range=(0,1), separator='<br>')\n",
        "# plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYO5x6zL87eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc[0:4].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjLz5-wPt8hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from contract_agents import find_org_names\n",
        "from typing import List\n",
        "\n",
        "def find_protocol_org(protocol)->List[SemanticTag]:\n",
        "  ret=[]\n",
        "  x:List[SemanticTag] = find_org_names(protocol[0:200])\n",
        "  ret.append( SemanticTag.find_by_kind(x, 'org.1.name'))\n",
        "  ret.append( SemanticTag.find_by_kind(x, 'org.1.type'))\n",
        "  return x\n",
        "\n",
        "x=find_protocol_org(doc)\n",
        "for c in x:\n",
        "  print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtq7PaJ9ism",
        "colab_type": "text"
      },
      "source": [
        "## topic_attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u02xJbnV8lNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_attention = make_topic_attention(doc, headline_attention_vector, 'p_agenda_')\n",
        "topic_attention = relu(topic_attention, 0.5)\n",
        "# renderer_.render_color_text(doc.tokens_cc, topic_attention)\n",
        "\n",
        "def mark_section(topic_attention:FixedVector) -> FixedVector:\n",
        "  section_paint = np.zeros_like(topic_attention)\n",
        "  start = np.argmax(topic_attention)\n",
        "  start_v=topic_attention[start]\n",
        "  section_paint[start:] = start_v\n",
        "\n",
        "  return section_paint\n",
        "\n",
        "\n",
        "  print(start)\n",
        "\n",
        "_sp = mark_section(topic_attention)\n",
        "renderer_.render_color_text(doc.tokens_cc, _sp, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2ZGKgpdjJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "all_headlines_indices = all_headlines_spans[:,0]\n",
        "\n",
        "# ---\n",
        "def find_sections_spans_by_indices_2(_doc: LegalDocument,\n",
        "                                   headline_pattern_names,\n",
        "                                   all_headlines_indices,\n",
        "                                   headlines_mask,\n",
        "                                   topic,\n",
        "                                   relu_threshold=0.6):\n",
        "  av_name = f'{topic}_headlines_attention_vector'\n",
        "  print(\"TOPIC:\", av_name)\n",
        "  pv = _doc.distances_per_pattern_dict\n",
        "  pv[av_name] = relu(combined_attention_vectors(pv, headline_pattern_names), relu_threshold)\n",
        "\n",
        "  # masking\n",
        "  pv[av_name] *= headlines_mask\n",
        "\n",
        "  _headlines_of_interest_spans = find_sentences_with_attention(_doc.tokens_map, pv[av_name])\n",
        "  _headlines_of_interest_indices = _headlines_of_interest_spans[:, 0]\n",
        "\n",
        "  for i in _headlines_of_interest_indices:\n",
        "    last = find_first_gt(i, all_headlines_indices)\n",
        "    if last is None:\n",
        "      last = len(_doc.tokens_map) - 1\n",
        "    yield [i, last]\n",
        "  pass\n",
        "\n",
        "\n",
        "##==========\n",
        "solution_spans = find_sections_spans_by_indices_2 (\n",
        "    doc, solution_pattern_names, \n",
        "    all_headlines_indices,\n",
        "    headline_attention_vector_relu, \n",
        "    topic='solutions', \n",
        "    relu_threshold=0.6)\n",
        "\n",
        "agenda_spans   = find_sections_spans_by_indices_2 (\n",
        "    doc, agenda_pattern_names, \n",
        "    all_headlines_indices, \n",
        "    headline_attention_vector_relu, \n",
        "    topic='agenda', \n",
        "    relu_threshold=0.6)\n",
        "\n",
        "_agenda_hd_v = doc.distances_per_pattern_dict['agenda_headlines_attention_vector']\n",
        "_solutions_hd_v = doc.distances_per_pattern_dict['solutions_headlines_attention_vector']\n",
        "\n",
        "for agenda_span in agenda_spans:\n",
        "  print('agenda_span', agenda_span)\n",
        "  # print(doc.tokens_map.text_range(agenda_span))\n",
        "  _slice = slice(agenda_span[0], agenda_span[1])\n",
        "  renderer_.render_color_text(doc.tokens_map [_slice ], _filtered_headline_attention_vector[_slice])\n",
        "\n",
        "print('*'*100)\n",
        "\n",
        "print('='*100)\n",
        "for solution_span in solution_spans:\n",
        "  print(':::::: solution_span', solution_span)\n",
        "  print('- '*50)\n",
        "  # print('‚ò¢Ô∏è ', doc.tokens_map.text_range(solution_span))\n",
        "  _slice = slice(solution_span[0],solution_span[1])\n",
        "  renderer_.render_color_text(doc.tokens_map [_slice ], _filtered_headline_attention_vector[_slice])\n",
        "  print('-'*100)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYMTP4UM09BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(doc.distances_per_pattern_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AOv_WQjxvld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(doc.tokens_cc, _solutions_hd_v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipr_V7-HsUhm",
        "colab_type": "text"
      },
      "source": [
        "# Test on uploaded file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaQks4R4wKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF47cwJDsakq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "\n",
        "document_parser_v='1.0.9'\n",
        "\n",
        "!wget https://github.com/nemoware/document-parser/releases/download/$document_parser_v/document-parser-$document_parser_v-distribution.zip\n",
        "!unzip document-parser-$document_parser_v-distribution.zip  \n",
        "os.environ ['documentparser']=f'/content/document-parser-{document_parser_v}'\n",
        "\n",
        "wordDocParser = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37317MlAd34u",
        "colab_type": "text"
      },
      "source": [
        "## read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYzmTDl8sxC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_res = wordDocParser.read_doc('/content/–ü—Ä–æ—Ç–æ–∫–æ–ª_–û–û–û –í–µ–∫—Ç–æ—Ä –ø—Ä–æ–¥–∞–∂–∞ –Ω–µ–¥–≤–∏–∂.doc')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv_DXzt7volx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from headers_detector import make_headline_attention_vector\n",
        "from integration.word_document_parser import join_paragraphs\n",
        "from ml_tools import find_non_zero_spans, merge_colliding_spans, relu\n",
        "\n",
        "\n",
        "def analyse_protocol(json_res, ctx):\n",
        "  #embedding, etc\n",
        "  protocol = join_paragraphs(json_res, '0')\n",
        "  protocol.embedd_tokens(ctx.protocols_factory.embedder)\n",
        "  #TODO: tetect agents\n",
        "  vd = protocol.calculate_distances_per_pattern(ctx.protocols_factory, verbosity=0)\n",
        "  \n",
        "  vd['headline_attention_vector'] = make_headline_attention_vector(protocol)\n",
        "  vd['headline_attention_vector_relu'] = relu(vd['headline_attention_vector'], HyperParameters.parser_headline_attention_vector_denominator)\n",
        "\n",
        "  all_headlines_spans = find_sentences_with_attention(protocol.tokens_map, vd['headline_attention_vector_relu'])\n",
        "  \n",
        "  filtered_all_headlines_spans = merge_colliding_spans(all_headlines_spans)\n",
        "  all_headlines_indices = filtered_all_headlines_spans[:,0]\n",
        "\n",
        "  # test\n",
        "  vd['filtered_headline_attention_vector'] = np.zeros_like(vd['headline_attention_vector_relu'])\n",
        "  for s in filtered_all_headlines_spans:\n",
        "    vd['filtered_headline_attention_vector'][s[0]:s[1]] += 1\n",
        "\n",
        "  return protocol, all_headlines_indices\n",
        "\n",
        "\n",
        "protocol, all_headlines_indices = analyse_protocol(json_res, ctx)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WorYmB9MjS",
        "colab_type": "text"
      },
      "source": [
        "## Solutions (—Ä–µ—à–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPRfImpzji2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "solution_spans = find_sections_spans_by_indices (protocol, \n",
        "                                                 solution_pattern_names, \n",
        "                                                 all_headlines_indices,\n",
        "                                                 protocol.distances_per_pattern_dict['headline_attention_vector_relu'], \n",
        "                                                 'solution',\n",
        "                                                  relu_threshold=0.75)\n",
        "\n",
        "\n",
        "print('='*100)\n",
        "for solution_span in solution_spans:\n",
        "  print(':::::: solution_span', solution_span)\n",
        "  print('- '*50)\n",
        "  print('‚ò¢Ô∏è ', protocol.tokens_map.text_range(solution_span))\n",
        "  print('-'*100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1I5MYLC9Drs",
        "colab_type": "text"
      },
      "source": [
        "## headline_attention_vector \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItgZsXaXzH_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(protocol.tokens_cc, protocol.distances_per_pattern_dict['headline_attention_vector'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_8wBZ7dczCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zGZo5rczsE",
        "colab_type": "text"
      },
      "source": [
        "## headline_attention_vector_relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nGqcsMazEhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(protocol.tokens_cc, protocol.distances_per_pattern_dict['headline_attention_vector_relu'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXFUW8F6b9-K",
        "colab_type": "text"
      },
      "source": [
        "## filtered_headline_attention_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQFmD7SV3yV4",
        "colab_type": "text"
      },
      "source": [
        "# Latent mean (center) sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tOA92qE4UoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TextDocument = LegalDocument #alias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_CuXwXXF-h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from patterns import *\n",
        "\n",
        "def latent_sense(embeddings, wnd_half):\n",
        "   \n",
        "  R = np.zeros(len(embeddings))\n",
        "  for i in range (len(embeddings)):\n",
        "    _start = 0\n",
        "    if i > wnd_half: _start = i-wnd_half\n",
        "    word_emb = embeddings[i:i+1]\n",
        "    window_emb = embeddings[_start : i+wnd_half]   \n",
        "    semantic_similarity =  1- dist_mean_cosine(window_emb, word_emb)\n",
        "    R[i] = semantic_similarity\n",
        "  \n",
        "  return R\n",
        "\n",
        "def render_any(t, v):\n",
        "  renderer_.render_color_text(t, v, _range=(min(v), max(v)) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R3ZVmOG33Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text = \"\"\"\n",
        "# –í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–ø . 25 –ø—É–Ω–∫—Ç 10.3. —Å—Ç–∞—Ç—å–∏ 10 –£—Å—Ç–∞–≤–∞ –û–û–û ¬´ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä ¬´ –ë–∞–∂–µ–Ω ¬ª –ø—Ä–∏–Ω—è—Ç–æ —Å–ª–µ–¥—É—é—â–µ–µ —Ä–µ—à–µ–Ω–∏–µ :\n",
        "# –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏ 17.1 . –ó–∞–∫–æ–Ω–∞ –†–§ –æ—Ç 21-02-1992 ‚Ññ 2395-1 ¬´ –û –Ω–µ–¥—Ä–∞—Ö ¬ª ( —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ–¥—Ä –≤—ã—Å—Ç—É–ø–∞–µ—Ç —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–º –Ω–æ–≤–æ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ , —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º —É—á–∞—Å—Ç–∫–µ –Ω–µ–¥—Ä –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ª–∏—Ü–µ–Ω–∑–∏–µ–π –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–æ–º –Ω–µ–¥—Ä , –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ , –µ—Å–ª–∏ –Ω–æ–≤–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –∏ –µ–º—É –ø–µ—Ä–µ–¥–∞–Ω–æ –∏–º—É—â–µ—Å—Ç–≤–æ , –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ , —É–∫–∞–∑–∞–Ω–Ω–æ–π –≤ –ª–∏—Ü–µ–Ω–∑–∏–∏ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–æ–º –Ω–µ–¥—Ä , –≤ —Ç–æ–º —á–∏—Å–ª–µ –∏–∑ —Å–æ—Å—Ç–∞–≤–∞ –∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –æ–±—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —É—á–∞—Å—Ç–∫–∞ –Ω–µ–¥—Ä , –∞ —Ç–∞–∫–∂–µ –∏–º–µ—é—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è ( –ª–∏—Ü–µ–Ω–∑–∏–∏ ) –Ω–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏–µ –≤–∏–¥–æ–≤ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ , —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–µ–¥—Ä–æ–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º , –∏ –¥–æ–ª—è –ø—Ä–µ–∂–Ω–µ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ–¥—Ä –≤ —É—Å—Ç–∞–≤–Ω–æ–º –∫–∞–ø–∏—Ç–∞–ª–µ –Ω–æ–≤–æ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ –Ω–∞ –º–æ–º–µ–Ω—Ç –ø–µ—Ä–µ—Ö–æ–¥–∞ –ø—Ä–∞–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—á–∞—Å—Ç–∫–æ–º –Ω–µ–¥—Ä —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ –º–µ–Ω–µ–µ –ø–æ–ª–æ–≤–∏–Ω—ã —É—Å—Ç–∞–≤–Ω–æ–≥–æ –∫–∞–ø–∏—Ç–∞–ª–∞ –Ω–æ–≤–æ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞ ) –æ–¥–æ–±—Ä–∏—Ç—å –ø–µ—Ä–µ–¥–∞—á—É –û–û–û ¬´ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä ¬´ –ë–∞–∂–µ–Ω ¬ª –û–±—â–µ—Å—Ç–≤—É —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é ¬´ –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å - –°–∞–ª—ã–º ¬ª , —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–µ—à–µ–Ω–∏—è —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π , –ø—Ä–∞–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–¥—Ä–∞–º–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö :\n",
        "# 1. —É—á–∞—Å—Ç–∫–∞ –°–∞–ª—ã–º—Å–∫–∏–π 3 , —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –≤ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–∫—Ä—É–≥–∞ ‚Äì –Æ–≥—Ä—ã –¢—é–º–µ–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ ( –ª–∏—Ü–µ–Ω–∑–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–¥—Ä–∞–º–∏ –•–ú–ù 03590 –ù–† , –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ 10-04-2019 –≥–æ–¥–∞ , —Å —Ü–µ–ª–µ–≤—ã–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ–º –∏ –≤–∏–¥–∞–º–∏ —Ä–∞–±–æ—Ç ‚Äì –¥–ª—è –≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è , –≤–∫–ª—é—á–∞—é—â–µ–≥–æ –ø–æ–∏—Å–∫–∏ –∏ –æ—Ü–µ–Ω–∫—É –º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏–π –ø–æ–ª–µ–∑–Ω—ã—Ö –∏—Å–∫–æ–ø–∞–µ–º—ã—Ö , —Ä–∞–∑–≤–µ–¥–∫–∏ –∏ –¥–æ–±—ã—á–∏ –ø–æ–ª–µ–∑–Ω—ã—Ö –∏—Å–∫–æ–ø–∞–µ–º—ã—Ö ) ;\n",
        "# 2. —É—á–∞—Å—Ç–∫–∞ –°–∞–ª—ã–º—Å–∫–∏–π 5 , —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –≤ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–∫—Ä—É–≥–∞ ‚Äì –Æ–≥—Ä—ã –¢—é–º–µ–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ ( –ª–∏—Ü–µ–Ω–∑–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–¥—Ä–∞–º–∏ –•–ú–ù 03590 –ù–† –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ 10-04-2019 –≥–æ–¥–∞ , —Å —Ü–µ–ª–µ–≤—ã–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ–º –∏ –≤–∏–¥–∞–º–∏ —Ä–∞–±–æ—Ç ‚Äì –¥–ª—è –≥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è , –≤–∫–ª—é—á–∞—é—â–µ–≥–æ –ø–æ–∏—Å–∫–∏ –∏ –æ—Ü–µ–Ω–∫—É –º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏–π –ø–æ–ª–µ–∑–Ω—ã—Ö –∏—Å–∫–æ–ø–∞–µ–º—ã—Ö , —Ä–∞–∑–≤–µ–¥–∫–∏ –∏ –¥–æ–±—ã—á–∏ –ø–æ–ª–µ–∑–Ω—ã—Ö –∏—Å–∫–æ–ø–∞–µ–º—ã—Ö ) ;\n",
        "# 3. —É—á–∞—Å—Ç–∫–∞ –°–∞–ª—ã–º—Å–∫–∏–π 3 , —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –≤ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ –•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–∫—Ä—É–≥–∞ ‚Äì –Æ–≥—Ä—ã –¢—é–º–µ–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ ( –ª–∏—Ü–µ–Ω–∑–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–¥—Ä–∞–º–∏ –•–ú–ù 20562 –í–≠ , –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ 21-06-2019 –≥–æ–¥–∞ , —Å —Ü–µ–ª–µ–≤—ã–º –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ–º –∏ –≤–∏–¥–∞–º–∏ —Ä–∞–±–æ—Ç ‚Äì –¥–æ–±—ã—á–∞ –ø–æ–¥–∑–µ–º–Ω—ã—Ö –≤–æ–¥ –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ–¥–æ–π –æ–±—ä–µ–∫—Ç–æ–≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ ) .\n",
        "# –ü—Ä–∞–≤–æ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—á–∞—Å—Ç–∫–∞–º–∏ –Ω–µ–¥—Ä –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –ø–µ—Ä–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –ª–∏—Ü–µ–Ω–∑–∏–π –Ω–∞ –ø—Ä–∞–≤–æ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–¥—Ä–∞–º–∏ , –≤—ã–¥–∞–Ω–Ω—ã—Ö –û–û–û ¬´ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä ¬´ –ë–∞–∂–µ–Ω ¬ª , –≤ –ø–æ—Ä—è–¥–∫–µ , —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –ó–∞–∫–æ–Ω–æ–º –†–§ –æ—Ç 21-02-1992 ‚Ññ 2395-1 ¬´ –û –Ω–µ–¥—Ä–∞—Ö ¬ª .\n",
        "# \"\"\"\n",
        " \n",
        "td = doc\n",
        "td.parse()\n",
        "td.embedd_tokens(elmo_embedder)\n",
        "td=doc\n",
        "\n",
        "\n",
        "local_sense = latent_sense(td.embeddings,  7 ) \n",
        "# mean_sense -= min(mean_sense)\n",
        "\n",
        "\n",
        "# R\n",
        "# mean_sense = relu(R, np.mean(R)*1.1)\n",
        "\n",
        "render_any(td.tokens_cc,  local_sense )\n",
        "render_any(td.tokens_cc,  local_sense-mean_sense)\n",
        "\n",
        "\n",
        "# meaninful_tokens = np.nonzero(R)[0:][0]\n",
        "# print('-'*100)\n",
        "# print(f'tokens: {len(td.tokens)} \\t meaninful {len(meaninful_tokens)}')\n",
        "# print('-'*100)\n",
        "# res=[ td.tokens_map.text_range([i, i+1]) for i in meaninful_tokens ]\n",
        "# print(' ' .join(res) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Mq_vt3Y9HR",
        "colab_type": "text"
      },
      "source": [
        "### Latent mean (mean-ing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pwKZ2WQZH3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_sense = latent_sense(td.embeddings,  120 )\n",
        "\n",
        "meaningless = relu (mean_sense, (np.max(mean_sense) + np.mean(mean_sense))/2  )\n",
        "outliers = relu (1.0 - mean_sense,0.8)\n",
        "\n",
        "render_any(td.tokens_cc, meaningless)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpsNnn-Z2J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ascPLyyrZZUz"
      },
      "source": [
        "### Latent local mean (mean-ing) without nonsense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R89jhCkrZZU4",
        "colab": {}
      },
      "source": [
        "mean_local_sense = latent_sense(td.embeddings,  7 )\n",
        "nonsense_subtract = mean_local_sense - meaningless\n",
        "nonsense_subtract = relu(nonsense_subtract, 0.2)\n",
        "render_any(td.tokens_cc, nonsense_subtract )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZYWJucjVwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsdMcThYjWTJ",
        "colab_type": "text"
      },
      "source": [
        "# Pattern search (distance functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKwdjgTji5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pattern = ctx.protocols_factory.create_pattern('test', ('','–ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –û–û–û ¬´ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä ¬´ –ë–∞–∂–µ–Ω ¬ª –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞ —Å –û–±—â–µ—Å—Ç–≤–æ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é ¬´ –†–µ–Ω–æ–≤–∞ –õ–∞–± ¬ª ;',''))\n",
        "test_pattern2 = ctx.protocols_factory.create_pattern('test2', ('','–ü–æ –≤–æ–ø—Ä–æ—Å—É ‚Ññ 7',''))\n",
        "ctx.protocols_factory.embedd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxLgckNdkYIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def per_token_dist_mean_cosine(window, pattern):\n",
        "   \n",
        "#   d=0\n",
        "  \n",
        "#   for p in pattern:\n",
        "#     for v in window:\n",
        "#       d += distance.cosine(p, v)\n",
        "  \n",
        "#   return d / (len(window) * len(pattern))\n",
        "def dist_cosine_to_point( text_emb, pt ):\n",
        "  t_distances = np.zeros(len(text_emb))\n",
        "  for i in range(len(text_emb)):\n",
        "    # print(text_emb[i][0].shape, pt[0].shape)\n",
        "    d = distance.cosine (text_emb[i], pt) \n",
        "    if d>1: d=1    \n",
        "    t_distances[i] = d\n",
        "\n",
        "  # print(min(t_distances), max(t_distances))\n",
        "  \n",
        "  return relu(t_distances,0.5)\n",
        "\n",
        "def per_token_similarity_cosine( text_emb, pattern_emb ):   \n",
        "  a_distances = np.zeros(len(text_emb))\n",
        "  for p in pattern_emb:\n",
        "    t_distances = 1 - dist_cosine_to_point(text_emb, p)    \n",
        "    # a_distances = sum_probabilities ([ t_distances, a_distances ])\n",
        "    a_distances = max_exclusive_pattern ([ t_distances, a_distances ])\n",
        "    \n",
        "  return a_distances\n",
        "\n",
        "test_dists = per_token_similarity_cosine(doc.embeddings, test_pattern.embeddings )\n",
        "# test_dists = relu (1-test_pattern._eval_distances(doc.embeddings), 0.5)\n",
        "test_dists = relu(test_dists, 0.5)\n",
        "render_any(td.tokens_cc, test_dists)\n",
        "\n",
        "\n",
        "\n",
        "# distance.cosine (test_pattern.embeddings[1], doc.embeddings[1]) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}