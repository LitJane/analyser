{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev Protocols.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "0n-DlKATL2JQ",
        "WPEjJV3Elb8t",
        "IoDRgHbFkdPL",
        "Aejr-po6QVeZ",
        "q58BgdvHanlN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQnl3VnQZQML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_git_branch = 'master'\n",
        "\n",
        "#Document parser, refer https://github.com/nemoware/document-parser/releases\n",
        "lib_version = '1.1.15'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJjbZeN6Pbtx",
        "colab_type": "text"
      },
      "source": [
        "# PROTCOLS analysre debug\n",
        "1. IN the left-side panel, change to files tab \n",
        "2. Upload protocol PROTOCOL file\n",
        "3. right click on uploaded file -> copy path\n",
        "2. enter its path/name into the form below\n",
        "3. runtime/run all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-DlKATL2JQ",
        "colab_type": "text"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPEjJV3Elb8t",
        "colab_type": "text"
      },
      "source": [
        "## pull code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFNIoGZL198",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "!pip install overrides\n",
        "\n",
        "–ù–∏—á—Ç–æ = None\n",
        "\n",
        "\n",
        "\n",
        "def exec(x):\n",
        "  r = subprocess.check_output(x, shell=True)\n",
        "  r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "  print(r)\n",
        "\n",
        "\n",
        "print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "try:\n",
        "  exec('rm -r nlp_tools')\n",
        "except:\n",
        "  pass\n",
        "exec(f'git clone --single-branch --branch {_git_branch} https://github.com/nemoware/analyser.git nlp_tools')\n",
        "\n",
        "print('ü¶ä GIT revision:')\n",
        "exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "print('‚ù§Ô∏èimporting Code from GitHub ... DONE')\n",
        "\n",
        "\n",
        "#----\n",
        "import matplotlib as mpl\n",
        "from analyser.documents import TextMap\n",
        "from analyser.legal_docs import DocumentJson\n",
        "from colab_support.renderer import HtmlRenderer\n",
        "\n",
        " \n",
        "\n",
        "class DemoRenderer(HtmlRenderer):\n",
        "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
        "    display(HTML(html))\n",
        "\n",
        "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
        "    return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range, separator=separator)\n",
        "\n",
        "   \n",
        "renderer_ = DemoRenderer()\n",
        "\n",
        "def print_json_summary(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['$words'])\n",
        "  print(f'read file {cd.filename}')\n",
        "\n",
        "  for tag in cd.tags:\n",
        "    span = tag.span\n",
        "    _map = cd.tokenization_maps[tag.span_map]\n",
        "    print(tag)\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_mlpzzdNAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyjarowinkler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMB4pkuu2wi",
        "colab_type": "text"
      },
      "source": [
        "### Init document-parser lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415zlWl16SLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "if not os.path.isfile(f'document-parser-{lib_version}-distribution.zip'):\n",
        "  !wget https://github.com/nemoware/document-parser/releases/download/$lib_version/document-parser-$lib_version-distribution.zip\n",
        "if not os.path.isdir(f'document-parser-{lib_version}'):\n",
        "  !unzip document-parser-$lib_version-distribution.zip\n",
        "\n",
        " \n",
        "os.environ ['documentparser']=f'/content/document-parser-{lib_version}'\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "wp = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmYb03dFcJ9",
        "colab_type": "text"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fbSX2h2MedM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from analyser.contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from analyser.contract_patterns import ContractPatternFactory\n",
        "from analyser.legal_docs import LegalDocument\n",
        " \n",
        "from analyser.ml_tools import *\n",
        "\n",
        "# from headers_detector import doc_features, load_model, make_headline_attention_vector\n",
        "from analyser.hyperparams import HyperParameters\n",
        "from analyser.protocol_parser import protocol_votes_re\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCveVZqvNhsr",
        "colab_type": "text"
      },
      "source": [
        "## üíÖ Init Embedder(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3k194xUFy20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from protocol_parser import  ProtocolPatternFactory\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        "# from contract_patterns import ContractPatternFactory\n",
        "elmo_embedder = ElmoEmbedder()\n",
        "elmo_embedder_default = ElmoEmbedder(layer_name=\"default\")\n",
        "\n",
        "# protocols_factory = ProtocolPatternFactory(elmo_embedder)\n",
        "# contracts_factory = ContractPatternFactory(elmo_embedder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to0MzdyWjEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.contract_parser import find_value_sign_currency_attention\n",
        "from analyser.legal_docs import tokenize_doc_into_sentences_map, ContractValue\n",
        "from analyser.ml_tools import *\n",
        "from analyser.parsing import ParsingContext\n",
        "from analyser.patterns import *\n",
        "from analyser.protocol_parser import ProtocolDocument, find_confident_spans, protocol_votes_re, ProtocolPatternFactory\n",
        "from analyser.protocol_parser import  find_org_structural_level, find_protocol_org, ProtocolParser\n",
        "from analyser.text_tools import *\n",
        "\n",
        "# legal_docs.py\n",
        "from tf_support.embedder_elmo import ElmoEmbedder\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ttjnsaMC3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "protocol_analyser = ProtocolParser(elmo_embedder, elmo_embedder_default)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRIF48ks3OV",
        "colab_type": "text"
      },
      "source": [
        "# Read doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVpcnaLgEJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ‚úÖPre-embedded or Upload?\n",
        "\n",
        "filename = '/content/3.\\u043F\\u0440\\u043E\\u0442\\u043E\\u043A\\u043E\\u043B.doc' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "results = wp.read_doc(filename)\n",
        "for doc in results['documents'][:1]:  # XXX\n",
        "  if 'PROTOCOL' == doc['documentType']:    \n",
        "    doc = join_paragraphs(doc, 'no_id')\n",
        "\n",
        "\n",
        "for p in doc.paragraphs:\n",
        "  print ('‚ò¢Ô∏è', p.header.value.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoDRgHbFkdPL",
        "colab_type": "text"
      },
      "source": [
        "### Embedd and back up pickle (TODO: fix üíÄ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LskyZVU-OHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# protocol_analyser.ebmedd(doc)        \n",
        "# with open(f'{filename}.pickle', 'wb') as handle:\n",
        "#   pickle.dump(doc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEgJ63pck0tM",
        "colab_type": "text"
      },
      "source": [
        "# Analyse PHASE 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrQOg1UimOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =====================================\n",
        "from analyser.parsing import AuditContext\n",
        "actx = AuditContext()\n",
        "protocol_analyser.find_org_date_number(doc, actx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aejr-po6QVeZ",
        "colab_type": "text"
      },
      "source": [
        "### render PHASE 0 results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEYJEzygQO9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in doc.get_tags():\n",
        "  print(t)\n",
        "renderer_.render_color_text(doc.tokens,  doc.get_tags_attention() )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUIhU4cCQhRx",
        "colab_type": "text"
      },
      "source": [
        "#PHASE 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr733JbFQj4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "protocol_analyser.find_attributes(doc, actx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtldMdmoQpj-",
        "colab_type": "text"
      },
      "source": [
        "### render PHASE 1 results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzPEiquCFEx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in doc.get_tags():\n",
        "  print(t)\n",
        "renderer_.render_color_text(doc.tokens,  doc.get_tags_attention() )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmCMIP-8NSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from analyser.dates import document_number_c, document_number_valid_c\n",
        "def find_document_number_in_subdoc(doc: LegalDocument, tagname='number', parent=None) -> [SemanticTag]:\n",
        "\n",
        "  ret =[]\n",
        "  findings = re.finditer(document_number_c, doc.text)\n",
        "  if findings:\n",
        "    for finding in findings:\n",
        "      print(finding)\n",
        "      _number = finding['number']\n",
        "      if document_number_valid_c.match(_number):\n",
        "        span = doc.tokens_map.token_indices_by_char_range(finding.span())\n",
        "        tag =  SemanticTag(tagname, _number, span, parent=parent)\n",
        "        tag.offset(doc.start)\n",
        "        ret.append(tag)\n",
        "      else:\n",
        "        print('invalid', _number)\n",
        "  \n",
        "  return ret\n",
        "\n",
        "def find_contract_numbers(self, doc) -> [ContractValue]:\n",
        "\n",
        "  values = []\n",
        "  for agenda_question_tag in doc.agenda_questions:\n",
        "    subdoc = doc[agenda_question_tag.as_slice()]\n",
        "    # print(subdoc.text)\n",
        "    numbers = find_document_number_in_subdoc(subdoc, tagname='number', parent=agenda_question_tag)\n",
        "\n",
        "    for k, v in enumerate(numbers):\n",
        "      # v.parent = agenda_question_tag\n",
        "      v.kind = SemanticTag.number_key(v.kind, k)\n",
        "    values += numbers\n",
        "  return values\n",
        " \n",
        "\n",
        "find_contract_numbers(protocol_analyser, doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsMYnR_WKjpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc.to_json_obj()['attributes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFBzr6huCJh-",
        "colab_type": "text"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3uKRCfMCMK0",
        "colab_type": "text"
      },
      "source": [
        "## Sections attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjAMCD52CPNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "protocol_sections_edges = protocol_analyser.find_protocol_sections_edges(doc.distances_per_sentence_pattern_dict)\n",
        "renderer_.render_color_text(doc.sentence_map.tokens,  protocol_sections_edges , _range=[0,1], separator='¬∂<br>')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ5ztxb3YHfj",
        "colab_type": "text"
      },
      "source": [
        "### sections spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHuaLeIOY7RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.documents import sentences_attention_to_words\n",
        "from analyser.dates import   document_number_c\n",
        "from analyser.contract_agents import complete_re as agents_re\n",
        "from analyser.transaction_values import complete_re as values_re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5-21a9vYz-",
        "colab_type": "text"
      },
      "source": [
        "### Print all sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dFsA7_ZEXtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tags = protocol_analyser.find_question_decision_sections(doc)\n",
        "# for t in tags:\n",
        "#   print('='*100)\n",
        "#   print(t.get_key().upper()+'>>>>>>>>>>>>')\n",
        "#   print(doc.substr(t), t.get_key())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiXOonoNZ-Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### votes and numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_QuRBqaBBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#DEAL APPROVAL SENTENCES\n",
        "v_deal_approval = max_exclusive_pattern_by_prefix(doc.distances_per_sentence_pattern_dict, 'deal_approval_')\n",
        "_spans, deal_approval_av = sentences_attention_to_words(v_deal_approval, doc.sentence_map,\n",
        "                                                                        doc.tokens_map)\n",
        "deal_approval_relu_av = best_above(deal_approval_av, 0.5)\n",
        "\n",
        "# VOTES\n",
        "votes_av = doc.tokens_map.regex_attention(protocol_votes_re)\n",
        "# DOC NUMBERS\n",
        "numbers_av = doc.tokens_map.regex_attention(document_number_c)\n",
        "# DOC AGENTS orgs\n",
        "agents_av = doc.tokens_map.regex_attention(agents_re)\n",
        "\n",
        "# DOC MARGIN VALUES\n",
        "margin_values_av = protocol_analyser._get_value_attention_vector(doc)\n",
        "margin_values_v = doc.tokens_map.regex_attention(values_re)\n",
        "margin_values_v*=margin_values_av\n",
        "\n",
        "\n",
        "renderer_.render_color_text(doc.tokens,  deal_approval_relu_av , _range=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkvbTVWjb8fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-hlVQgoZz03",
        "colab_type": "text"
      },
      "source": [
        "## Combined attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5rRAeNxZ37F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined_av = sum_probabilities([deal_approval_relu_av,\n",
        "                                 margin_values_v, \n",
        "                                 agents_av/2,\n",
        "                                 votes_av/2, \n",
        "                                 numbers_av/2])\n",
        "\n",
        "\n",
        "combined_av_norm = combined_av = best_above(combined_av, 0.2) \n",
        "renderer_.render_color_text(doc.tokens,  combined_av_norm , _range=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0kEkf60fpDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# v_sections_attention = find_protocol_sections_edges(protocol_analyser, doc.distances_per_sentence_pattern_dict)\n",
        "\n",
        "_question_spans_sent = spans_between_non_zero_attention(protocol_sections_edges)\n",
        "question_spans_words = doc.sentence_map.remap_slices(_question_spans_sent, doc.tokens_map)\n",
        "agenda_questions = list(find_confident_spans(question_spans_words, combined_av_norm, 'agenda_item', 0.5))\n",
        "\n",
        "for x in agenda_questions:\n",
        "  print(\"=\"*100)\n",
        "  print(x)\n",
        "  print(doc.substr(x))\n",
        "\n",
        "for span in question_spans_words:\n",
        "  print(\"=\"*100)\n",
        "  sl=slice(span[0],span[1])\n",
        "  renderer_.render_color_text(doc[sl].tokens,  combined_av_norm[sl] , _range=[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ5aMD3mVB3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raise ('stop here please for now')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJe7FMSnwXcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "ax.plot(deal_approval_relu_av,  color=(1.0,0.2,0.3))   \n",
        "ax.plot(best_above(deal_approval_relu_av, 0.4),  color=(0.0,1.0,0.3))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Gv_KvT69cR",
        "colab_type": "text"
      },
      "source": [
        "## Debug votes finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjJBz6R1-SVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from analyser.protocol_parser import ProtocolAV\n",
        "renderer_.render_color_text(doc.tokens,  doc.distances_per_pattern_dict[ProtocolAV.bin_votes_attention.name] , _range=[0,1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqiAalo_UYhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "renderer_.render_color_text(doc.tokens, numbers_av+votes_av, _range=[0,2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce204cJH-S_N",
        "colab_type": "text"
      },
      "source": [
        "### debug protocol_votes_re"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UheBuqWQ-E6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = protocol_votes_re.search(doc.text)\n",
        "\n",
        "match = doc.text[x.span()[0]:x.span()[1]]\n",
        "print(f'[{match}]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpLHXne-Vzs",
        "colab_type": "text"
      },
      "source": [
        "### debug spans_having_votes_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVJ-oVXP-hmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_sections_attention = protocol_analyser.find_protocol_sections_edges(doc.distances_per_sentence_pattern_dict)\n",
        "question_spans_sent = spans_between_non_zero_attention(v_sections_attention)\n",
        "question_spans_words = doc.sentence_map.remap_slices(question_spans_sent, doc.tokens_map)\n",
        "\n",
        "for c in question_spans_words:\n",
        "  print('-'*80)\n",
        "  print (c, doc.tokens_map.text_range(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEt3zDPe7O0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "_spans_having_votes = list(protocol_analyser.collect_spans_having_votes(question_spans_sent, doc.sentence_map))\n",
        "spans_having_votes_words = doc.sentence_map.remap_slices(_spans_having_votes, doc.tokens_map)\n",
        "x=spans_having_votes_words[0]\n",
        "# x\n",
        "\n",
        "# print(protocol_votes_re)\n",
        "print(doc.tokens[x[0]:x[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-IpTYV_7A8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "protocol_analyser.collect_spans_having_votes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "501s04zxVzCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = protocol_analyser.find_question_decision_sections(doc)\n",
        "wa=doc.distances_per_pattern_dict\n",
        "# doc.distances_per_pattern_dict['relu_deal_approval']\n",
        "# bc=wa['relu_deal_approval'], wa['bin_votes_attention'] / 5]\n",
        "block_confidence = sum_probabilities([wa['numbers_attention'], wa['relu_deal_approval'], wa['bin_votes_attention'] / 5])\n",
        "renderer_.render_color_text(doc.tokens,  wa['bin_votes_attention'], _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD_qdJ49Xxrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class ProtocolAttentionVectors(Enum):\n",
        "#   numbers_attention=1,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrATOpXx4ABA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#helpers\n",
        "\n",
        "\n",
        "# def _paint_tags(self, tags) -> [SemanticTag]:\n",
        "   \n",
        "#   _attention = np.zeros(self.__len__())\n",
        "\n",
        "#   for t in tags:\n",
        "#     _attention[t.as_slice()] += t.confidence\n",
        "#   return _attention\n",
        " \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-r-VX30ISb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# protocol_analyser.find_values(doc)\n",
        "\n",
        "for mv in doc.margin_values:\n",
        "  print(mv.parent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiy3Co9k1Fdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _get_value_attention_vector(self, doc: LegalDocument):\n",
        "#   s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')\n",
        "  \n",
        "#   doc.distances_per_pattern_dict [ '__max_value_av' ] = s_value_attention_vector #just for debugging\n",
        "#   not_value_av = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'not_sum_')\n",
        "  \n",
        "#   not_value_av = smooth_safe(not_value_av, window_len=5)\n",
        "  \n",
        "#   not_value_av = relu(not_value_av, 0.5)\n",
        "#   doc.distances_per_pattern_dict['__not_value_av'] = not_value_av #just for debugging\n",
        "\n",
        "\n",
        "#   s_value_attention_vector -= not_value_av * 0.8\n",
        "#   s_value_attention_vector = relu(s_value_attention_vector, 0.3)\n",
        "#   return s_value_attention_vector\n",
        "\n",
        "# v= _get_value_attention_vector(None, doc)\n",
        "\n",
        "\n",
        "wa = doc.distances_per_pattern_dict  # words attention\n",
        "v_sections_attention = protocol_analyser.find_protocol_sections_edges(doc.distances_per_sentence_pattern_dict)\n",
        "\n",
        "# --------------\n",
        "question_spans_sent = spans_between_non_zero_attention(v_sections_attention)\n",
        "question_spans_words = doc.sentence_map.remap_slices(question_spans_sent, doc.tokens_map)\n",
        "# --------------\n",
        "\n",
        "# *More* attention to spans having votes\n",
        "spans_having_votes = list(protocol_analyser.collect_spans_having_votes(question_spans_sent, doc.sentence_map))\n",
        "\n",
        "spans_having_votes_words = doc.sentence_map.remap_slices(spans_having_votes, doc.tokens_map)\n",
        "# questions_attention =  spans_to_attention(question_spans_words, len(doc))\n",
        "wa['bin_votes_attention'] = spans_to_attention(spans_having_votes_words, len(doc))\n",
        "\n",
        "# v_deal_approval_words = sentence_map.remap_spans(v_deal_approval,  doc.tokens_map )\n",
        "v_deal_approval = max_exclusive_pattern_by_prefix(doc.distances_per_sentence_pattern_dict, 'deal_approval_')\n",
        "_spans, v_deal_approval_words_attention = sentences_attention_to_words(v_deal_approval, doc.sentence_map,\n",
        "                                                                        doc.tokens_map)\n",
        "\n",
        "## value attention\n",
        "\n",
        "wa['value_relu'] = protocol_analyser._get_value_attention_vector(doc)\n",
        "wa['deal_approval_relu05'] = relu(v_deal_approval_words_attention, 0.5)\n",
        "\n",
        "\n",
        "wa['value_in_context'] = sum_probabilities(\n",
        "      [ wa['value_relu'],\n",
        "       smooth_safe(wa['deal_approval_relu05']),\n",
        "       smooth_safe(wa['bin_votes_attention']) / 3.0])\n",
        "\n",
        "# wa [ 'value_in_context' ]\n",
        "renderer_.render_color_text(doc.tokens,  wa [ '__not_value_av' ], _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3gsmGO7nFiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _get_value_attention_vector(self, doc: LegalDocument):\n",
        "#   # vectors = list(filter_values_by_key_prefix(doc.distances_per_pattern_dict, 'sum_max_p_'))\n",
        "  \n",
        "#   s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')\n",
        "#   doc.distances_per_pattern_dict [ '__max_value_av' ] = s_value_attention_vector #just for debugging\n",
        "#   s_value_attention_vector_neg = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_neg')\n",
        "  \n",
        "#   s_value_attention_vector_neg = relu(s_value_attention_vector_neg, 0.33)\n",
        "#   doc.distances_per_pattern_dict['__not_value_av'] = s_value_attention_vector_neg #just for debugging\n",
        "#   s_value_attention_vector -= s_value_attention_vector_neg * 0.66\n",
        "#   s_value_attention_vector = relu(s_value_attention_vector, 0.25)\n",
        "#   return s_value_attention_vector\n",
        "\n",
        "# v = _get_value_attention_vector(protocol_analyser, doc)\n",
        "\n",
        "# # print(doc.distances_per_pattern_dict.['relu_value_attention_vector'])\n",
        "\n",
        "# # vectors = list(filter_values_by_key_prefix(doc.distances_per_pattern_dict, 'sum_max_neg'))\n",
        "# # s_value_attention_vector =sum_probabilities (vectors)\n",
        "\n",
        "f = protocol_analyser.protocols_factory\n",
        "f.create_pattern('not_sum_0', ('', '–ø—É–Ω–∫—Ç 0.', ''))\n",
        "f.create_pattern('not_sum_ad.1', ('', '190000 , –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', ''))\n",
        "f.create_pattern('not_sum_ad.2', ('', '–¥–æ–º 0', ''))\n",
        "f.create_pattern('not_sum_ad.3', ('', '—É–ª–∏—Ü–∞ 0', ''))\n",
        "f.create_pattern('not_sum_1', ('', '0 –¥–Ω–µ–π', ''))\n",
        "f.create_pattern('not_sum_1.1', ('', '–≤ —Ç–µ—á–µ–Ω–∏–µ 0 ( –Ω–æ–ª—è ) –¥–Ω–µ–π', ''))\n",
        "f.create_pattern('not_sum_1.2', ('', '0 —è–Ω–≤–∞—Ä—è', ''))\n",
        "f.create_pattern('not_sum_2', ('', '0 –º–∏–Ω—É—Ç', ''))\n",
        "f.create_pattern('not_sum_3', ('', '0 —á–∞—Å–æ–≤', ''))\n",
        "f.create_pattern('not_sum_4', ('', '0 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤', ''))\n",
        "f.create_pattern('not_sum_5', ('', '0 %', ''))\n",
        "f.create_pattern('not_sum_5.1', ('', '0 % –≥–æ–ª–æ—Å–æ–≤', ''))\n",
        "f.create_pattern('not_sum_6', ('', '2000 –≥–æ–¥', ''))\n",
        "f.create_pattern('not_sum_7', ('', '0 —á–µ–ª–æ–≤–µ–∫', ''))\n",
        "f.create_pattern('not_sum_8', ('', '0 –º–µ—Ç—Ä–æ–≤', ''))\n",
        "\n",
        "\n",
        "protocol_analyser.protocols_factory.embedd()\n",
        "\n",
        "doc.calculate_distances_per_pattern(protocol_analyser.protocols_factory)\n",
        "not_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'not_sum_')\n",
        "not_value_attention_vector = relu(not_value_attention_vector, 0.5)\n",
        "renderer_.render_color_text(doc.tokens,  not_value_attention_vector, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoEsyvrQsQ6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "values: [ContractValue] = find_value_sign_currency_attention(doc, doc.distances_per_pattern_dict ['relu_value_attention_vector'])\n",
        "renderer_.render_color_text(doc.tokens,  doc.distances_per_pattern_dict ['relu_value_attention_vector'] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLByDUP5k9ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CwTLsUk9kX",
        "colab_type": "text"
      },
      "source": [
        "### Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzYi7lb0ECV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jj = doc.to_json()\n",
        "parsed = json.loads(jj)\n",
        "del parsed['tokenization_maps']\n",
        "print(json.dumps(parsed, indent=4, sort_keys=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58BgdvHanlN",
        "colab_type": "text"
      },
      "source": [
        "### üë®üèø‚ÄçüöÄTry find subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLXPnhB9Sa75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subject1 = '''–æ–¥–æ–±—Ä—è—Ç—å –¥–æ–≥–æ–≤–æ—Ä–∞ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤'''\n",
        "subject2 = '''—Å—É–º–º–∞ 0 (–º–∏–ª–ª–∏–æ–Ω–æ–≤) —Ä—É–±–ª–µ–π 0 –∫–æ–ø–µ–µ–∫ '''\n",
        "  \n",
        "subject_embeddings = elmo_embedder_default.embedd_strings([subject1, subject2 ])\n",
        "\n",
        "# import scipy.spatial.distance as distance\n",
        "\n",
        "\n",
        "\n",
        "contract_subj_attention1 = calc_distances_to_pattern(doc.sentences_embeddings , subject_embeddings[0])\n",
        "contract_subj_attention2 = calc_distances_to_pattern(doc.sentences_embeddings , subject_embeddings[1])\n",
        "contract_subj_attention =  contract_subj_attention1# relu(contract_subj_attention1, 0.5)\n",
        "\n",
        "\n",
        "\n",
        "renderer_.render_color_text(doc.sentence_map.tokens, contract_subj_attention, _range=(0,1), separator='<br>')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIPteR9xPllB",
        "colab_type": "text"
      },
      "source": [
        "## Split text by agenda questions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgpcI4fLMsWl",
        "colab_type": "text"
      },
      "source": [
        "### üìù Sections attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31WDPHa5YqDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_protocol_sections_edges(distances_per_pattern_dict):\n",
        "\n",
        "  patterns = ['deal_approval_', 'footers_', 'question_']\n",
        "  vv_=[]\n",
        "  for p in patterns:\n",
        "    v_ = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, p)\n",
        "    v_ = relu(v_, 0.5)\n",
        "    vv_.append(v_)\n",
        "  \n",
        "  v_sections_attention = sum_probabilities(vv_)\n",
        "\n",
        "  # v_deal_approval = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'deal_approval_')\n",
        "  # v_deal_approval = relu(v_deal_approval, 0.5)\n",
        "\n",
        "  # v_question = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'question_')\n",
        "  # v_question = relu(v_question, 0.5)\n",
        "\n",
        "  # v_sections_attention = sum_probabilities([v_deal_approval, v_question])\n",
        "  v_sections_attention = relu(v_sections_attention, 0.7)\n",
        "  return v_sections_attention\n",
        "\n",
        "\n",
        "distances_per_pattern_dict = calc_distances_per_pattern_dict(sentences_embeddings_, patterns_dict, patterns_embeddings_)\n",
        "v_sections_attention = find_protocol_sections_edges(distances_per_pattern_dict)\n",
        "\n",
        "# renderer_.render_color_text(sentence_map.tokens , v_question+v_deal_approval, _range=(0,1), separator='<br>')\n",
        "renderer_.render_color_text(sentence_map.tokens,  v_sections_attention, _range=(0,1), separator='<br>')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W1nF8aLUWrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_spans_sent = spans_between_non_zero_attention(v_sections_attention)\n",
        "print('question_spans_sent', question_spans_sent)\n",
        "\n",
        "question_spans_words =  sentence_map.remap_slices(question_spans_sent,  doc.tokens_map )\n",
        "print('question_spans_words', question_spans_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewa65VrR0i0",
        "colab_type": "text"
      },
      "source": [
        "More attention to spans having votes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUhUfyGPwnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spans_having_votes = list(collect_spans_having_votes(question_spans_sent, sentence_map))\n",
        "\n",
        "\n",
        "spans_having_votes_words =  sentence_map.remap_slices(spans_having_votes,  doc.tokens_map )\n",
        "# questions_attention =  spans_to_attention(question_spans_words, len(doc))\n",
        "votes_attention =  spans_to_attention(spans_having_votes_words, len(doc))\n",
        "\n",
        "# v_deal_approval_words = sentence_map.remap_spans(v_deal_approval,  doc.tokens_map )\n",
        "v_deal_approval = max_exclusive_pattern_by_prefix(distances_per_pattern_dict, 'deal_approval_')\n",
        "_spans, v_deal_approval_words_attention =  sentences_attention_to_words(v_deal_approval, sentence_map,  doc.tokens_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### debug -------------votes_attention\n",
        "\n",
        "renderer_.render_color_text(doc.tokens_map.tokens , relu(v_deal_approval_words_attention, 0.3)   , _range=(0,1))\n",
        "\n",
        "for span in spans_having_votes:\n",
        "  print(span, '=' * 50)\n",
        "  subdoc = sentence_map.slice(span)\n",
        "  print(subdoc.text)\n",
        "  print(sentence_map.char_range( [span.start, span.stop] ))\n",
        "  print('^' * 50)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURFn02TYCJB",
        "colab_type": "text"
      },
      "source": [
        "### Value attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBPCKXeSpPnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "s_value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_p_')\n",
        "s_value_attention_vector_neg = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, 'sum_max_neg')\n",
        "s_value_attention_vector -=   s_value_attention_vector_neg/3\n",
        "s_value_attention_vector = relu(s_value_attention_vector, 0.25)\n",
        "\n",
        "v_deal_approval_words_attention = relu (v_deal_approval_words_attention, 0.5)\n",
        "value_attention_vector = sum_probabilities ( [s_value_attention_vector, v_deal_approval_words_attention, votes_attention/3.0] )\n",
        "# value_attention_vector = sum_probabilities ( [ v_deal_approval_words_attention, votes_attention/10.0] )\n",
        "value_attention_vector = relu(value_attention_vector, 0.5)\n",
        " \n",
        "renderer_.render_color_text(doc.tokens_map.tokens, value_attention_vector  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuduhrZNgIFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWh7gxN8om2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentences to words\n",
        "from contract_parser import find_value_sign_currency_attention \n",
        "from legal_docs import ContractValue\n",
        "\n",
        "words_spans_having_votes =  sentence_map.remap_slices (spans_having_votes,  doc.tokens_map )\n",
        "\n",
        "\n",
        "#--------------\n",
        "#color things\n",
        "print('spans_having_votes', list(spans_having_votes))\n",
        "print('words_spans_having_votes', list(words_spans_having_votes))\n",
        "\n",
        "#\n",
        "\n",
        "# for span in list(words_spans_having_votes):\n",
        "#   print(span)\n",
        "#   selection [ span[0]:span[1]+1  ] += 0.4\n",
        "#--------------\n",
        "\n",
        "#find contract values\n",
        "# for span in words_spans_having_votes:\n",
        "#   subdoc = doc.subdoc_slice(slice(span[0],span[1]) )\n",
        "values:List[ContractValue] = find_value_sign_currency_attention(doc, value_attention_vector)\n",
        "\n",
        "#color \n",
        "numbers_attention = np.zeros(len(doc.tokens_map))\n",
        "numbers_confidence = np.zeros(len(doc.tokens_map))\n",
        "for v in values:\n",
        "  numbers_confidence [ v.value.as_slice() ] +=v.value.confidence\n",
        "  numbers_attention  [ v.value.as_slice() ] = 1\n",
        "  numbers_attention  [ v.currency.as_slice() ] = 1\n",
        "  numbers_attention  [ v.sign.as_slice() ] = 1\n",
        "\n",
        "block_confidence = sum_probabilities ( [numbers_attention, v_deal_approval_words_attention, votes_attention/5] )\n",
        "\n",
        "# value_attention_vector = max_exclusive_pattern_by_prefix(doc.distances_per_pattern_dict, '_phrase')\n",
        "renderer_.render_color_text(doc.tokens_map.tokens, numbers_attention, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JdE_vopgImk",
        "colab_type": "text"
      },
      "source": [
        "## Collect only meaningful sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozb9qbYQqvL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block_confidence = sum_probabilities ( [ numbers_attention, value_attention_vector, v_deal_approval_words_attention] )\n",
        "\n",
        "renderer_.render_color_text(doc.tokens_map.tokens, block_confidence, _range=(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGhXXSFZbWDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_confident_spans(slices, block_confidence, tag_name):\n",
        "  k = 0\n",
        "  for _slice in slices:\n",
        "    k+=1\n",
        "    pv = block_confidence[_slice[0]:_slice[1]]\n",
        "    confidence = estimate_confidence_by_mean_top_non_zeros(pv, 5)\n",
        "    print('-' * 100)\n",
        "    print(_slice, confidence)\n",
        "    if confidence > 0.6:\n",
        "      st = SemanticTag(f\"{tag_name}_{k}\", None, _slice)\n",
        "      st.confidence = confidence\n",
        "      yield (st)\n",
        "\n",
        "\n",
        "tags = find_confident_spans(question_spans_words, block_confidence, 'agenda_item' )\n",
        "for tag in tags:\n",
        "  print(tag.kind)\n",
        "  print(doc.substr(tag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS4gLoAuAZxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QknxIdlTZoN9",
        "colab_type": "text"
      },
      "source": [
        "# MISCL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYfcp9PZBc_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_agenda(distances_per_pattern_dict):\n",
        "  v1 = max_exclusive_pattern_by_prefix(distances_per_pattern_dict,  'agenda_start_' )\n",
        "  agenda_index_start = np.argmax(v1)\n",
        "  # v =max_exclusive_pattern_by_prefix( [distances_per_pattern_dict['deal_approval_1'], distances_per_pattern_dict['deal_approval_2']]  )\n",
        "  v2 = max_exclusive_pattern_by_prefix(distances_per_pattern_dict,  'agenda_end_' )\n",
        "  agenda_index_end = np.argmax(v2)\n",
        "\n",
        "  v2[:agenda_index_start] = 0\n",
        "  v2[agenda_index_end:] = 0\n",
        "\n",
        "\n",
        "  return None, v2\n",
        "  pass\n",
        "\n",
        "questions, attention_v = detect_agenda(distances_per_pattern_dict)\n",
        "print(questions)\n",
        "renderer_.render_color_text(spans_map.tokens , attention_v, _range=(0,1), separator='<br>')\n",
        "# plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYO5x6zL87eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc[0:4].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjLz5-wPt8hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from contract_agents import find_org_names\n",
        "from typing import List\n",
        "\n",
        "def find_protocol_org(protocol)->List[SemanticTag]:\n",
        "  ret=[]\n",
        "  x:List[SemanticTag] = find_org_names(protocol[0:200])\n",
        "  ret.append( SemanticTag.find_by_kind(x, 'org.1.name'))\n",
        "  ret.append( SemanticTag.find_by_kind(x, 'org.1.type'))\n",
        "  return x\n",
        "\n",
        "x=find_protocol_org(doc)\n",
        "for c in x:\n",
        "  print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtq7PaJ9ism",
        "colab_type": "text"
      },
      "source": [
        "## topic_attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u02xJbnV8lNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_attention = make_topic_attention(doc, headline_attention_vector, 'p_agenda_')\n",
        "topic_attention = relu(topic_attention, 0.5)\n",
        "# renderer_.render_color_text(doc.tokens_cc, topic_attention)\n",
        "\n",
        "def mark_section(topic_attention:FixedVector) -> FixedVector:\n",
        "  section_paint = np.zeros_like(topic_attention)\n",
        "  start = np.argmax(topic_attention)\n",
        "  start_v=topic_attention[start]\n",
        "  section_paint[start:] = start_v\n",
        "\n",
        "  return section_paint\n",
        "\n",
        "\n",
        "  print(start)\n",
        "\n",
        "_sp = mark_section(topic_attention)\n",
        "renderer_.render_color_text(doc.tokens_cc, _sp, _range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2ZGKgpdjJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "all_headlines_indices = all_headlines_spans[:,0]\n",
        "\n",
        "# ---\n",
        "def find_sections_spans_by_indices_2(_doc: LegalDocument,\n",
        "                                   headline_pattern_names,\n",
        "                                   all_headlines_indices,\n",
        "                                   headlines_mask,\n",
        "                                   topic,\n",
        "                                   relu_threshold=0.6):\n",
        "  av_name = f'{topic}_headlines_attention_vector'\n",
        "  print(\"TOPIC:\", av_name)\n",
        "  pv = _doc.distances_per_pattern_dict\n",
        "  pv[av_name] = relu(combined_attention_vectors(pv, headline_pattern_names), relu_threshold)\n",
        "\n",
        "  # masking\n",
        "  pv[av_name] *= headlines_mask\n",
        "\n",
        "  _headlines_of_interest_spans = find_sentences_with_attention(_doc.tokens_map, pv[av_name])\n",
        "  _headlines_of_interest_indices = _headlines_of_interest_spans[:, 0]\n",
        "\n",
        "  for i in _headlines_of_interest_indices:\n",
        "    last = find_first_gt(i, all_headlines_indices)\n",
        "    if last is None:\n",
        "      last = len(_doc.tokens_map) - 1\n",
        "    yield [i, last]\n",
        "  pass\n",
        "\n",
        "\n",
        "##==========\n",
        "solution_spans = find_sections_spans_by_indices_2 (\n",
        "    doc, solution_pattern_names, \n",
        "    all_headlines_indices,\n",
        "    headline_attention_vector_relu, \n",
        "    topic='solutions', \n",
        "    relu_threshold=0.6)\n",
        "\n",
        "agenda_spans   = find_sections_spans_by_indices_2 (\n",
        "    doc, agenda_pattern_names, \n",
        "    all_headlines_indices, \n",
        "    headline_attention_vector_relu, \n",
        "    topic='agenda', \n",
        "    relu_threshold=0.6)\n",
        "\n",
        "_agenda_hd_v = doc.distances_per_pattern_dict['agenda_headlines_attention_vector']\n",
        "_solutions_hd_v = doc.distances_per_pattern_dict['solutions_headlines_attention_vector']\n",
        "\n",
        "for agenda_span in agenda_spans:\n",
        "  print('agenda_span', agenda_span)\n",
        "  # print(doc.tokens_map.text_range(agenda_span))\n",
        "  _slice = slice(agenda_span[0], agenda_span[1])\n",
        "  renderer_.render_color_text(doc.tokens_map [_slice ], _filtered_headline_attention_vector[_slice])\n",
        "\n",
        "print('*'*100)\n",
        "\n",
        "print('='*100)\n",
        "for solution_span in solution_spans:\n",
        "  print(':::::: solution_span', solution_span)\n",
        "  print('- '*50)\n",
        "  # print('‚ò¢Ô∏è ', doc.tokens_map.text_range(solution_span))\n",
        "  _slice = slice(solution_span[0],solution_span[1])\n",
        "  renderer_.render_color_text(doc.tokens_map [_slice ], _filtered_headline_attention_vector[_slice])\n",
        "  print('-'*100)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYMTP4UM09BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(doc.distances_per_pattern_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AOv_WQjxvld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(doc.tokens_cc, _solutions_hd_v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipr_V7-HsUhm",
        "colab_type": "text"
      },
      "source": [
        "# Test on uploaded file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaQks4R4wKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF47cwJDsakq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "\n",
        "document_parser_v='1.0.9'\n",
        "\n",
        "!wget https://github.com/nemoware/document-parser/releases/download/$document_parser_v/document-parser-$document_parser_v-distribution.zip\n",
        "!unzip document-parser-$document_parser_v-distribution.zip  \n",
        "os.environ ['documentparser']=f'/content/document-parser-{document_parser_v}'\n",
        "\n",
        "wordDocParser = WordDocParser()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37317MlAd34u",
        "colab_type": "text"
      },
      "source": [
        "## read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYzmTDl8sxC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_res = wordDocParser.read_doc('/content/–ü—Ä–æ—Ç–æ–∫–æ–ª_–û–û–û –í–µ–∫—Ç–æ—Ä –ø—Ä–æ–¥–∞–∂–∞ –Ω–µ–¥–≤–∏–∂.doc')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv_DXzt7volx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from headers_detector import make_headline_attention_vector\n",
        "from integration.word_document_parser import join_paragraphs\n",
        "from ml_tools import find_non_zero_spans, merge_colliding_spans, relu\n",
        "\n",
        "\n",
        "def analyse_protocol(json_res, ctx):\n",
        "  #embedding, etc\n",
        "  protocol = join_paragraphs(json_res, '0')\n",
        "  protocol.embedd_tokens(ctx.protocols_factory.embedder)\n",
        "  #TODO: tetect agents\n",
        "  vd = protocol.calculate_distances_per_pattern(ctx.protocols_factory, verbosity=0)\n",
        "  \n",
        "  vd['headline_attention_vector'] = make_headline_attention_vector(protocol)\n",
        "  vd['headline_attention_vector_relu'] = relu(vd['headline_attention_vector'], HyperParameters.parser_headline_attention_vector_denominator)\n",
        "\n",
        "  all_headlines_spans = find_sentences_with_attention(protocol.tokens_map, vd['headline_attention_vector_relu'])\n",
        "  \n",
        "  filtered_all_headlines_spans = merge_colliding_spans(all_headlines_spans)\n",
        "  all_headlines_indices = filtered_all_headlines_spans[:,0]\n",
        "\n",
        "  # test\n",
        "  vd['filtered_headline_attention_vector'] = np.zeros_like(vd['headline_attention_vector_relu'])\n",
        "  for s in filtered_all_headlines_spans:\n",
        "    vd['filtered_headline_attention_vector'][s[0]:s[1]] += 1\n",
        "\n",
        "  return protocol, all_headlines_indices\n",
        "\n",
        "\n",
        "protocol, all_headlines_indices = analyse_protocol(json_res, ctx)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WorYmB9MjS",
        "colab_type": "text"
      },
      "source": [
        "## Solutions (—Ä–µ—à–µ–Ω–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPRfImpzji2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "solution_spans = find_sections_spans_by_indices (protocol, \n",
        "                                                 solution_pattern_names, \n",
        "                                                 all_headlines_indices,\n",
        "                                                 protocol.distances_per_pattern_dict['headline_attention_vector_relu'], \n",
        "                                                 'solution',\n",
        "                                                  relu_threshold=0.75)\n",
        "\n",
        "\n",
        "print('='*100)\n",
        "for solution_span in solution_spans:\n",
        "  print(':::::: solution_span', solution_span)\n",
        "  print('- '*50)\n",
        "  print('‚ò¢Ô∏è ', protocol.tokens_map.text_range(solution_span))\n",
        "  print('-'*100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1I5MYLC9Drs",
        "colab_type": "text"
      },
      "source": [
        "## headline_attention_vector \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItgZsXaXzH_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(protocol.tokens_cc, protocol.distances_per_pattern_dict['headline_attention_vector'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_8wBZ7dczCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zGZo5rczsE",
        "colab_type": "text"
      },
      "source": [
        "## headline_attention_vector_relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nGqcsMazEhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renderer_.render_color_text(protocol.tokens_cc, protocol.distances_per_pattern_dict['headline_attention_vector_relu'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXFUW8F6b9-K",
        "colab_type": "text"
      },
      "source": [
        "## filtered_headline_attention_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQFmD7SV3yV4",
        "colab_type": "text"
      },
      "source": [
        "# Latent mean (center) sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tOA92qE4UoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TextDocument = LegalDocument #alias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_CuXwXXF-h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from patterns import *\n",
        "\n",
        "def latent_sense(embeddings, wnd_half):\n",
        "   \n",
        "  R = np.zeros(len(embeddings))\n",
        "  for i in range (len(embeddings)):\n",
        "    _start = 0\n",
        "    if i > wnd_half: _start = i-wnd_half\n",
        "    word_emb = embeddings[i:i+1]\n",
        "    window_emb = embeddings[_start : i+wnd_half]   \n",
        "    semantic_similarity =  1- dist_mean_cosine(window_emb, word_emb)\n",
        "    R[i] = semantic_similarity\n",
        "  \n",
        "  return R\n",
        "\n",
        "def render_any(t, v):\n",
        "  renderer_.render_color_text(t, v, _range=(min(v), max(v)) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R3ZVmOG33Yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        " \n",
        "td = doc\n",
        "td.parse()\n",
        "td.embedd_tokens(elmo_embedder)\n",
        "td=doc\n",
        "\n",
        "\n",
        "local_sense = latent_sense(td.embeddings,  7 ) \n",
        "# mean_sense -= min(mean_sense)\n",
        "\n",
        "\n",
        "# R\n",
        "# mean_sense = relu(R, np.mean(R)*1.1)\n",
        "\n",
        "render_any(td.tokens_cc,  local_sense )\n",
        "render_any(td.tokens_cc,  local_sense-mean_sense)\n",
        "\n",
        "\n",
        "# meaninful_tokens = np.nonzero(R)[0:][0]\n",
        "# print('-'*100)\n",
        "# print(f'tokens: {len(td.tokens)} \\t meaninful {len(meaninful_tokens)}')\n",
        "# print('-'*100)\n",
        "# res=[ td.tokens_map.text_range([i, i+1]) for i in meaninful_tokens ]\n",
        "# print(' ' .join(res) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Mq_vt3Y9HR",
        "colab_type": "text"
      },
      "source": [
        "### Latent mean (mean-ing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pwKZ2WQZH3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_sense = latent_sense(td.embeddings,  120 )\n",
        "\n",
        "meaningless = relu (mean_sense, (np.max(mean_sense) + np.mean(mean_sense))/2  )\n",
        "outliers = relu (1.0 - mean_sense,0.8)\n",
        "\n",
        "render_any(td.tokens_cc, meaningless)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpsNnn-Z2J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ascPLyyrZZUz"
      },
      "source": [
        "### Latent local mean (mean-ing) without nonsense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R89jhCkrZZU4",
        "colab": {}
      },
      "source": [
        "mean_local_sense = latent_sense(td.embeddings,  7 )\n",
        "nonsense_subtract = mean_local_sense - meaningless\n",
        "nonsense_subtract = relu(nonsense_subtract, 0.2)\n",
        "render_any(td.tokens_cc, nonsense_subtract )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZYWJucjVwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsdMcThYjWTJ",
        "colab_type": "text"
      },
      "source": [
        "# Pattern search (distance functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKwdjgTji5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pattern = ctx.protocols_factory.create_pattern('test', ('','–ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –û–û–û ¬´ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä ¬´ –ë–∞–∂–µ–Ω ¬ª –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞ —Å –û–±—â–µ—Å—Ç–≤–æ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é ¬´ –†–µ–Ω–æ–≤–∞ –õ–∞–± ¬ª ;',''))\n",
        "test_pattern2 = ctx.protocols_factory.create_pattern('test2', ('','–ü–æ –≤–æ–ø—Ä–æ—Å—É ‚Ññ 7',''))\n",
        "ctx.protocols_factory.embedd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxLgckNdkYIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def per_token_dist_mean_cosine(window, pattern):\n",
        "   \n",
        "#   d=0\n",
        "  \n",
        "#   for p in pattern:\n",
        "#     for v in window:\n",
        "#       d += distance.cosine(p, v)\n",
        "  \n",
        "#   return d / (len(window) * len(pattern))\n",
        "def dist_cosine_to_point( text_emb, pt ):\n",
        "  t_distances = np.zeros(len(text_emb))\n",
        "  for i in range(len(text_emb)):\n",
        "    # print(text_emb[i][0].shape, pt[0].shape)\n",
        "    d = distance.cosine (text_emb[i], pt) \n",
        "    if d>1: d=1    \n",
        "    t_distances[i] = d\n",
        "\n",
        "  # print(min(t_distances), max(t_distances))\n",
        "  \n",
        "  return relu(t_distances,0.5)\n",
        "\n",
        "def per_token_similarity_cosine( text_emb, pattern_emb ):   \n",
        "  a_distances = np.zeros(len(text_emb))\n",
        "  for p in pattern_emb:\n",
        "    t_distances = 1 - dist_cosine_to_point(text_emb, p)    \n",
        "    # a_distances = sum_probabilities ([ t_distances, a_distances ])\n",
        "    a_distances = max_exclusive_pattern ([ t_distances, a_distances ])\n",
        "    \n",
        "  return a_distances\n",
        "\n",
        "test_dists = per_token_similarity_cosine(doc.embeddings, test_pattern.embeddings )\n",
        "# test_dists = relu (1-test_pattern._eval_distances(doc.embeddings), 0.5)\n",
        "test_dists = relu(test_dists, 0.5)\n",
        "render_any(td.tokens_cc, test_dists)\n",
        "\n",
        "\n",
        "\n",
        "# distance.cosine (test_pattern.embeddings[1], doc.embeddings[1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooZtf4dE2yJh",
        "colab_type": "text"
      },
      "source": [
        "# C CHARTER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPwXNyYT21qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = wp.read_doc('/content/–ú–ù–ì –£—Å—Ç–∞–≤.docx')\n",
        "for d in results['documents'][:1]:  # XXX\n",
        "  if 'CHARTER' == d['documentType']:    \n",
        "    charter = join_paragraphs(d, 'no_id')\n",
        "\n",
        "\n",
        "# from legal_docs import tokenize_doc_into_sentences_map\n",
        "sentence_map = tokenize_doc_into_sentences_map(charter, 250)\n",
        "\n",
        "for token in sentence_map.tokens[0:30]:\n",
        "  print('-->', token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hauat0ab3E3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def json2html(cd:DocumentJson):\n",
        "  wordsmap = TextMap(cd.normal_text, cd.tokenization_maps['words'])\n",
        "  markup_vector = np.zeros(len(wordsmap))\n",
        " \n",
        "  for hd in cd.headers:    \n",
        "    span = hd['span']      \n",
        "    markup_vector[span[0]:span[1]] += 2\n",
        "\n",
        "  for tag in cd.attributes:\n",
        "    if 'span' in  cd.attributes[tag]:\n",
        "      span = cd.attributes[tag]['span']\n",
        "      confidence = cd.attributes[tag]['confidence']\n",
        "      markup_vector[span[0]:span[1]] += confidence\n",
        "    else:\n",
        "      warnings.warn(f'{tag} has no span')\n",
        "\n",
        "  return renderer_.to_color_text(wordsmap.tokens, markup_vector)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "json_obj:DocumentJson = DocumentJson.from_json( charter.to_json())\n",
        "\n",
        "json_obj.attributes['d']  = SemanticTag('kind', 'value', (7982, 8951))\n",
        "html = json2html(json_obj)\n",
        "display(HTML(html))\n",
        "\n",
        "\n",
        "\n",
        "# 7982\n",
        "\n",
        "v = np.zeros(len(charter))\n",
        "# test_dists = relu (1-test_pattern._eval_distances(doc.embeddings), 0.5)\n",
        "# v = relu(test_dists, 0.5)\n",
        "# renderer_.render_color_text(charter.tokens, v)\n",
        "# show_json(charter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2FFdOzAjaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_obj.attributes"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}