{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev: tool: picke doc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/document-parser-lib/notebooks/dev_tool_picke_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGoExQvowHLP",
        "colab_type": "text"
      },
      "source": [
        "# Pickle\n",
        "- processing all files and saving results to scv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "_git_branch = \"document-parser-lib\" #@param {type:\"string\"}\n",
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9nfwicwxBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "ca3d32b5-9115-4d2d-d620-bb51df29b142"
      },
      "source": [
        "''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        " \n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(module_url = 'https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz')\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "def _init_the_code():\n",
        "  if '_init_the_code' in GLOBALS__:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import SilentRenderer\n",
        "\n",
        "  class RendererForBatch(SilentRenderer):\n",
        "    pass\n",
        "\n",
        "  GLOBALS__['renderer'] = RendererForBatch()\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "# AZ:---------------------------------------------------------------------------END OF THE THE CODE, See you later\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "## do preparation here   \n",
        "    \n",
        "#1.\n",
        "_init_import_code_from_gh()\n",
        "#2.\n",
        "_init_embedder()\n",
        "#3.\n",
        "_init_the_code()\n",
        "#4. \n",
        "# if batch_charters:\n",
        "# #   _init_charters()\n",
        "# if batch_contracts:\n",
        "#   _init_contracts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "fetching code from GitHub.....document-parser-lib\n",
            "\n",
            "ü¶ä GIT revision:\n",
            "564\n",
            "* document-parser-lib\n",
            "softly Getting rid of docx2txt\n",
            "\n",
            "Code clean-up\n",
            "\n",
            "- mongo tests\n",
            "- new JSON format\n",
            "- updated document-parser to v 1.0.5\n",
            "#41 #43\n",
            "\n",
            "\n",
            "‚ù§Ô∏è DONE importing Code fro GitHub\n",
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:132: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:132: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "‚ù§Ô∏è DONE creating words embedding model\n",
            "[nltk_data] Downloading package punkt to nltk_data_download...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "loading word cases stats model /content/nlp_tools/vocab/word_cases_stats.pickle\n",
            "‚ù§Ô∏è DONE initializing the code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ENUGvcVCrC",
        "colab_type": "code",
        "outputId": "da8cbd8f-d82f-4554-ef4a-54cacb2d70cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X72vKg7Yiy1O",
        "colab_type": "code",
        "outputId": "4bcb5de7-ade9-4fbe-ed17-eb387a7da8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install overrides\n",
        "from overrides import overrides\n",
        "# from integration.doc_providers import GDriveTestDocProvider\n",
        "\n",
        "\n",
        "# doc_provider = GDriveTestDocProvider()\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from documents import TextMap\n",
        "from legal_docs import LegalDocument\n",
        "from ml_tools import SemanticTag\n",
        "from structures import ContractTags"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9nEHJDuFyXZ",
        "colab_type": "text"
      },
      "source": [
        "# UTIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cC3wQNEoA0t",
        "colab_type": "code",
        "outputId": "576775de-3ffe-4c89-af4b-3e55c04a8c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://github.com/nemoware/document-parser/releases/download/1.0.5/document-parser-1.0.5-distribution.zip\n",
        "!unzip document-parser-1.0.5-distribution.zip\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-26 02:19:13--  https://github.com/nemoware/document-parser/releases/download/1.0.5/document-parser-1.0.5-distribution.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/201466058/e01da980-da2b-11e9-94ad-ebb5bf61a60f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190926%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190926T021913Z&X-Amz-Expires=300&X-Amz-Signature=0b904474d1a5d0a87f878b31a0cabfae7e37a2b60676803ffe4478df6659a48b&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddocument-parser-1.0.5-distribution.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-09-26 02:19:13--  https://github-production-release-asset-2e65be.s3.amazonaws.com/201466058/e01da980-da2b-11e9-94ad-ebb5bf61a60f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190926%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190926T021913Z&X-Amz-Expires=300&X-Amz-Signature=0b904474d1a5d0a87f878b31a0cabfae7e37a2b60676803ffe4478df6659a48b&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddocument-parser-1.0.5-distribution.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.240.92\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.240.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74760300 (71M) [application/octet-stream]\n",
            "Saving to: ‚Äòdocument-parser-1.0.5-distribution.zip‚Äô\n",
            "\n",
            "document-parser-1.0 100%[===================>]  71.30M  34.7MB/s    in 2.1s    \n",
            "\n",
            "2019-09-26 02:19:15 (34.7 MB/s) - ‚Äòdocument-parser-1.0.5-distribution.zip‚Äô saved [74760300/74760300]\n",
            "\n",
            "Archive:  document-parser-1.0.5-distribution.zip\n",
            "   creating: document-parser-1.0.5/\n",
            "   creating: document-parser-1.0.5/classes/\n",
            "   creating: document-parser-1.0.5/classes/com/\n",
            "   creating: document-parser-1.0.5/classes/com/nemo/\n",
            "   creating: document-parser-1.0.5/classes/com/nemo/document/\n",
            "   creating: document-parser-1.0.5/classes/com/nemo/document/parser/\n",
            "   creating: document-parser-1.0.5/classes/com/nemo/document/parser/web/\n",
            "   creating: document-parser-1.0.5/lib/\n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/web/DocumentParserRequest.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/web/DocumentParserController.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/App.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/DocumentParser.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/DocumentFileType.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/Paragraph.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/DocumentStructure.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/TextSegment.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/DocumentParser$1.class  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/DocumentType.class  \n",
            "  inflating: document-parser-1.0.5/classes/application.properties  \n",
            "  inflating: document-parser-1.0.5/classes/logback-spring.xml  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-starter-web-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-starter-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-starter-logging-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/logback-classic-1.2.3.jar  \n",
            " extracting: document-parser-1.0.5/lib/logback-core-1.2.3.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-datatype-jdk8-2.9.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-datatype-jsr310-2.9.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-module-parameter-names-2.9.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/hibernate-validator-6.0.17.Final.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-beans-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-webmvc-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-aop-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-context-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-expression-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-starter-actuator-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-actuator-autoconfigure-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-actuator-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-collections4-4.3.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-math3-3.6.1.jar  \n",
            " extracting: document-parser-1.0.5/lib/poi-ooxml-4.1.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/poi-ooxml-schemas-4.1.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/javax.annotation-api-1.3.2.jar  \n",
            " extracting: document-parser-1.0.5/lib/document-parser-1.0.5.jar  \n",
            "  inflating: document-parser-1.0.5/classes/com/nemo/document/parser/web/DocumentParserService.class  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-autoconfigure-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/log4j-to-slf4j-2.11.2.jar  \n",
            " extracting: document-parser-1.0.5/lib/log4j-api-2.11.2.jar  \n",
            " extracting: document-parser-1.0.5/lib/jul-to-slf4j-1.7.26.jar  \n",
            " extracting: document-parser-1.0.5/lib/snakeyaml-1.23.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-boot-starter-json-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-databind-2.9.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-annotations-2.9.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/jackson-core-2.9.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/validation-api-2.0.1.Final.jar  \n",
            " extracting: document-parser-1.0.5/lib/jboss-logging-3.3.2.Final.jar  \n",
            " extracting: document-parser-1.0.5/lib/classmate-1.4.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-web-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/micrometer-core-1.1.5.jar  \n",
            " extracting: document-parser-1.0.5/lib/HdrHistogram-2.1.9.jar  \n",
            " extracting: document-parser-1.0.5/lib/LatencyUtils-2.0.3.jar  \n",
            " extracting: document-parser-1.0.5/lib/slf4j-api-1.7.26.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-core-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/spring-jcl-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.5/lib/poi-4.1.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-codec-1.11.jar  \n",
            " extracting: document-parser-1.0.5/lib/xmlbeans-3.1.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-compress-1.18.jar  \n",
            " extracting: document-parser-1.0.5/lib/curvesapi-1.06.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-cli-1.4.jar  \n",
            " extracting: document-parser-1.0.5/lib/poi-scratchpad-4.1.0.jar  \n",
            " extracting: document-parser-1.0.5/lib/commons-lang3-3.9.jar  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6s5qIs-LHoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from contract_patterns import ContractPatternFactory\n",
        "\n",
        "contract_pattern_factory = ContractPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "contract_pattern_factory.embedder=None\n",
        "\n",
        "with open('contract_pattern_factory.pickle', 'wb') as handle:\n",
        "  pickle.dump(contract_pattern_factory, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJI7NBqsF2Ji",
        "colab_type": "code",
        "outputId": "ca293443-7fbe-4a87-edf8-5ae5b08544ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from contract_parser import ContractDocument\n",
        "from integration.word_document_parser import WordDocParser, join_paragraphs\n",
        "from legal_docs import LegalDocument, Paragraph\n",
        "from ml_tools import SemanticTag\n",
        "\n",
        "import pickle, os\n",
        "\n",
        "\n",
        "# def join_paragraphs(res):\n",
        "#   #TODO: check type of res\n",
        "#   doc: ContractDocument = ContractDocument('')\n",
        "#   doc.parse()\n",
        "\n",
        "#   last = 0\n",
        "#   for p in res['paragraphs']:\n",
        "#     header_text = p['paragraphHeader']['text'] + '\\n'\n",
        "    \n",
        "\n",
        "#     header = LegalDocument(header_text)\n",
        "#     header.parse()\n",
        "\n",
        "#     doc += header\n",
        "#     headerspan = (last, len(doc.tokens_map))\n",
        "#     print(headerspan)\n",
        "#     last = len(doc.tokens_map)\n",
        "\n",
        "#     if p['paragraphBody']:\n",
        "#       body_text = p['paragraphBody']['text'] + '\\nA'\n",
        "#       body = LegalDocument(body_text)\n",
        "#       body.parse()\n",
        "#       doc += body\n",
        "#     bodyspan = (last, len(doc.tokens_map))\n",
        "\n",
        "#     header_tag = SemanticTag('headline', header_text, headerspan)\n",
        "#     body_tag = SemanticTag('paragraphBody', None, bodyspan)\n",
        "\n",
        "#     print(header_tag)\n",
        "#     print(body_tag)\n",
        "#     para = Paragraph(header_tag, body_tag)\n",
        "#     doc.paragraphs.append(para)\n",
        "#     last = len(doc.tokens_map)\n",
        " \n",
        "#   return doc\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'   \n",
        "\n",
        "FN=\"2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\"\n",
        "FILENAME = contracts_filename_prefix + FN\n",
        "os.environ ['documentparser']='/content/document-parser-1.0.5'\n",
        "wp = WordDocParser()\n",
        "\n",
        "res = wp.read_doc(FILENAME)\n",
        "\n",
        "#------------------------\n",
        "doc = join_paragraphs(res, FILENAME)\n",
        "#------------------------\n",
        "\n",
        "\n",
        " \n",
        "print(FN)\n",
        "# cd = ContractDocument(text)\n",
        "# cd.parse()\n",
        "cd=doc\n",
        "cd.embedd_tokens(GLOBALS__['elmo_embedder'])\n",
        "print('embedded ok', cd.embeddings.shape)\n",
        "\n",
        "with open(f'{FN}.pickle', 'wb') as handle:\n",
        "  pickle.dump(cd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/document-parser-1.0.5/classes:/content/document-parser-1.0.5/lib/*\n",
            "2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\n",
            "embedded ok (440, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DFbQIcNEcj8",
        "colab_type": "code",
        "outputId": "3756200e-2a31-4cce-c5e0-8cf3f7c716ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "print(' ' .join(cd.tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "–î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è \n",
            " –≥. –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ ¬´ 11 ¬ª –¥–µ–∫–∞–±—Ä—è 2018 –≥–æ–¥ . \n",
            " –ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã ¬´ –†–∞–¥—É–≥–∞ ¬ª –∏–º–µ–Ω—É–µ–º—ã–π –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å ¬ª , –≤ –ª–∏—Ü–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ –°–æ–ª—è–Ω–æ–π –ú–∞—Ä–∏–Ω—ã –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–Ω—ã , –¥–µ–π—Å—Ç–≤—É—é—â–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –£—Å—Ç–∞–≤–∞ , —Å –æ–¥–Ω–æ–π –°—Ç–æ—Ä–æ–Ω—ã , –∏ –û–û–û ¬´ –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏ ¬ª –≤ –ª–∏—Ü–µ –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ —Å–≤—è–∑—è–º —Å –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é –ò–≤–∞–Ω–æ–≤–∞ –°–µ–º–µ–Ω–∞ –ï–≤–≥–µ–Ω—å–µ–≤–∏—á–∞ , –¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´ –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å ¬ª , —Å –¥—Ä—É–≥–æ–π –°—Ç–æ—Ä–æ–Ω—ã –∑–∞–∫–ª—é—á–∏–ª–∏ –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –æ –Ω–∏–∂–µ—Å–ª–µ–¥—É—é—â–µ–º : \n",
            " 1 . –ü—Ä–µ–¥–º–µ—Ç –î–æ–≥–æ–≤–æ—Ä–∞ . \n",
            " 1.1 –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Å—á–µ—Ç , –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è : \n",
            " 1.1.1 . —Å—á–µ—Ç ‚Ññ 115 –Ω–∞ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è ( —Ç–µ–Ω–Ω–∏—Å–Ω—ã–π —Å—Ç–æ–ª , —Ä—É–∫–æ—Ö–æ–¥ —Å –ø–µ—Ä–µ–∫–ª–∞–¥–∏–Ω–∞–º–∏ , —à–≤–µ–¥—Å–∫–∞—è —Å—Ç–µ–Ω–∫–∞ ) . —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è 80000,00 ( –≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π —Ä—É–±–ª–µ–π 00 –∫–æ–ø–µ–µ–∫ ) —Ä—É–±–ª–µ–π , –ù–î–° –Ω–µ –æ–±–ª–∞–≥–∞–µ—Ç—Å—è . \n",
            " –ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –°—Ç–æ—Ä–æ–Ω . \n",
            " 2.1 . –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ : \n",
            " 2.1.1 . –ø–æ –æ–ø–ª–∞—Ç–µ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞ , –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è . \n",
            " 2.2 . –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ : \n",
            " 2.2.1 . –ø–æ —Ü–µ–ª–µ–≤–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞ , –∞ —Ç–∞–∫–∂–µ –ø–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—è –∑–∞ —Ü–µ–ª–µ–≤—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞ . \n",
            " 2.2.2 . –¥–æ 31-12-2018 –≥–æ–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—é –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç–µ : –∫–æ–ø–∏—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –Ω–∞–∫–ª–∞–¥–Ω–æ–π –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è . \n",
            " 2.2.3 . –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ä–≥–∞–Ω–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–∫–∞–∑—ã–≤–∞–µ–º–æ–π –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏ –≤ —Ñ–æ—Ä–º–µ —Å–≤–æ–µ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ . \n",
            " 3 . —Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –î–æ–≥–æ–≤–æ—Ä–∞ . \n",
            " 3.1.–ù–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è –°—Ç–æ—Ä–æ–Ω–∞–º–∏ –∏ –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ 31-01-2019 –≥–æ–¥ . \n",
            " 4 . –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è . \n",
            " 4.1 . –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –µ–≥–æ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è –°—Ç–æ—Ä–æ–Ω–∞–º–∏ . \n",
            " 4.2 . –≤—Å–µ —Å–ø–æ—Ä—ã , –≤—ã—Ç–µ–∫–∞—é—â–∏–µ –∏–∑ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –î–æ–≥–æ–≤–æ—Ä–∞ , –±—É–¥—É—Ç –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ—à–∞—Ç—å—Å—è –°—Ç–æ—Ä–æ–Ω–∞–º–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–∞—é—Ç—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ , –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§ . \n",
            " 5 . —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –î–æ–≥–æ–≤–æ—Ä–∞ . \n",
            " –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Ç–æ—Ä–≥–Ω—É—Ç –¥–æ—Å—Ä–æ—á–Ω–æ –≤ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –ø–æ—Ä—è–¥–∫–µ –≤ —Å–ª—É—á–∞—è—Ö –∏ –Ω–∞ —É—Å–ª–æ–≤–∏–∏ , –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º . –≤ —Å–ª—É—á–∞–µ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –î–æ–≥–æ–≤–æ—Ä–∞ –¥—Ä—É–≥–∞—è –°—Ç–æ—Ä–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å –Ω–µ –ø–æ–∑–¥–Ω–µ–µ , —á–µ–º –∑–∞ 7 ( —Å–µ–º—å ) –¥–Ω–µ–π . \n",
            " 6 . –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è . \n",
            " 6.1 . –ª—é–±—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É –î–æ–≥–æ–≤–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã , –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–≤–µ—Ä—à–µ–Ω—ã –≤ –ø–∏—Å—å–º–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ –∏ –ø–æ–¥–ø–∏—Å–∞–Ω—ã –°—Ç–æ—Ä–æ–Ω–∞–º–∏ . \n",
            " 6.2 . –≤—Å–µ , —á—Ç–æ –Ω–µ —É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞—Å—Ç–æ—è—â–∏–º –î–æ–≥–æ–≤–æ—Ä–æ–º , —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§ . \n",
            " 6.3 . –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –¥–≤—É—Ö –ø–æ–¥–ª–∏–Ω–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö , –∏–º–µ—é—â–∏—Ö —Ä–∞–≤–Ω—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Å–∏–ª—É , –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –°—Ç–æ—Ä–æ–Ω . \n",
            " 7 . –†–ï–ö–í–ò–ó–ò–¢–´ –°—Ç–æ—Ä–æ–Ω . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAD1B1QxHa3Y",
        "colab_type": "code",
        "outputId": "22bd61d3-cc4d-489e-f9c4-6c56ce69d2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(f'{FN}.pickle', 'rb') as handle:\n",
        "  b = pickle.load(handle)\n",
        "  print('embedded ok', b.embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedded ok (439, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3t0lkgqWW02",
        "colab_type": "text"
      },
      "source": [
        "#Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry768DPkWWIQ",
        "colab_type": "code",
        "outputId": "55cff097-d39e-4009-f81c-fe9ddbce68ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "  def test_find_contract_subject_region_in_doc_head(doc, ctx):\n",
        "    section = doc.subdoc_slice(slice(0, 1500))\n",
        "    denominator = 0.7\n",
        " \n",
        "    # ----------------------------------------\n",
        "    result = ctx.find_contract_subject_regions(section, denominator)\n",
        "    # ---------------------\n",
        "\n",
        "    self.print_semantic_tag(result, doc.tokens_map)\n",
        "    # assert  '1.1 –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Å—á–µ—Ç, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è:'== doc.tokens_map.text_range(result.span).strip())\n",
        "\n",
        "\n",
        "\n",
        "test_find_contract_subject_region_in_doc_head()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fad11264d92c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtest_find_contract_subject_region_in_doc_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: test_find_contract_subject_region_in_doc_head() missing 2 required positional arguments: 'doc' and 'ctx'"
          ]
        }
      ]
    }
  ]
}