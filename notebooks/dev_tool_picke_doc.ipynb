{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dev: tool: picke doc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/document-parser-lib/notebooks/dev_tool_picke_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGoExQvowHLP",
        "colab_type": "text"
      },
      "source": [
        "# Pickle\n",
        "- processing all files and saving results to scv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Config\n",
        "_git_branch = \"document-parser-lib\" #@param {type:\"string\"}\n",
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9nfwicwxBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "   \n",
        "\n",
        "  # print('installing antiword...')\n",
        "  # exec('sudo apt-get install antiword')\n",
        "\n",
        "  # print('installing docx2txt...')\n",
        "  # exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "outputId": "3139f301-7936-4496-96e2-f2246e6aa05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "# AZ:-INIT EMBEDDER-----------------------------------------------------------------------------------\n",
        "def _init_embedder():\n",
        "  if 'elmo_embedder' in GLOBALS__:\n",
        "    print('üëå Embedder is already created! ')\n",
        "    return\n",
        "\n",
        "  from embedding_tools import ElmoEmbedder\n",
        "  GLOBALS__['elmo_embedder'] = ElmoEmbedder(module_url = 'https://storage.googleapis.com/az-nlp/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz')\n",
        "  print('‚ù§Ô∏è DONE creating words embedding model')\n",
        "  return GLOBALS__['elmo_embedder']\n",
        "\n",
        "\n",
        "def _init_contracts():\n",
        "  if 'ContractAnlysingContext' in GLOBALS__:\n",
        "    print('üëå Contracts-related tools are already inited ')\n",
        "    return\n",
        "\n",
        "  from contract_parser import ContractAnlysingContext\n",
        "  GLOBALS__['ContractAnlysingContext'] = ContractAnlysingContext(GLOBALS__['elmo_embedder'], GLOBALS__['renderer'])\n",
        "  print('‚ù§Ô∏è DONE initing Contracts-related tools and models ')\n",
        "\n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "def _init_the_code():\n",
        "  if '_init_the_code' in GLOBALS__:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import SilentRenderer\n",
        "\n",
        "  class RendererForBatch(SilentRenderer):\n",
        "    pass\n",
        "\n",
        "  GLOBALS__['renderer'] = RendererForBatch()\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "\n",
        "\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "# AZ:---------------------------------------------------------------------------END OF THE THE CODE, See you later\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKaWFEoWcK_u",
        "colab_type": "code",
        "outputId": "92859608-b355-4456-d059-10adc2d26744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "source": [
        "## do preparation here   \n",
        "    \n",
        "#1.\n",
        "_init_import_code_from_gh()\n",
        "#2.\n",
        "_init_embedder()\n",
        "#3.\n",
        "_init_the_code()\n",
        "#4. \n",
        "# if batch_charters:\n",
        "# #   _init_charters()\n",
        "# if batch_contracts:\n",
        "#   _init_contracts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fetching code from GitHub.....document-parser-lib\n",
            "\n",
            "ü¶ä GIT revision:\n",
            "548\n",
            "* document-parser-lib\n",
            "fixing slicing\n",
            "\n",
            "Deleting structure parser #48\n",
            "\n",
            "test Reading doc paragraphs using document-parser java\n",
            "\n",
            "\n",
            "‚ù§Ô∏è DONE importing Code fro GitHub\n",
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:132: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:132: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:143: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From nlp_tools/embedding_tools.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "‚ù§Ô∏è DONE creating words embedding model\n",
            "[nltk_data] Downloading package punkt to nltk_data_download...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "loading word cases stats model /content/nlp_tools/vocab/word_cases_stats.pickle\n",
            "‚ù§Ô∏è DONE initializing the code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ENUGvcVCrC",
        "colab_type": "code",
        "outputId": "ed34d15d-47d0-4727-eacf-cabdf7469e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X72vKg7Yiy1O",
        "colab_type": "code",
        "outputId": "1a64f451-5897-40dd-b0e4-7cd1ec347cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!pip install overrides\n",
        "from overrides import overrides\n",
        "# from integration.doc_providers import GDriveTestDocProvider\n",
        "\n",
        "\n",
        "# doc_provider = GDriveTestDocProvider()\n",
        "\n",
        "from contract_parser import ContractAnlysingContext, ContractDocument\n",
        "from contract_patterns import ContractPatternFactory\n",
        "from documents import TextMap\n",
        "from legal_docs import LegalDocument\n",
        "from ml_tools import SemanticTag\n",
        "from structures import ContractTags"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Building wheels for collected packages: overrides\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-1.9-cp36-none-any.whl size=4214 sha256=a6acb07e4e817b01f64ea2a0afe98bb488d8705545129eccb7e265e65c322bef\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "Successfully built overrides\n",
            "Installing collected packages: overrides\n",
            "Successfully installed overrides-1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9nEHJDuFyXZ",
        "colab_type": "text"
      },
      "source": [
        "# UTIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVWhmahYETYW",
        "colab_type": "code",
        "outputId": "8fff0f1e-a049-4fa8-c29e-17b3a995b5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!pip install docx2txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3965 sha256=7315b884e954789dc76c56f70f6febfb96c98926febff28a11a8e3f10e2c127c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cC3wQNEoA0t",
        "colab_type": "code",
        "outputId": "3cce4cf3-ba94-4aca-e6b9-1e5eadb77cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://github.com/nemoware/document-parser/releases/download/1.0.4/document-parser-1.0.2-distribution.zip\n",
        "!unzip document-parser-1.0.2-distribution.zip\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-17 12:26:25--  https://github.com/nemoware/document-parser/releases/download/1.0.4/document-parser-1.0.2-distribution.zip\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/201466058/7b5de600-cb34-11e9-9c90-b82c44529d2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190917%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190917T122626Z&X-Amz-Expires=300&X-Amz-Signature=22ddd7b6745aef197a11f0193975e939223c3e025f7968ce6bcef75bb844bed7&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddocument-parser-1.0.2-distribution.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-09-17 12:26:26--  https://github-production-release-asset-2e65be.s3.amazonaws.com/201466058/7b5de600-cb34-11e9-9c90-b82c44529d2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190917%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190917T122626Z&X-Amz-Expires=300&X-Amz-Signature=22ddd7b6745aef197a11f0193975e939223c3e025f7968ce6bcef75bb844bed7&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddocument-parser-1.0.2-distribution.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.136.43\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.136.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74759867 (71M) [application/octet-stream]\n",
            "Saving to: ‚Äòdocument-parser-1.0.2-distribution.zip‚Äô\n",
            "\n",
            "document-parser-1.0 100%[===================>]  71.30M  6.10MB/s    in 30s     \n",
            "\n",
            "2019-09-17 12:26:57 (2.35 MB/s) - ‚Äòdocument-parser-1.0.2-distribution.zip‚Äô saved [74759867/74759867]\n",
            "\n",
            "Archive:  document-parser-1.0.2-distribution.zip\n",
            "   creating: document-parser-1.0.2/\n",
            "   creating: document-parser-1.0.2/classes/\n",
            "   creating: document-parser-1.0.2/classes/com/\n",
            "   creating: document-parser-1.0.2/classes/com/nemo/\n",
            "   creating: document-parser-1.0.2/classes/com/nemo/document/\n",
            "   creating: document-parser-1.0.2/classes/com/nemo/document/parser/\n",
            "   creating: document-parser-1.0.2/classes/com/nemo/document/parser/web/\n",
            "   creating: document-parser-1.0.2/lib/\n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/web/DocumentParserRequest.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/web/DocumentParserController.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/App.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/DocumentParser.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/DocumentFileType.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/Paragraph.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/DocumentStructure.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/TextSegment.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/DocumentParser$1.class  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/DocumentType.class  \n",
            "  inflating: document-parser-1.0.2/classes/application.properties  \n",
            "  inflating: document-parser-1.0.2/classes/logback-spring.xml  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-starter-web-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-starter-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/jul-to-slf4j-1.7.26.jar  \n",
            " extracting: document-parser-1.0.2/lib/snakeyaml-1.23.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-starter-json-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-databind-2.9.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-annotations-2.9.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-core-2.9.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-datatype-jdk8-2.9.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-datatype-jsr310-2.9.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/jackson-module-parameter-names-2.9.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/hibernate-validator-6.0.17.Final.jar  \n",
            " extracting: document-parser-1.0.2/lib/validation-api-2.0.1.Final.jar  \n",
            " extracting: document-parser-1.0.2/lib/jboss-logging-3.3.2.Final.jar  \n",
            " extracting: document-parser-1.0.2/lib/classmate-1.4.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-web-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/micrometer-core-1.1.5.jar  \n",
            " extracting: document-parser-1.0.2/lib/HdrHistogram-2.1.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/LatencyUtils-2.0.3.jar  \n",
            " extracting: document-parser-1.0.2/lib/slf4j-api-1.7.26.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-core-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-jcl-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/poi-4.1.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/xmlbeans-3.1.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-compress-1.18.jar  \n",
            " extracting: document-parser-1.0.2/lib/curvesapi-1.06.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-cli-1.4.jar  \n",
            " extracting: document-parser-1.0.2/lib/poi-scratchpad-4.1.0.jar  \n",
            "  inflating: document-parser-1.0.2/classes/com/nemo/document/parser/web/DocumentParserService.class  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-autoconfigure-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-starter-logging-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/logback-classic-1.2.3.jar  \n",
            " extracting: document-parser-1.0.2/lib/logback-core-1.2.3.jar  \n",
            " extracting: document-parser-1.0.2/lib/log4j-to-slf4j-2.11.2.jar  \n",
            " extracting: document-parser-1.0.2/lib/log4j-api-2.11.2.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-beans-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-webmvc-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-aop-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-context-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-expression-5.1.8.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-starter-actuator-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-actuator-autoconfigure-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/spring-boot-actuator-2.1.6.RELEASE.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-codec-1.11.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-collections4-4.3.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-math3-3.6.1.jar  \n",
            " extracting: document-parser-1.0.2/lib/poi-ooxml-4.1.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/poi-ooxml-schemas-4.1.0.jar  \n",
            " extracting: document-parser-1.0.2/lib/commons-lang3-3.9.jar  \n",
            " extracting: document-parser-1.0.2/lib/javax.annotation-api-1.3.2.jar  \n",
            " extracting: document-parser-1.0.2/lib/document-parser-1.0.2.jar  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6s5qIs-LHoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from contract_patterns import ContractPatternFactory\n",
        "\n",
        "contract_pattern_factory = ContractPatternFactory(GLOBALS__['elmo_embedder'])\n",
        "contract_pattern_factory.embedder=None\n",
        "\n",
        "with open('contract_pattern_factory.pickle', 'wb') as handle:\n",
        "  pickle.dump(contract_pattern_factory, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJI7NBqsF2Ji",
        "colab_type": "code",
        "outputId": "f3e30840-eba4-4bbb-d464-64f28c76bad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        }
      },
      "source": [
        "from contract_parser import ContractDocument\n",
        "from integration.word_document_parser import WordDocParser\n",
        "from legal_docs import LegalDocument, Paragraph\n",
        "from ml_tools import SemanticTag\n",
        "\n",
        "import pickle, os\n",
        "\n",
        "\n",
        "def join_paragraphs(res):\n",
        "  #TODO: check type of res\n",
        "  doc: ContractDocument = ContractDocument('')\n",
        "  doc.parse()\n",
        "\n",
        "  last = 0\n",
        "  for p in res['paragraphs']:\n",
        "    header_text = p['paragraphHeader']['text'] + '\\n'\n",
        "    \n",
        "\n",
        "    header = LegalDocument(header_text)\n",
        "    header.parse()\n",
        "\n",
        "    doc += header\n",
        "    headerspan = (last, len(doc.tokens_map))\n",
        "    print(headerspan)\n",
        "    last = len(doc.tokens_map)\n",
        "\n",
        "    if p['paragraphBody']:\n",
        "      body_text = p['paragraphBody']['text'] + '\\n'\n",
        "      body = LegalDocument(body_text)\n",
        "      body.parse()\n",
        "      doc += body\n",
        "    bodyspan = (last, len(doc.tokens_map))\n",
        "\n",
        "    header_tag = SemanticTag('headline', header_text, headerspan)\n",
        "    body_tag = SemanticTag('paragraphBody', None, bodyspan)\n",
        "\n",
        "    print(header_tag)\n",
        "    print(body_tag)\n",
        "    para = Paragraph(header_tag, body_tag)\n",
        "    doc.paragraphs.append(para)\n",
        "    last = len(doc.tokens_map)\n",
        " \n",
        "  return doc\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'   \n",
        "\n",
        "FN=\"2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\"\n",
        "FILENAME = contracts_filename_prefix + FN\n",
        "os.environ ['documentparser']='/content/document-parser-1.0.2'\n",
        "wp = WordDocParser()\n",
        "\n",
        "res = wp.read_doc(FILENAME)\n",
        "\n",
        "doc=join_paragraphs(res)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "print(FN)\n",
        "# cd = ContractDocument(text)\n",
        "# cd.parse()\n",
        "cd=doc\n",
        "cd.embedd_tokens(GLOBALS__['elmo_embedder'])\n",
        "print('embedded ok', cd.embeddings.shape)\n",
        "\n",
        "with open(f'{FN}.pickle', 'wb') as handle:\n",
        "  pickle.dump(cd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        " "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/document-parser-1.0.2/classes:/content/document-parser-1.0.2/lib/*\n",
            "{\"documentDate\":\"2018-12-11\",\"documentType\":\"CONTRACT\",\"paragraphs\":[{\"paragraphHeader\":{\"offset\":0,\"text\":\"–î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\\n–≥. –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥                                                                                           ¬´11¬ª  –¥–µ–∫–∞–±—Ä—è 2018 –≥.\",\"length\":152},\"paragraphBody\":{\"offset\":151,\"text\":\"–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã ¬´–†–∞–¥—É–≥–∞¬ª –∏–º–µ–Ω—É–µ–º—ã–π –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´–ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å¬ª, –≤ –ª–∏—Ü–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ –°–æ–ª—è–Ω–æ–π –ú–∞—Ä–∏–Ω—ã –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–Ω—ã, –¥–µ–π—Å—Ç–≤—É—é—â–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –£—Å—Ç–∞–≤–∞, —Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –∏ –û–û–û ¬´–ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏¬ª –≤ –ª–∏—Ü–µ –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ —Å–≤—è–∑—è–º —Å –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é –ò–≤–∞–Ω–æ–≤–∞ –°–µ–º–µ–Ω–∞ –ï–≤–≥–µ–Ω—å–µ–≤–∏—á–∞, –¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å¬ª, —Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∑–∞–∫–ª—é—á–∏–ª–∏ –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –æ –Ω–∏–∂–µ—Å–ª–µ–¥—É—é—â–µ–º:\",\"length\":455}},{\"paragraphHeader\":{\"offset\":606,\"text\":\"1. –ü–†–ï–î–ú–ï–¢ –î–û–ì–û–í–û–†–ê\",\"length\":19},\"paragraphBody\":{\"offset\":625,\"text\":\"1.1 –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Å—á–µ—Ç, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è: \\n1.1.1. –°—á–µ—Ç ‚Ññ 115 –Ω–∞ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (—Ç–µ–Ω–Ω–∏—Å–Ω—ã–π —Å—Ç–æ–ª, —Ä—É–∫–æ—Ö–æ–¥ —Å –ø–µ—Ä–µ–∫–ª–∞–¥–∏–Ω–∞–º–∏, —à–≤–µ–¥—Å–∫–∞—è —Å—Ç–µ–Ω–∫–∞). –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è 80¬†000,00 (–≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π —Ä—É–±. 00 –∫–æ–ø.) —Ä—É–±., –ù–î–° –Ω–µ –æ–±–ª–∞–≥–∞–µ—Ç—Å—è. \",\"length\":297}},{\"paragraphHeader\":{\"offset\":921,\"text\":\"–ü–†–ê–í–ê –ò –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò –°–¢–û–†–û–ù\\n2.1. –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\",\"length\":79},\"paragraphBody\":{\"offset\":999,\"text\":\"2.1.1. –ü–æ –æ–ø–ª–∞—Ç–µ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è.\",\"length\":72}},{\"paragraphHeader\":{\"offset\":1071,\"text\":\"2.2. –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\",\"length\":53},\"paragraphBody\":{\"offset\":1124,\"text\":\"2.2.1. –ü–æ —Ü–µ–ª–µ–≤–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞, –∞ —Ç–∞–∫–∂–µ –ø–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—è –∑–∞ —Ü–µ–ª–µ–≤—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞.\\n2.2.2. –î–æ 31.12.2018 –≥–æ–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—é –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç–µ: –∫–æ–ø–∏—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –Ω–∞–∫–ª–∞–¥–Ω–æ–π –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.\\n2.2.3. –î–æ–∫—É–º–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ä–≥–∞–Ω–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–∫–∞–∑—ã–≤–∞–µ–º–æ–π –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏ –≤ —Ñ–æ—Ä–º–µ —Å–≤–æ–µ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏.\",\"length\":527}},{\"paragraphHeader\":{\"offset\":1649,\"text\":\"3. –°–†–û–ö –î–ï–ô–°–¢–í–ò–Ø –î–û–ì–û–í–û–†–ê\",\"length\":25},\"paragraphBody\":{\"offset\":1674,\"text\":\"3.1.–ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –∏ –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ 31.01.2019 –≥. \",\"length\":98}},{\"paragraphHeader\":{\"offset\":1772,\"text\":\"4. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\",\"length\":25},\"paragraphBody\":{\"offset\":1797,\"text\":\"4.1. –ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –µ–≥–æ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏.\\n4.2. –í—Å–µ —Å–ø–æ—Ä—ã, –≤—ã—Ç–µ–∫–∞—é—â–∏–µ –∏–∑ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞, –±—É–¥—É—Ç –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ—à–∞—Ç—å—Å—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–∞—é—Ç—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ, –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§.\",\"length\":273}},{\"paragraphHeader\":{\"offset\":2069,\"text\":\"5. –†–ê–°–¢–û–†–ñ–ï–ù–ò–ï –î–û–ì–û–í–û–†–ê\",\"length\":23},\"paragraphBody\":{\"offset\":2092,\"text\":\"–ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Ç–æ—Ä–≥–Ω—É—Ç –¥–æ—Å—Ä–æ—á–Ω–æ –≤ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –ø–æ—Ä—è–¥–∫–µ –≤ —Å–ª—É—á–∞—è—Ö –∏ –Ω–∞ —É—Å–ª–æ–≤–∏–∏, –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º. –í —Å–ª—É—á–∞–µ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –î–æ–≥–æ–≤–æ—Ä–∞ –¥—Ä—É–≥–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å –Ω–µ –ø–æ–∑–¥–Ω–µ–µ, —á–µ–º –∑–∞ 7 (—Å–µ–º—å) –¥–Ω–µ–π.\",\"length\":274}},{\"paragraphHeader\":{\"offset\":2366,\"text\":\"6. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\",\"length\":25},\"paragraphBody\":{\"offset\":2391,\"text\":\"6.1. –õ—é–±—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É –î–æ–≥–æ–≤–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã, –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–≤–µ—Ä—à–µ–Ω—ã –≤ –ø–∏—Å—å–º–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ –∏ –ø–æ–¥–ø–∏—Å–∞–Ω—ã —Å—Ç–æ—Ä–æ–Ω–∞–º–∏.\\n6.2. –í—Å–µ, —á—Ç–æ –Ω–µ —É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞—Å—Ç–æ—è—â–∏–º –¥–æ–≥–æ–≤–æ—Ä–æ–º, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§.\\n6.3. –ù–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –¥–≤—É—Ö –ø–æ–¥–ª–∏–Ω–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö, –∏–º–µ—é—â–∏—Ö —Ä–∞–≤–Ω—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Å–∏–ª—É, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –°—Ç–æ—Ä–æ–Ω.\",\"length\":370}},{\"paragraphHeader\":{\"offset\":2759,\"text\":\"7. –†–ï–ö–í–ò–ó–ò–¢–´ –°–¢–û–†–û–ù\",\"length\":19},\"paragraphBody\":null}]}\n",
            "\n",
            "(0, 26)\n",
            "SemanticTag: headline (0, 26) –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\n",
            "–≥. –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥                                                                                           ¬´11¬ª  –¥–µ–∫–∞–±—Ä—è 2018 –≥.\n",
            " –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\n",
            "–≥. –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥                                                                                           ¬´11¬ª  –¥–µ–∫–∞–±—Ä—è 2018 –≥.\n",
            "  1\n",
            "SemanticTag: paragraphBody (26, 164) None None  1\n",
            "(164, 176)\n",
            "SemanticTag: headline (164, 176) 1. –ü–†–ï–î–ú–ï–¢ –î–û–ì–û–í–û–†–ê\n",
            " 1. –ü–†–ï–î–ú–ï–¢ –î–û–ì–û–í–û–†–ê\n",
            "  1\n",
            "SemanticTag: paragraphBody (176, 276) None None  1\n",
            "(276, 306)\n",
            "SemanticTag: headline (276, 306) –ü–†–ê–í–ê –ò –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò –°–¢–û–†–û–ù\n",
            "2.1. –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            " –ü–†–ê–í–ê –ò –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò –°–¢–û–†–û–ù\n",
            "2.1. –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            "  1\n",
            "SemanticTag: paragraphBody (306, 330) None None  1\n",
            "(330, 348)\n",
            "SemanticTag: headline (330, 348) 2.2. –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            " 2.2. –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            "  1\n",
            "SemanticTag: paragraphBody (348, 488) None None  1\n",
            "(488, 502)\n",
            "SemanticTag: headline (488, 502) 3. –°–†–û–ö –î–ï–ô–°–¢–í–ò–Ø –î–û–ì–û–í–û–†–ê\n",
            " 3. –°–†–û–ö –î–ï–ô–°–¢–í–ò–Ø –î–û–ì–û–í–û–†–ê\n",
            "  1\n",
            "SemanticTag: paragraphBody (502, 534) None None  1\n",
            "(534, 546)\n",
            "SemanticTag: headline (534, 546) 4. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\n",
            " 4. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\n",
            "  1\n",
            "SemanticTag: paragraphBody (546, 632) None None  1\n",
            "(632, 644)\n",
            "SemanticTag: headline (632, 644) 5. –†–ê–°–¢–û–†–ñ–ï–ù–ò–ï –î–û–ì–û–í–û–†–ê\n",
            " 5. –†–ê–°–¢–û–†–ñ–ï–ù–ò–ï –î–û–ì–û–í–û–†–ê\n",
            "  1\n",
            "SemanticTag: paragraphBody (644, 730) None None  1\n",
            "(730, 742)\n",
            "SemanticTag: headline (730, 742) 6. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\n",
            " 6. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø\n",
            "  1\n",
            "SemanticTag: paragraphBody (742, 866) None None  1\n",
            "(866, 878)\n",
            "SemanticTag: headline (866, 878) 7. –†–ï–ö–í–ò–ó–ò–¢–´ –°–¢–û–†–û–ù\n",
            " 7. –†–ï–ö–í–ò–ó–ò–¢–´ –°–¢–û–†–û–ù\n",
            "  1\n",
            "SemanticTag: paragraphBody (878, 878) None None  1\n",
            "2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\n",
            "embedded ok (878, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DFbQIcNEcj8",
        "colab_type": "code",
        "outputId": "77f45b10-b759-4f3f-bf0a-abce58c51f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "print(cd.text)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "–î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è\n",
            "–≥. –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥                                              ¬´11¬ª –¥–µ–∫–∞–±—Ä—è 2018 –≥–æ–¥.\n",
            "–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã ¬´–†–∞–¥—É–≥–∞¬ª –∏–º–µ–Ω—É–µ–º—ã–π –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´–ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å¬ª, –≤ –ª–∏—Ü–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ –°–æ–ª—è–Ω–æ–π –ú–∞—Ä–∏–Ω—ã –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–Ω—ã, –¥–µ–π—Å—Ç–≤—É—é—â–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –£—Å—Ç–∞–≤–∞, —Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –∏ –û–û–û ¬´–ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏¬ª –≤ –ª–∏—Ü–µ –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ —Å–≤—è–∑—è–º —Å –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é –ò–≤–∞–Ω–æ–≤–∞ –°–µ–º–µ–Ω–∞ –ï–≤–≥–µ–Ω—å–µ–≤–∏—á–∞, –¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º ¬´–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å¬ª, —Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –∑–∞–∫–ª—é—á–∏–ª–∏ –Ω–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä –æ –Ω–∏–∂–µ—Å–ª–µ–¥—É—é—â–µ–º:\n",
            "1. –ü–†–ï–î–ú–ï–¢ –î–û–ì–û–í–û–†–ê.\n",
            "1.1 –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –æ–ø–ª–∞—á–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Å—á–µ—Ç, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è: \n",
            "1.1.1. –°—á–µ—Ç ‚Ññ 115 –Ω–∞ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (—Ç–µ–Ω–Ω–∏—Å–Ω—ã–π —Å—Ç–æ–ª, —Ä—É–∫–æ—Ö–æ–¥ —Å –ø–µ—Ä–µ–∫–ª–∞–¥–∏–Ω–∞–º–∏, —à–≤–µ–¥—Å–∫–∞—è —Å—Ç–µ–Ω–∫–∞). –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è 80000,00 (–≤–æ—Å–µ–º—å–¥–µ—Å—è—Ç —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π —Ä—É–±–ª–µ–π 00 –∫–æ–ø–µ–µ–∫) —Ä—É–±–ª–µ–π, –ù–î–° –Ω–µ –æ–±–ª–∞–≥–∞–µ—Ç—Å—è. \n",
            "–ü–†–ê–í–ê –ò –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò –°–¢–û–†–û–ù.\n",
            "2.1. –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            "2.1.1. –ü–æ –æ–ø–ª–∞—Ç–µ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å—á–µ—Ç–∞, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞ –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—è.\n",
            "2.2. –ë–ª–∞–≥–æ–ø–æ–ª—É—á–∞—Ç–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ —Å–µ–±—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:\n",
            "2.2.1. –ü–æ —Ü–µ–ª–µ–≤–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞, –∞ —Ç–∞–∫–∂–µ –ø–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—è –∑–∞ —Ü–µ–ª–µ–≤—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –∏/–∏–ª–∏ –∏–º—É—â–µ—Å—Ç–≤–∞.\n",
            "2.2.2. –î–æ 31-12-2018 –≥–æ–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—é –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç–µ: –∫–æ–ø–∏—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π —Ç–æ–≤–∞—Ä–Ω–æ–π –Ω–∞–∫–ª–∞–¥–Ω–æ–π –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.\n",
            "2.2.3. –î–æ–∫—É–º–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ä–≥–∞–Ω–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–∫–∞–∑—ã–≤–∞–µ–º–æ–π –±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏ –≤ —Ñ–æ—Ä–º–µ —Å–≤–æ–µ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏.\n",
            "3. –°–†–û–ö –î–ï–ô–°–¢–í–ò–Ø –î–û–ì–û–í–û–†–ê.\n",
            "3.1.–ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –∏ –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ 31-01-2019 –≥–æ–¥.\n",
            "4. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø.\n",
            "4.1. –ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É —Å –º–æ–º–µ–Ω—Ç–∞ –µ–≥–æ –ø–æ–¥–ø–∏—Å–∞–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏.\n",
            "4.2. –í—Å–µ —Å–ø–æ—Ä—ã, –≤—ã—Ç–µ–∫–∞—é—â–∏–µ –∏–∑ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞, –±—É–¥—É—Ç –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–µ—à–∞—Ç—å—Å—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–∞—é—Ç—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ, –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§.\n",
            "5. –†–ê–°–¢–û–†–ñ–ï–ù–ò–ï –î–û–ì–û–í–û–†–ê.\n",
            "–ù–∞—Å—Ç–æ—è—â–∏–π –¥–æ–≥–æ–≤–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Ç–æ—Ä–≥–Ω—É—Ç –¥–æ—Å—Ä–æ—á–Ω–æ –≤ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –ø–æ—Ä—è–¥–∫–µ –≤ —Å–ª—É—á–∞—è—Ö –∏ –Ω–∞ —É—Å–ª–æ–≤–∏–∏, –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º. –í —Å–ª—É—á–∞–µ –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –î–æ–≥–æ–≤–æ—Ä–∞ –¥—Ä—É–≥–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å –Ω–µ –ø–æ–∑–¥–Ω–µ–µ, —á–µ–º –∑–∞ 7 (—Å–µ–º—å) –¥–Ω–µ–π.\n",
            "6. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø.\n",
            "6.1. –õ—é–±—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É –î–æ–≥–æ–≤–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã, –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–≤–µ—Ä—à–µ–Ω—ã –≤ –ø–∏—Å—å–º–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ –∏ –ø–æ–¥–ø–∏—Å–∞–Ω—ã —Å—Ç–æ—Ä–æ–Ω–∞–º–∏.\n",
            "6.2. –í—Å–µ, —á—Ç–æ –Ω–µ —É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞—Å—Ç–æ—è—â–∏–º –¥–æ–≥–æ–≤–æ—Ä–æ–º, —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§.\n",
            "6.3. –ù–∞—Å—Ç–æ—è—â–∏–π –î–æ–≥–æ–≤–æ—Ä —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –¥–≤—É—Ö –ø–æ–¥–ª–∏–Ω–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö, –∏–º–µ—é—â–∏—Ö —Ä–∞–≤–Ω—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Å–∏–ª—É, –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ –°—Ç–æ—Ä–æ–Ω.\n",
            "7. –†–ï–ö–í–ò–ó–ò–¢–´ –°–¢–û–†–û–ù.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAD1B1QxHa3Y",
        "colab_type": "code",
        "outputId": "d0b25b25-f2f3-4758-8c11-de4a02aee9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(f'{FN}.pickle', 'rb') as handle:\n",
        "  b = pickle.load(handle)\n",
        "  print('embedded ok', b.embeddings.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedded ok (878, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}