{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER Matcher (reg exps).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/compartia/nlp_tools/blob/tensorflow-model/NER_Matcher_(reg_exps).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"tensorflow-model\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "def read_doc(fn):\n",
        "  import docx2txt, sys, os\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    text = docx2txt.process(fn)\n",
        "\n",
        "  except:\n",
        "    print(\"Unexpected error:\", sys.exc_info())\n",
        "    os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "    with open(fn + '.txt') as f:\n",
        "      text = f.read()\n",
        "\n",
        "  return text\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_B3i8-7PfL",
        "colab_type": "text"
      },
      "source": [
        "# Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLhqg3zVxn2",
        "colab_type": "text"
      },
      "source": [
        "### load files from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADQZo1lu7RgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gspread\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "# google_spread = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "\n",
        "import glob\n",
        "def read_documents(filename_prefix):\n",
        "  texts = {}\n",
        "  for file in glob.glob(filename_prefix+\"*.doc\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.doc FILE!!', file)\n",
        "\n",
        "  for file in glob.glob(filename_prefix+\"*.docx\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.docx FILE!!', file)\n",
        "      \n",
        "  return texts\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "contracts = read_documents(contracts_filename_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2mkCARg1dC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BAD_DOCS=\"\"\"\n",
        "–î–æ–≥–æ–≤–æ—Ä 5.docx\n",
        "–ü—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –ü—É—Ä –∞–¥–º.docx\n",
        "–ü—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ –ü—É—Ä –∞–¥–º - –±–ª–∞–≥–æ—Ç–≤.docx\n",
        "–ú–ù–ü–ó 1 –î–æ–≥–æ–≤–æ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è –î–û –ì–ü–ù - –§–û–ù–î 2019_–£–º–Ω–æ–∂–∞—è —Ç–∞–ª–∞–Ω—Ç—ã_.docx\n",
        "–ú–ù–ü–ó 2 –î–æ–≥–æ–≤–æ—Ä –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞.docx\n",
        "–î–æ–ø.—Å–æ–≥–ª3 –î–æ–≤–µ—Ä–∏–µ.doc\n",
        "–î–æ–≥–æ–≤–æ—Ä –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç_–ü–µ—Ç—Ä–æ–≤.docx\n",
        "–î–û–ì–û–í–û–†_–î–µ—Ç—Å–∫–∏–∏ÃÜ –¥–æ–º.docx\n",
        "7. –°–æ–≥–ª–∞—à–µ–Ω–∏–µ –ë–ì –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å-–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏.doc\n",
        "1. –î–æ–ø. —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –∫ –î–æ–≥–æ–≤–æ—Ä—É_—Ñ–∏–Ω.docx\n",
        "2. –î–æ–≥–æ–≤–æ—Ä –ø–æ –±–ª–∞–≥-—Ç–∏ –†–∞–¥—É–≥–∞.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 4.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä 3.docx\n",
        "–î–æ–≥–æ–≤–æ—Ä1.docx\n",
        "–ï–Æ2 –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è (–æ–¥–∞—Ä—è–µ–º—ã–∏ÃÜ).docx\n",
        "–ï–Æ2 –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è (–ø–æ–ª—É—á–∞—Ç–µ–ª—å).docx\n",
        "–î–ö–ü –ü–æ–ª–æ–≤–∏–Ω–Ω–æ–∏ÃÜ –ù–ë –∏ –ê–ó–°.docx\n",
        "1.1. –î–æ–≥–æ–≤–æ—Ä –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–∏—è.docx\n",
        "\"\"\"\n",
        "\n",
        "BAD_DOCS = [ '/content/gdrive/My Drive/GazpromOil/Contracts/'+x.strip() for x in BAD_DOCS.split('\\n') if x.strip()!='' ]\n",
        "for i in  range(len(BAD_DOCS)):\n",
        "  print(i, BAD_DOCS[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfbFXzmWs60s",
        "colab_type": "text"
      },
      "source": [
        "#Reg exp solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQuPBaWV-tB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ru_cap(xx):\n",
        "  return '\\s+'.join(  [ f'[{x[0].upper()}{x[0].lower()}]{x[1:-2]}[–∞-—è]{{0,3}}' for x in xx.split(' ')] )\n",
        "  \n",
        "print(ru_cap('–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v3Mr74D2p4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "from text_normalize import replacements_regex, normalize_text\n",
        "\n",
        "def ru_cap(xx):\n",
        "  return '\\s+'.join([f'[{x[0].upper()}{x[0].lower()}]{x[1:-2]}[–∞-—è]{{0,3}}' for x in xx.split(' ')])\n",
        "\n",
        "\n",
        "q_re_open = '\\'\\\"¬´'\n",
        "q_re_close = '\\'\\\"¬ª'\n",
        "\n",
        "sf = '[–∞-—è]{1,3}'\n",
        "\n",
        "r_few_words = r'\\s+[–ê-–Ø]{1}[–∞-—è\\-, ]{1,80}\\s+'\n",
        "r_few_words_s = r'\\s+[–ê-–Ø–∞-—è\\-, ]{1,80}\\s+'\n",
        "\n",
        "ORG_TYPES_re = [\n",
        "  ru_cap('–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–ê–û',\n",
        "  ru_cap('–ó–∞–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–ó–ê–û',\n",
        "  ru_cap('–û—Ç–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–û–ê–û',\n",
        "  ru_cap('–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ')+r_few_words_s,\n",
        "  ru_cap('–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'),\n",
        "  ru_cap('–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'),\n",
        "  ru_cap('–û–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é'), '–û–û–û',\n",
        "  ru_cap('–ù–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'),\n",
        "  ru_cap('–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'),\n",
        "  ru_cap('–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ–Ω–¥'),\n",
        "  \n",
        "    \n",
        "    \n",
        "  ru_cap('–§–æ–Ω–¥') + r_few_words_s,\n",
        "  r_few_words_s + ru_cap('—É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'),\n",
        "  ru_cap('–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å'), '–ò–ü'\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# ORG_TYPES_re =[ru_cap('–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ')]\n",
        "\n",
        "r_types_ = '|'.join([x for x in ORG_TYPES_re])\n",
        "r_types = f'({r_types_})'\n",
        "\n",
        "r_name_a = r'([‚Äì–ê-–Øa-—è\\- ]{0,30})'\n",
        "r_name = r'([‚Äì–ê-–Øa-—è\\- ]{0,50})'\n",
        "\n",
        "r_quote_open = r'([¬´\"<]|[\\']{2})'\n",
        "r_quote_close = r'([¬ª\">]|[\\']{2})'\n",
        "\n",
        "r_alter = r'(\\s+\\(.{1,40}\\))?'\n",
        "\n",
        "complete_re = re.compile(r_types + r'\\s*' + r_name_a + r_quote_open + r_name + r_quote_close + r_alter,\n",
        "                         re.MULTILINE)\n",
        "\n",
        "\n",
        "def find_org_names(txt):\n",
        "  def clean(x):\n",
        "    if x is None: return x\n",
        "    return x.replace('\\t', ' ').replace('\\n', ' ').replace(' ‚Äì ', '-')\n",
        "\n",
        "  def to_dict(_org):\n",
        "    return {\n",
        "      '1_type': clean(_org[1]),\n",
        "      '2_type_ext': clean(_org[2]),\n",
        "      '3_name': f'\"{clean(_org[4])}\"',\n",
        "      '4_alt_name': clean(_org[6])\n",
        "    }\n",
        "\n",
        "  def find_from(start):\n",
        "    _next = txt[start:]\n",
        "    r = complete_re.search(_next)\n",
        "    if r is not None:\n",
        "      return to_dict(r), start+r.span()[1]\n",
        "    else: \n",
        "      return None, None\n",
        "    \n",
        "    \n",
        "  org_names = {}\n",
        "  r, end = find_from(0)\n",
        "  if r is not None:\n",
        "    org_names[0] = r\n",
        "    \n",
        "    r2, end = find_from(end)\n",
        "    while r2 is not None and r2['3_name'].lower() ==  r['3_name'].lower():\n",
        "#       print(end)\n",
        "      r2, end = find_from(end)\n",
        "    \n",
        "    if r2 is not None:\n",
        "      org_names[1] = r2\n",
        " \n",
        "        \n",
        "\n",
        "  return org_names\n",
        "\n",
        "\n",
        "##################TEST\n",
        "\n",
        "\n",
        "def _str_none(x):\n",
        "  if x is None: \n",
        "    return '-'\n",
        "  else:\n",
        "    return x\n",
        "\n",
        "def orgs_to_tsv(org_names)->str:\n",
        "  ret=''\n",
        "  if 0 in org_names:   \n",
        "    ret+=org_to_tsv(org_names[0])\n",
        "  else:\n",
        "    ret+='\\t'*4\n",
        "    \n",
        "  if 1 in org_names:   \n",
        "    ret+=org_to_tsv(org_names[1])\n",
        "  else:\n",
        "    ret+='\\t'*4\n",
        "  return ret\n",
        "    \n",
        "def org_to_tsv(org)->str:     \n",
        "  b = [ _str_none(x) for x in org.values() ]\n",
        "  return '\\t'.join(b)+'\\t'\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "fc=0\n",
        "export_strings=''\n",
        "for fn in  contracts:#\n",
        "\n",
        "  export_string = f'{fn.split(\"/\")[-1]}\\t'\n",
        "  fc += 1\n",
        " \n",
        "  txt = contracts[fn][0:2000]\n",
        "  txt=normalize_text(txt, replacements_regex)\n",
        "  org_names = find_org_names(txt)\n",
        "   \n",
        "  export_string+=orgs_to_tsv(org_names)\n",
        "  export_strings+=export_string+'\\n'\n",
        "print(export_strings)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}