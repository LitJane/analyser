{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER Matcher (reg exps).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjbkIfc8Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBALS__={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpPPXqRQs6-",
        "colab_type": "text"
      },
      "source": [
        "## MAIN, init, load code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Oe-BsTcCIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"300px\", display-mode: \"form\" }\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ''' AZ:-IMPORT CODE GITHUB----------------------------------------------üò∫------ '''\n",
        "import sys\n",
        "# ====================================\n",
        "# ====================================\n",
        "_git_branch = \"tensorflow-model\"  # @param {type:\"string\"}\n",
        "# ====================================\n",
        "# ====================================\n",
        "\n",
        "\n",
        "def _init_import_code_from_gh():\n",
        "  if 'GLOBALS__' not in globals():\n",
        "    print('adding global GLOBALS__')\n",
        "    global GLOBALS__\n",
        "    GLOBALS__ = {}\n",
        "\n",
        "  if '_init_import_code_from_gh' in GLOBALS__:\n",
        "    print('üëå code already imported from GitHub!')\n",
        "    return\n",
        "\n",
        "  import subprocess\n",
        "  def exec(x):\n",
        "    r = subprocess.check_output(x, shell=True)\n",
        "    r = r.decode('unicode-escape').encode('latin1').decode('utf8')\n",
        "    print(r)\n",
        "\n",
        "  print(f\"fetching code from GitHub.....{_git_branch}\")\n",
        "  try:\n",
        "    exec('rm -r nlp_tools')\n",
        "  except:\n",
        "    pass\n",
        "  exec(f'git clone --single-branch --branch {_git_branch} https://github.com/compartia/nlp_tools.git nlp_tools')\n",
        "\n",
        "  print('ü¶ä GIT revision:')\n",
        "  exec('cd nlp_tools\\ngit rev-list --reverse HEAD | awk \"{ print NR }\" | tail -n 1\\ngit branch\\ngit log -3 --pretty=%B')\n",
        "\n",
        "  sys.path.insert(0, 'nlp_tools')\n",
        "\n",
        "  # self-test\n",
        "  from text_tools import untokenize\n",
        "  print(untokenize(['code', 'imported', 'OK üëç']))\n",
        "\n",
        "  print('installing antiword...')\n",
        "  exec('sudo apt-get install antiword')\n",
        "\n",
        "  print('installing docx2txt...')\n",
        "  exec(\"pip install docx2txt\")\n",
        "\n",
        "  GLOBALS__['_init_import_code_from_gh'] = True\n",
        "\n",
        "  ''' AZ:-------------------------------------------------IMPORT CODE GITHUB-üò∫---'''\n",
        "  print('‚ù§Ô∏è DONE importing Code fro GitHub')\n",
        "\n",
        "\n",
        "# AZ:-INIT ELMO-----------------------------------------------------------------------------------\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# AZ:- THE CODE----------------------------------------------------------------------------------\n",
        "\n",
        "def _init_the_code(reset=False):\n",
        "  if '_init_the_code' in GLOBALS__ and not reset:\n",
        "    print('üëå Code is alredy imported!')\n",
        "    return\n",
        "\n",
        "  from renderer import HtmlRenderer\n",
        "  from renderer import to_multicolor_text\n",
        "  from renderer import known_subjects_dict\n",
        "\n",
        "  from structures import ContractSubject\n",
        "  from contract_parser import ContractDocument3\n",
        "\n",
        "  from ml_tools import ProbableValue\n",
        "\n",
        "  from legal_docs import LegalDocument\n",
        "  from renderer import as_warning, as_headline_3, as_offset, as_smaller\n",
        "\n",
        "  class DemoRenderer(HtmlRenderer):\n",
        "\n",
        "    def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      html = self.to_color_text(tokens, weights, colormap, print_debug, _range)\n",
        "      display(HTML(html))\n",
        "\n",
        "    def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None):\n",
        "      return super()._to_color_text(tokens, weights, mpl, colormap=colormap, _range=_range)\n",
        "\n",
        "    def render_multicolor_text(self, tokens, vectors, colormap, min_color=None, _slice=None):\n",
        "      display(HTML(to_multicolor_text(tokens, vectors, colormap, min_color=min_color, _slice=_slice)))\n",
        "\n",
        "     \n",
        "     \n",
        "\n",
        "     \n",
        "  GLOBALS__['renderer'] = DemoRenderer()\n",
        "\n",
        "  GLOBALS__['_init_the_code'] = True\n",
        "  print(\"‚ù§Ô∏è DONE initializing the code\")\n",
        "\n",
        "  # AZ:-------------------------------------------------Init Protocols context===\n",
        "\n",
        "\n",
        "def read_doc(fn):\n",
        "  import docx2txt, sys, os\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    text = docx2txt.process(fn)\n",
        "\n",
        "  except:\n",
        "    print(\"Unexpected error:\", sys.exc_info())\n",
        "    os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "    with open(fn + '.txt') as f:\n",
        "      text = f.read()\n",
        "\n",
        "  return text\n",
        "\n",
        "def interactive_upload(filetype):\n",
        "  from google.colab import files\n",
        "  import docx2txt\n",
        "\n",
        "  print(f'Please select \"{filetype}\" .docx file:')\n",
        "  uploaded = files.upload()\n",
        "  docs = []\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    with open(fn, \"wb\") as df:\n",
        "      df.write(uploaded[fn])\n",
        "      df.close()\n",
        "\n",
        "    # extract text\n",
        "\n",
        "    text = ''\n",
        "    try:\n",
        "      text = docx2txt.process(fn)\n",
        "    except:\n",
        "      print(\"Unexpected error:\", sys.exc_info())\n",
        "      os.system('antiword -w 0 \"' + fn + '\" > \"' + fn + '.txt\"')\n",
        "      with open(fn + '.txt') as f:\n",
        "        text = f.read()\n",
        "    print(\"–°–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:\", len(text))\n",
        "    docs.append(text)\n",
        "    return docs\n",
        "\n",
        "  \n",
        "# AZ:- ENDO OF THE THE CODE------------------------------------------------XXXX\n",
        "\n",
        "\n",
        "# 1.\n",
        "_init_import_code_from_gh()\n",
        " \n",
        "# 3.\n",
        "_init_the_code(True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_B3i8-7PfL",
        "colab_type": "text"
      },
      "source": [
        "# Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLhqg3zVxn2",
        "colab_type": "text"
      },
      "source": [
        "### load files from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADQZo1lu7RgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gspread\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# auth.authenticate_user()\n",
        "# google_spread = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "\n",
        "import glob\n",
        "def read_documents(filename_prefix):\n",
        "  texts = {}\n",
        "  for file in glob.glob(filename_prefix+\"*.doc\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.doc FILE!!', file)\n",
        "\n",
        "  for file in glob.glob(filename_prefix+\"*.docx\"):\n",
        "    try:\n",
        "      text = read_doc(file)\n",
        "      texts[file] = text\n",
        "      print(\"good:\", file)\n",
        "    except:\n",
        "      print('WRONG *.docx FILE!!', file)\n",
        "      \n",
        "  return texts\n",
        "\n",
        "contracts_filename_prefix='/content/gdrive/My Drive/GazpromOil/Contracts/'\n",
        "contracts = read_documents(contracts_filename_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2mkCARg1dC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BAD_DOCS=\"\"\"\n",
        "–î–æ–≥–æ–≤–æ—Ä 2.docx\n",
        "\"\"\"\n",
        "\n",
        "BAD_DOCS = [ '/content/gdrive/My Drive/GazpromOil/Contracts/'+x.strip() for x in BAD_DOCS.split('\\n') if x.strip()!='' ]\n",
        "for i in  range(len(BAD_DOCS)):\n",
        "  print(i, BAD_DOCS[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfbFXzmWs60s",
        "colab_type": "text"
      },
      "source": [
        "#Reg exp solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v3Mr74D2p4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from typing import AnyStr, Match\n",
        "\n",
        "from text_normalize import r_group, r_bracketed, r_quoted, r_capitalized_ru, \\\n",
        "  _r_name, r_quoted_name, replacements_regex, ru_cap, r_few_words_s, r_human_name\n",
        "\n",
        "import re\n",
        "from typing import AnyStr, Match\n",
        "\n",
        "from text_normalize import r_group, r_bracketed, r_quoted, r_capitalized_ru, \\\n",
        "  _r_name, r_quoted_name, replacements_regex, ru_cap, r_few_words_s, r_human_name\n",
        "\n",
        "ORG_TYPES_re = [\n",
        "  ru_cap('–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–ê–û',\n",
        "  ru_cap('–ó–∞–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–ó–ê–û',\n",
        "  ru_cap('–û—Ç–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ'), '–û–ê–û',\n",
        "  ru_cap('–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'),\n",
        "  ru_cap('–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ –±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'),\n",
        "  ru_cap('—É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'),\n",
        "  ru_cap('–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'),\n",
        "  ru_cap('–û–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é'), '–û–û–û',\n",
        "  ru_cap('–ù–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'),\n",
        "  ru_cap('–ë–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ–Ω–¥'),\n",
        "  ru_cap('–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å'), '–ò–ü',\n",
        "\n",
        "  r'[–§—Ñ]–æ–Ω–¥[–∞-—è]{0,2}' + r_few_words_s,\n",
        "\n",
        "]\n",
        "_r_types_ = '|'.join([x for x in ORG_TYPES_re])\n",
        "\n",
        "\n",
        "r_few_words = r'\\s+[–ê-–Ø]{1}[–∞-—è\\-, ]{1,80}'\n",
        "\n",
        "r_type_ext = r_group(r'[\\w\\s]*', 'type_ext')\n",
        "r_name_alias = r_group(_r_name, 'alias')\n",
        "\n",
        "r_quoted_name_alias = r_group(r_quoted(r_name_alias))\n",
        "r_alias_prefix = r_group(''\n",
        "                         + r_group(r'(–∏–º–µ–Ω—É–µ[–∞-—è]{1,3}\\s+)?–≤?\\s*–¥–∞–ª[–∞-—è]{2,8}\\s?[‚Äì\\-]?') + '|'\n",
        "                         + r_group(r'–¥–∞–ª–µ–µ\\s?[‚Äì\\-]?\\s?'))\n",
        "r_alias = r_group(r\".{0,140}\" + r_alias_prefix + r'\\s*' + r_quoted_name_alias)\n",
        "\n",
        "r_types = r_group(f'{_r_types_}', 'type')\n",
        "r_type_and_name = r_types + r_type_ext + r_quoted_name\n",
        "\n",
        "r_alter = r_group(r_bracketed(r'.{1,70}') + r'{0,2}', 'alt_name')\n",
        "complete_re_str = r_type_and_name + '\\s*' + r_alter + r_alias + '?'\n",
        "# ----------------------------------\n",
        "complete_re = re.compile(complete_re_str, re.MULTILINE)\n",
        "\n",
        "\n",
        "# ----------------------------------\n",
        "\n",
        "\n",
        "def find_org_names(txt):\n",
        "  def clean(x):\n",
        "    if x is None:\n",
        "      return x\n",
        "    return x.replace('\\t', ' ').replace('\\n', ' ').replace(' ‚Äì ', '-')\n",
        "\n",
        "  def to_dict(m: Match[AnyStr]):\n",
        "\n",
        "    return {\n",
        "      'type': (clean(m['type']), m.span('type')),\n",
        "      'type_ext': (clean(m['type_ext']), m.span('type_ext')),\n",
        "      'name': (clean(m['name']), m.span('name')),\n",
        "      'alt_name': (clean(m['alt_name']), m.span('alt_name')),\n",
        "      'alias': (clean(m['alias']), m.span('alias')),\n",
        "    }\n",
        "\n",
        "  org_names = {}\n",
        "\n",
        "  i = 0\n",
        "  for r in re.finditer(complete_re, txt):\n",
        "    org = to_dict(r)\n",
        "    _name = org['name'][0]\n",
        "    if _name not in org_names:\n",
        "      org_names[_name] = org\n",
        "    i += 1\n",
        "\n",
        "  return list(org_names.values())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(r_group(r_capitalized_ru, 'alias'))\n",
        "  pass\n",
        "\n",
        "\n",
        "def normalize_contract(_t):\n",
        "  t = _t\n",
        "  for (reg, to) in alias_quote_regex + replacements_regex:\n",
        "    t = reg.sub(to, t)\n",
        "\n",
        "  return t\n",
        "\n",
        "\n",
        "r_ip = r_group('(\\s|^)' + ru_cap('–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å') + '\\s*' + '|(\\s|^)–ò–ü\\s*', 'ip')\n",
        "sub_ip_quoter = (re.compile(r_ip + r_human_name), r'\\1¬´\\g<human_name>¬ª')\n",
        "sub_org_name_quoter = (re.compile(r_quoted_name + '\\s*' + r_bracketed(r_types)), r'\\g<type> ¬´\\g<name>¬ª ')\n",
        "\n",
        "sub_alias_quote = (re.compile(r_alias_prefix + r_group(r_capitalized_ru, '_alias')), r'\\1¬´\\g<_alias>¬ª')\n",
        "alias_quote_regex = [\n",
        "  sub_alias_quote,\n",
        "  sub_ip_quoter,\n",
        "  sub_org_name_quoter\n",
        "]\n",
        "\n",
        "\n",
        "##################TEST\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "from contract_agents import  normalize_contract\n",
        "\n",
        " \n",
        "def _str_none(x):\n",
        "  if x is None:\n",
        "    return '-'\n",
        "  else:\n",
        "    return str(x)\n",
        "\n",
        "\n",
        "def orgs_to_tsv(org_names) -> str:\n",
        "  ret = ''\n",
        "\n",
        "  for i in range(len(org_names)):\n",
        "    ret += org_to_tsv(org_names[i])\n",
        " \n",
        "  return ret\n",
        "\n",
        "\n",
        "def org_to_tsv(org) -> str:\n",
        "  b = [_str_none(x[0]) for x in org.values()]\n",
        "  return '\\t'.join(b) + '\\t'\n",
        "\n",
        "\n",
        "fc = 0\n",
        "export_strings = ''\n",
        "for fn in contracts:  #\n",
        "\n",
        "  export_string = f'{fn.split(\"/\")[-1]}\\t'\n",
        "  fc += 1\n",
        "\n",
        "  txt = contracts[fn][0:2000]\n",
        "   \n",
        "  txt = normalize_contract(txt)\n",
        "  org_names = find_org_names(txt)\n",
        "\n",
        "  export_string += orgs_to_tsv(org_names)\n",
        "  export_strings += export_string + '\\n'\n",
        "  \n",
        "  \n",
        "print(export_strings)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wteSUb5zCNmk",
        "colab_type": "text"
      },
      "source": [
        "## Color all\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txps_koed7fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from contract_agents import find_org_names\n",
        "from text_tools import tokenize_text, tokens_in_range\n",
        "\n",
        "colormap = {\n",
        "  '0.type': (0, 1, 0.6),\n",
        "  '1.type': (0, 1, 0.5),\n",
        "\n",
        "  '0.name': (1, 0.7, 0),\n",
        "  '1.name': (1, 0.5, 0),\n",
        "\n",
        "  '0.alias': (0.3, 1, 0),\n",
        "  '1.alias': (0.2, 1, 0),\n",
        "\n",
        "  '0.alt_name': (0.6, 0.2, 0),\n",
        "  '1.alt_name': (0.6, 0.2, 0),\n",
        "\n",
        "}\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = list(contracts.keys())\n",
        "i = 0\n",
        "for fn in data:  #:[39:45]:\n",
        "  print(i, fn)\n",
        "  i += 1\n",
        "  con = contracts[fn]\n",
        "  SAMPLE_CONTRACT = normalize_contract(con)\n",
        "  TOKENS = tokenize_text(SAMPLE_CONTRACT)\n",
        "\n",
        "  d = find_org_names(SAMPLE_CONTRACT)\n",
        "\n",
        "  _first = tokens_in_range(d[0]['type'][1], TOKENS, SAMPLE_CONTRACT)\n",
        "\n",
        "  vectors = {}\n",
        "  o = 0\n",
        "  for org in d:\n",
        "    for k in org:\n",
        "      span = org[k][1]\n",
        "      if span[0] > 0:\n",
        "        _s = tokens_in_range(span, TOKENS, SAMPLE_CONTRACT)\n",
        "        attention_type = np.zeros(len(TOKENS))\n",
        "        attention_type[_s] = 0.8\n",
        "\n",
        "        key = f'{o}.{k}'\n",
        "\n",
        "        vectors[key] = attention_type\n",
        "        if key not in colormap:\n",
        "          colormap[key] = (1, 0, 0)\n",
        "\n",
        "    o += 1\n",
        "  try:\n",
        "    print('-' * 100)\n",
        "    GLOBALS__['renderer'].render_multicolor_text(TOKENS, vectors, colormap, min_color=(0.4, 0.4, 0.4),\n",
        "                                                 _slice=slice(_first.start, _first.start + 150))\n",
        "\n",
        "  except:\n",
        "    print('error', o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}