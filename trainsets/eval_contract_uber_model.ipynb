{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "DISABLE_GPU = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'USE_CONTROL_SET' not in globals():\n",
    "    USE_CONTROL_SET = True\n",
    "\n",
    "if 'COLLECTION_NAME' not in globals():\n",
    "    COLLECTION_NAME = 'documents_temp'\n",
    "    \n",
    "print (f'{COLLECTION_NAME=}')\n",
    "print (f'{USE_CONTROL_SET=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DISABLE GPU\n",
    "if DISABLE_GPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print(f'{DISABLE_GPU=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg"
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger('eval_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - [%(filename)s:%(lineno)d] - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.info('--=logging started=--')\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "  sys.path.append(nb_dir)\n",
    "\n",
    "import analyser.hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print('mlflow.active_run', mlflow.active_run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_mlflow_run = None\n",
    "if mlflow.active_run() is None:\n",
    "\n",
    "    ml_flow_url = os.environ.get('MLFLOW_URL', \"http://192.168.10.38:5000\")\n",
    "    mlflow.set_tracking_uri(ml_flow_url)\n",
    "    logger.warn(f'set MLFLOW_URL env var to re-define; MLFLOW_URL={ml_flow_url}')\n",
    "\n",
    "    mlflow.set_experiment(\"ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°\")\n",
    "    sub_mlflow_run = mlflow.start_run(nested=True)\n",
    " \n",
    "    print('sub_mlflow_run', sub_mlflow_run)\n",
    "    \n",
    "mlflow.set_tag(\"release.version\", analyser.__version__)\n",
    "mlflow.set_tag(\"test_use_control_set\", USE_CONTROL_SET)\n",
    "mlflow.set_tag(\"test_db_collection\", COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from bson import json_util\n",
    "from bson import ObjectId\n",
    "\n",
    "import traceback\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analyser.schemas import ContractPrice, merge_spans\n",
    "from analyser.finalizer import get_doc_by_id\n",
    "from analyser.documents import TextMap\n",
    "from analyser.ml_tools import SemanticTag\n",
    "from analyser.parsing import AuditContext\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from analyser.legal_docs import find_value_sign\n",
    "from analyser.transaction_values import ValueSpansFinder\n",
    "from analyser.text_tools import to_float, span_len\n",
    "\n",
    "from analyser.contract_parser import nn_get_tag_values, nn_find_contract_value\n",
    "from analyser.contract_parser import nn_find_org_names, nn_get_subject, nn_get_contract_number, nn_get_contract_date\n",
    "from analyser.contract_parser import fix_contract_number\n",
    "\n",
    "from tf_support.tools import KerasTrainingContext\n",
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "from tf_support.super_contract_model import make_xyw, semantic_map_keys_contract\n",
    "from tf_support.super_contract_model import validate_datapoint\n",
    "\n",
    "from integration.db import get_doc_by_id\n",
    "\n",
    "from trainsets.retrain_contract_uber_model import UberModelTrainsetManager\n",
    "\n",
    "from colab_support.renderer import plot_embedding, plot_cm\n",
    "from colab_support.renderer import HtmlRenderer\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "from tf_support.super_contract_model import t_semantic_map_keys_price, semantic_map_keys, t_semantic_map_keys_common, t_semantic_map_keys_org\n",
    "\n",
    "\n",
    "semantic_keys_numeric = ['amount', 'amount_brutto', 'amount_netto']\n",
    "org_keys=['name', 'alias', 'type']\n",
    "org_numbered_keys=[ f'org-1-{v}' for v in org_keys]\n",
    "org_numbered_keys+=[ f'org-2-{v}' for v in org_keys]\n",
    "org_numbered_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DemoRenderer(HtmlRenderer):\n",
    "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
    "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
    "    display(HTML(html))\n",
    "\n",
    "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
    "    return super()._to_color_text(tokens, weights, matplotlib, colormap=colormap, _range=_range, separator=separator)\n",
    "\n",
    "renderer_ = DemoRenderer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRQOy7o0uyTv"
   },
   "source": [
    "# Prepare paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "work_dir = Path(analyser.hyperparams.work_dir)\n",
    "training_reports_path = Path(analyser.hyperparams.__file__).parent.parent / 'training_reports/'\n",
    "\n",
    "print(f'{training_reports_path=}')\n",
    "print(f'{analyser.hyperparams.work_dir=}')\n",
    "print(f'{work_dir=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_semantic_map_keys_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading control test set from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from integration.db import get_mongodb_connection\n",
    "from bson import ObjectId\n",
    "\n",
    "def doc_as_table_row(jd:DbJsonDoc, df, col_suffix='_expected'):\n",
    "    _id = str(jd._id)\n",
    "    tree = jd.get_attributes_tree()\n",
    "    df.at[ _id, f'number{col_suffix}'] = jd.get_attribute_value('number')\n",
    "    df.at[ _id, f'date{col_suffix}']   = jd.get_attribute_value('date')\n",
    "    df.at[ _id, f'subject{col_suffix}']= jd.get_attribute_value('subject')\n",
    "    df.at[ _id, 'subject']= jd.get_attribute_value('subject')\n",
    "\n",
    "    orgs = tree.get('orgs', [])\n",
    "\n",
    "    if len(orgs)>0:\n",
    "        o1=orgs[0]\n",
    "    else:\n",
    "        o1={}\n",
    "\n",
    "    if len(orgs)>1:\n",
    "        o2=orgs[1]\n",
    "    else:\n",
    "        o2={}\n",
    "\n",
    "    for part in org_keys:\n",
    "        v1 = o1.get(part, {}).get('value', \"\").lower()\n",
    "        v2 = o2.get(part, {}).get('value', \"\").lower()\n",
    "        vv=sorted([v1,v2])\n",
    "        df.at[ _id, f'org-1-{part}{col_suffix}'] = vv[1]\n",
    "        df.at[ _id, f'org-2-{part}{col_suffix}'] = vv[0]\n",
    "\n",
    "\n",
    "    for v in t_semantic_map_keys_price[0:4]:\n",
    "        df.at[ _id, f'{v}{col_suffix}'] = -1\n",
    "    for v in t_semantic_map_keys_price[5:]:\n",
    "        df.at[ _id, f'{v}{col_suffix}'] = ''\n",
    "        \n",
    "    for v in t_semantic_map_keys_price:\n",
    "        df.at[ _id, f'{v}{col_suffix}'] =  tree.get('price', {}).get(v, {}).get('value')\n",
    "\n",
    "    df.at[ _id, f'analysis.version{col_suffix}'] = jd.analysis['version']\n",
    "    return jd\n",
    "\n",
    "\n",
    "\n",
    "def load_doc_as_table_row(documents_collection, _id, df, col_suffix='_expected'):\n",
    "    doc = documents_collection.find_one({'_id': ObjectId(_id)})  \n",
    "    if doc is None:\n",
    "        print (f'{_id} not found in db')\n",
    "        return\n",
    "\n",
    "    jd = DbJsonDoc(doc)\n",
    "    doc_as_table_row(jd, df, col_suffix)\n",
    "    return jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    \n",
    " \n",
    "if USE_CONTROL_SET:    \n",
    "    query = {\n",
    "      '$and': [\n",
    "        {\"parse.documentType\":{ '$in': [\"AGREEMENT\", \"CONTRACT\", \"SUPPLEMENTARY_AGREEMENT\"] }  },      \n",
    "        { 'subset': 'CONTROL_TEST'}\n",
    "      ]\n",
    "    }    \n",
    "\n",
    "    res = documents_collection.find(filter=query, \n",
    "                                    projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "                                   ).limit(5000)\n",
    "\n",
    "\n",
    "    test_meta = DataFrame()\n",
    "\n",
    "    for i in res:\n",
    "        load_doc_as_table_row(documents_collection, str(i[\"_id\"]), test_meta)\n",
    "\n",
    "    test_meta['sample_weight'] = 1\n",
    "    test_meta['subject_weight'] = 1\n",
    "\n",
    "    # test_ids  = [i[\"_id\"] for i in res]\n",
    "    # print(len(test_ids))\n",
    "\n",
    "    _s = f\"#### {len(test_meta)} -- total test docs in {COLLECTION_NAME} collection\"\n",
    "    display(Markdown(_s))\n",
    "    \n",
    "    # mlflow.log_param('test set', len(test_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainsets.retrain_contract_uber_model import save_contract_data_arrays\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def recreate_data_point(_id:str, test_meta):\n",
    "    try:\n",
    "        doc = documents_collection.find_one({'_id': ObjectId(_id)})  \n",
    "        jd = DbJsonDoc(doc)\n",
    "        save_contract_data_arrays(jd)\n",
    "\n",
    "        test_meta.at[_id, 'valid'] = True\n",
    "        test_meta.at[_id, 'error'] = ''\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(e)\n",
    "        logger.exception(e)\n",
    "\n",
    "        test_meta.at[_id, 'valid'] = False\n",
    "        test_meta.at[_id, 'error'] = str(e)\n",
    "            \n",
    "                        \n",
    "if USE_CONTROL_SET:\n",
    "    test_meta['error']=''\n",
    "    test_meta['valid']=True\n",
    "\n",
    "    \n",
    "    for k, i in enumerate(test_meta.index):\n",
    "      print(i, 'validating....')\n",
    "      if k % 10 == 0:\n",
    "          clear_output(wait=True)\n",
    "\n",
    "      try:\n",
    "        validate_datapoint(str(i), test_meta)\n",
    "        print(i, 'is ok')\n",
    "      except Exception as e:\n",
    "        logger.error(e)\n",
    "        # logger.exceptoin(e)\n",
    "\n",
    "        test_meta.at[i, 'valid'] = False\n",
    "        test_meta.at[i, 'error'] = str(e)\n",
    "\n",
    "        recreate_data_point(str(i), test_meta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loading data set meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not USE_CONTROL_SET:\n",
    "\n",
    "    umtm = UberModelTrainsetManager (work_dir, training_reports_path)\n",
    "    umtm.load_contract_trainset_meta()\n",
    "    stats = umtm.stats\n",
    "    stats['sample_weight']  = -1.0 #TODO: describe why?\n",
    "    stats['subject_weight'] = -1.0\n",
    "\n",
    "    _s = f\"#### {len(stats)} -- total records in contract_trainset_meta\"\n",
    "    display(Markdown(_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxEdSGOuq62R"
   },
   "source": [
    "## Validate data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not USE_CONTROL_SET:\n",
    "\n",
    "    # stats['valid'] = True\n",
    "    stats['error'] = ''\n",
    "\n",
    "    for i in stats.index:\n",
    "\n",
    "      try:\n",
    "        validate_datapoint(str(i), stats)\n",
    "\n",
    "      except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "        stats.at[i, 'valid'] = False\n",
    "        stats.at[i, 'error'] = str(e)\n",
    "         \n",
    "        recreate_data_point(str(i), stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not USE_CONTROL_SET:\n",
    "    display(Markdown(f'####  {len(stats[stats.valid == False])} invalid records'))    \n",
    "    stats[stats.valid == False]['error']\n",
    "\n",
    "    stats_valid = stats[stats['valid']]\n",
    "    display(Markdown(f'####  {len(stats_valid)} valid records'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Debug] Reading legacy docs from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn = work_dir / 'documents.json'\n",
    "legacy_json_exists = fn.is_file()\n",
    "\n",
    "if DEBUG and legacy_json_exists:\n",
    "\n",
    "    with open(fn) as file:\n",
    "        file_data = json.load(file, object_hook=json_util.object_hook)    \n",
    "        display(Markdown(f'#### {len(file_data)} total docs in {fn}'))  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Debug] Validating legacy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_file_meta = DataFrame()\n",
    "json_file_meta['sample_weight']=1\n",
    "json_file_meta['subject_weight']=1\n",
    "\n",
    "if DEBUG and legacy_json_exists:\n",
    "\n",
    "    docs = {}\n",
    "    errors = 0\n",
    "\n",
    "    for fd in file_data:\n",
    "        try:\n",
    "          # validate_datapoint(str(fd['_id']), json_file_meta)\n",
    "          jd = DbJsonDoc(fd)\n",
    "          docs [fd['_id']] =  jd\n",
    "          doc_as_table_row(jd, json_file_meta, col_suffix='' )\n",
    "        \n",
    "        except NameError as e:\n",
    "            raise e\n",
    "            \n",
    "        except KeyError as e:\n",
    "          errors += 1\n",
    "          logger.error(f'No key in json_file_meta: {str(e)}')\n",
    "            \n",
    "        except Exception as e:\n",
    "          errors += 1\n",
    "\n",
    "          logger.error(f'{type(e)}, {str(e)}')\n",
    "          traceback.print_exc()\n",
    "\n",
    "    display(Markdown(f'#### {errors} invalid docs in in {fn}')) \n",
    "    display(Markdown(f'#### {len( list( docs.values() )  )} valid docs in {fn}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Debug] Get sample doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    a_doc_from_json = list(docs.values())[1]\n",
    "    print(a_doc_from_json.get_tokens_map_unchaged().text[:230])\n",
    "    \n",
    "    a_doc_from_json.get_attributes_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_kJ0L2RnrHl3",
    "outputId": "4863cd27-1459-458a-951d-b054235ffaea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "if DEBUG and legacy_json_exists:\n",
    "    SAMPLE_DOC_ID = str(a_doc_from_json.get_id()) # stats_valid.index[0]\n",
    "\n",
    "    print(f'{SAMPLE_DOC_ID=}')\n",
    "\n",
    "    (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = make_xyw(SAMPLE_DOC_ID, json_file_meta)\n",
    "\n",
    "\n",
    "    print(f'semantic map shape {sm.shape=}')\n",
    "    _crop = 700\n",
    "    plot_embedding(tok_f[:_crop], title=f'Tokens features {SAMPLE_DOC_ID}') \n",
    "    plot_embedding(emb[:_crop], title=f'Embedding {SAMPLE_DOC_ID}') \n",
    "    plot_embedding(sm[:_crop], title=f'Semantic map {SAMPLE_DOC_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAFmo0sG4H9k"
   },
   "source": [
    "# Init Model ðŸ¦–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TEST_FLOW' not in globals():\n",
    "    TEST_FLOW = False\n",
    "print(f'{TEST_FLOW=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'model_factory_fn' in globals():\n",
    "    print ('*'*80)\n",
    "    print('model_factory_fn defined by external process as', model_factory_fn.__name__)\n",
    "else:\n",
    "    from tf_support.super_contract_model import make_att_model\n",
    "    model_factory_fn = make_att_model\n",
    "\n",
    "print(f'{model_factory_fn=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ctx = KerasTrainingContext(work_dir, session_index=21)\n",
    "ctx.EVALUATE_ONLY = True\n",
    " \n",
    "    \n",
    "if 'umodel' in globals() and umodel is not None:\n",
    "    print ('*'*80)\n",
    "    print('umodel defined as', umodel.name)\n",
    "else:\n",
    " \n",
    "    weights = training_reports_path / f'{model_factory_fn.__name__}.h5'\n",
    "    mlflow.log_param('weights', str(weights))\n",
    "    \n",
    "    logger.warn (f'LOADING: {model_factory_fn} -- {weights}')\n",
    "    \n",
    "    \n",
    "    umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, weights=weights)\n",
    "    mlflow.log_param('model_name', str(umodel.name))\n",
    "    mlflow.log_param('model_params', umodel.count_params())\n",
    "    \n",
    "    \n",
    "umodel.trainable = False\n",
    "umodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUum89Tdhg-9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# [Debug] Check model on a sigle doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debug] Evaluate single doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_index = umtm.stats [umtm.stats['value']>0].index[2]\n",
    "\n",
    "if DEBUG and legacy_json_exists:\n",
    "    print(f'{SAMPLE_DOC_ID=}')\n",
    "    prediction = umodel.predict(x=[np.expand_dims(emb, axis=0), np.expand_dims(tok_f, axis=0)], batch_size=1)\n",
    "\n",
    "\n",
    "    tagsmap = pd.DataFrame(prediction[0][0], columns=semantic_map_keys_contract)\n",
    "    tagsmap_e = pd.DataFrame(sm, columns=semantic_map_keys_contract)\n",
    "    delta = tagsmap - tagsmap_e \n",
    "    # .T\n",
    "    plot_embedding(tagsmap[:_crop], f'Predicted Semantic Map {tagsmap.shape}')\n",
    "    plot_embedding(delta[:_crop], title=f'DELTA Semantic map {tagsmap_e.shape}')\n",
    "    plot_embedding(tagsmap_e[:_crop], title=f'EXPECTED Semantic map {tagsmap_e.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    print(\"mean delta\", delta.abs().sum().sum() / tagsmap_e.sum().sum())\n",
    "    print(\"sum of delatas\", delta.abs().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "if DEBUG and legacy_json_exists:\n",
    "    av = tagsmap.max(axis=1) #tagsmap['amount-begin'] + tagsmap['vat-begin'] + tagsmap['number-begin'] + tagsmap['org-name-begin']\n",
    "\n",
    "    # av = tagsmap.sum(axis=1)\n",
    "    renderer_.render_color_text(a_doc_from_json.get_tokens_map_unchaged().tokens[:600], av[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Debug mode only] Getting tag values from inferred semantic map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AuditContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    cas = nn_find_org_names(a_doc_from_json.get_tokens_map_unchaged(), tagsmap, ac)\n",
    "    if cas:\n",
    "        if len(cas)>0:\n",
    "\n",
    "            print(cas[0].name)\n",
    "            print(cas[0].type)\n",
    "            print(cas[0].alias)\n",
    "        if len(cas)>1:\n",
    "            print()\n",
    "            print(cas[1].name)\n",
    "            print(cas[1].type)\n",
    "            print(cas[1].alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date/number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    date_tag = nn_get_contract_date     (a_doc_from_json.get_tokens_map_unchaged(), tagsmap) \n",
    "    number_tag = nn_get_contract_number (a_doc_from_json.get_tokens_map_unchaged(), tagsmap)\n",
    "    if date_tag:\n",
    "        print( f'{date_tag.value=}')\n",
    "    if number_tag:\n",
    "        print( f'{number_tag.value=}' )\n",
    "    \n",
    " \n",
    "    attention = tagsmap['date' + '-begin'].values.copy()\n",
    "\n",
    "    threshold = max(attention.max() * 0.8, 0.1)\n",
    "    print(f'{attention.max()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    textmap = a_doc_from_json.get_tokens_map_unchaged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    #---\n",
    "    cps = nn_find_contract_value(textmap, tagsmap)\n",
    "    if cps:\n",
    "      print(str(cps[0].get_span()))\n",
    "      for k in cps[0].list_children():\n",
    "        print(str(k))\n",
    "\n",
    "      print()\n",
    "      print()\n",
    "\n",
    "      print('brutto', str(cps[0].amount_brutto))\n",
    "      print('netto', str(cps[0].amount_netto))\n",
    "      print('amount', str(cps[0].amount))\n",
    "      print('vat', str(cps[0].vat))\n",
    "    else:\n",
    "      print('nothing found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    thresholds = dict(tagsmap.max()*.8)\n",
    "    thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG and legacy_json_exists:\n",
    "    subject_tag = nn_get_tag_values('subject',  a_doc_from_json.get_tokens_map_unchaged(), tagsmap, max_tokens=200, threshold=0.02, limit=1, return_single=True)\n",
    "\n",
    "    print(subject_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ev = None\n",
    "\n",
    "if not USE_CONTROL_SET:\n",
    "    ev =   umtm.stats.copy()\n",
    "else:\n",
    "    ev = test_meta.copy()\n",
    "    \n",
    "    \n",
    "tags =          pd.DataFrame()\n",
    "errors_report = pd.DataFrame()\n",
    "\n",
    "\n",
    "if USE_CONTROL_SET:\n",
    "    userdocs = ev\n",
    "else:\n",
    "    userdocs = ev[ev.unseen==False]    \n",
    "    userdocs = userdocs[userdocs.source=='db']\n",
    "    userdocs = userdocs[userdocs.score < 50000]\n",
    "    \n",
    "userdocs = userdocs[userdocs['valid']==True]\n",
    "\n",
    "\n",
    "display(Markdown(f'#### {len(userdocs)} (userdocs) in total for evaluation'))   \n",
    "mlflow.log_param('test set', len(userdocs))\n",
    "\n",
    "if len(test_meta) != len(userdocs):\n",
    "    mlflow.log_param('test set invalid', len(test_meta) - len(userdocs) )\n",
    "display(Markdown(f'#### {len(test_meta) -  len(userdocs)} invalid docs'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "try:\n",
    "    cnt = userdocs['subject'].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(12, 6 ))\n",
    "    sns.barplot(x=cnt.values, y=cnt.index)\n",
    "\n",
    "    print(  cnt )\n",
    "\n",
    "    plt.title(f'test: Frequency Distribution of subjects; {len(userdocs)} total')\n",
    "    plt.xlabel('Number of Occurrences')\n",
    "    \n",
    "    plt.savefig( training_reports_path / 'Distribution of subjects -test.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(training_reports_path / 'Distribution of subjects -test.png')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for v in semantic_map_keys:\n",
    "    for s in ['_expected', '_predicted']:\n",
    "        userdocs[f'{v}{s}'] = ''\n",
    "        userdocs[f'{v}{s}'] = userdocs[f'{v}{s}'].astype(str)\n",
    "        \n",
    "for v in semantic_keys_numeric:\n",
    "    for s in ['_expected', '_predicted']:\n",
    "        userdocs[f'{v}{s}'] = np.NaN\n",
    "        userdocs[f'{v}{s}'] = userdocs[f'{v}{s}'].astype(float)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "maxlen = 128 * 12\n",
    "\n",
    "\n",
    "ac = AuditContext()\n",
    "\n",
    "def interpret_prediction(_id, tagsmap, df):\n",
    "    col_suffix=\"_predicted\"\n",
    "    \n",
    "    doc = load_doc_as_table_row(documents_collection, _id, df)\n",
    "    if doc is None: \n",
    "        return\n",
    "    \n",
    "    tokens = doc.get_tokens_map_unchaged()\n",
    "    \n",
    "    #---\n",
    "    # ORGS ------------------\n",
    "    orgs = nn_find_org_names(tokens, tagsmap, ac)\n",
    "    if len(orgs)>0:\n",
    "        o1 = orgs[0]\n",
    "    else:\n",
    "        o1={}\n",
    "        \n",
    "    if len(orgs)>1:\n",
    "        o2 = orgs[1]\n",
    "    else:\n",
    "        o2={}\n",
    "        \n",
    "    for part in ['name', 'alias', 'type']:\n",
    "        part_tag1 = getattr(o1, part, {})\n",
    "        part_tag2 = getattr(o2, part, {})\n",
    "        \n",
    "        v1 = getattr(part_tag1, 'value', '').lower()\n",
    "        v2 = getattr(part_tag2, 'value', '').lower()\n",
    "#         vv= [v1,v2]\n",
    "        vv=sorted([v1,v2])\n",
    "        df.at[ _id, f'org-1-{part}{col_suffix}'] = vv[1] \n",
    "        df.at[ _id, f'org-2-{part}{col_suffix}'] = vv[0] \n",
    " \n",
    "    # PRICE ------------------\n",
    "    cps = nn_find_contract_value(tokens, tagsmap)\n",
    "    if cps:\n",
    "        cps=cps[0]\n",
    "        for v in t_semantic_map_keys_price:\n",
    "            part = getattr(cps, v)\n",
    "            if part:\n",
    "                df.at[ _id, f'{v}{col_suffix}'] = part.value\n",
    "                 \n",
    " \n",
    "    # DATE NUMBER ------------------\n",
    "    number_tag = nn_get_contract_number(tokens, tagsmap)\n",
    "    date_tag =   nn_get_contract_date  (tokens, tagsmap) \n",
    "    \n",
    "        \n",
    "    if number_tag:\n",
    "        df.at[ _id, f'number{col_suffix}'] = str(number_tag.value)\n",
    "    else:\n",
    "        df.at[ _id, f'number{col_suffix}'] = ''\n",
    "        \n",
    "        \n",
    "    if date_tag:\n",
    "        df.at[ _id, f'date{col_suffix}'] = str(date_tag.value)\n",
    "    else:\n",
    "        df.at[ _id, f'date{col_suffix}'] = ''\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_set = userdocs\n",
    "\n",
    "if TEST_FLOW:\n",
    "    validation_set = userdocs[0:20]\n",
    "    \n",
    "    \n",
    "for i in range(0, len(validation_set), batch_size):\n",
    "    batch = userdocs[i:i+batch_size]\n",
    "    actual_batch_size=len(batch)\n",
    "    print(f'{actual_batch_size=}')\n",
    "    batch_input_emb=[]\n",
    "    batch_input_token_f=[]\n",
    "    for _id in batch.index.values:\n",
    " \n",
    "        dp = make_xyw(_id, userdocs)\n",
    "        dp = UberModelTrainsetManager.trim_maxlen(dp, 0, maxlen  )\n",
    "        (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "        batch_input_emb.append(emb)\n",
    "        batch_input_token_f.append(tok_f)\n",
    "        \n",
    "        expected_subj =  decode_subj_prediction(subj)[0]\n",
    "        userdocs.at[_id, 'subject_expected'] = expected_subj.name\n",
    " \n",
    "\n",
    "    del _id\n",
    "    del dp\n",
    "\n",
    "    \n",
    "    dps =  [np.array(batch_input_emb), np.array(batch_input_token_f)]\n",
    " \n",
    "    \n",
    "    predictions = umodel.predict(x=dps, batch_size=actual_batch_size)\n",
    "    del batch_input_emb\n",
    "    del batch_input_token_f\n",
    "    \n",
    "    p_tags = predictions[0]\n",
    "    p_subj = predictions[1]\n",
    "    for k,_id in enumerate(batch.index.values):\n",
    "                   \n",
    "        tagsmap = pd.DataFrame(p_tags[k], columns=semantic_map_keys_contract)\n",
    "        interpret_prediction(_id, tagsmap, userdocs)\n",
    "        \n",
    "        subj_1hot = p_subj[k]\n",
    "\n",
    "        predicted = decode_subj_prediction(subj_1hot)[0]\n",
    "        userdocs.at[_id, 'subject_predicted'] = predicted.name\n",
    "        \n",
    "userdocs[\n",
    "  ['org-1-type_expected', 'org-1-type_predicted', 'org-1-name_expected', 'org-1-name_predicted', 'org-1-alias_expected', 'org-1-alias_predicted', \n",
    "   'org-2-type_expected', 'org-2-type_predicted', 'org-2-name_expected', 'org-2-name_predicted', 'org-2-alias_expected', 'org-2-alias_predicted', \n",
    "   'subject_expected', 'subject_predicted', 'date_expected',\n",
    "   'date_predicted', 'number_expected', 'number_predicted']].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean predicted/expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = []\n",
    "for v in semantic_keys_numeric:\n",
    "    for s in ['_expected', '_predicted' ]:\n",
    "        col = f'{v}{s}'\n",
    "        userdocs[col] = userdocs[col].replace('',np.NaN).replace(np.NaN, -1).astype(int)\n",
    "        columns.append(col)\n",
    "        \n",
    "userdocs['date_predicted'] = userdocs['date_predicted'].astype(str)        \n",
    "userdocs['date_expected'] = userdocs['date_expected'].astype(str)        \n",
    "# userdocs[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdocs=userdocs.replace('None', '')\n",
    "userdocs=userdocs.replace('none', '')\n",
    "userdocs=userdocs.replace('nan', '')\n",
    "\n",
    "userdocs.date_expected =  userdocs.date_expected.replace(np.NaN, '')\n",
    "userdocs.date_predicted = userdocs.date_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.number_expected =  userdocs.number_expected.replace(np.NaN, '')\n",
    "userdocs.number_predicted = userdocs.number_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.vat_unit_expected =  userdocs.vat_unit_expected.replace(np.NaN, '')\n",
    "userdocs.vat_unit_predicted = userdocs.vat_unit_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.vat_expected =  userdocs.vat_expected.replace(np.NaN, '')\n",
    "userdocs.vat_predicted = userdocs.vat_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.sign_expected =  userdocs.sign_expected.replace(np.NaN, '')\n",
    "userdocs.sign_predicted = userdocs.sign_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.currency_expected =  userdocs.currency_expected.replace(np.NaN, '')\n",
    "userdocs.currency_predicted = userdocs.currency_predicted.replace(np.NaN, '')\n",
    "\n",
    "# userdocs[['date_expected', 'date_predicted', 'number_expected', 'number_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_wrong(userdocs, key):\n",
    "    s= userdocs[userdocs[f'{key}_expected'] != userdocs[f'{key}_predicted']][[f'{key}_expected', f'{key}_predicted']]\n",
    "    p=float(len(s)) / len(userdocs)\n",
    "    return s,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags report\n",
    "## accuracy report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userdocs['number_of_errors'] = 0\n",
    "report = pd.DataFrame()\n",
    "\n",
    "n   = 0\n",
    "avg = 0\n",
    "\n",
    "for k in t_semantic_map_keys_common[1:] + t_semantic_map_keys_price + org_numbered_keys :\n",
    "    s, p = select_wrong(userdocs, k)\n",
    "    for _id, _ in s.iterrows():\n",
    "       userdocs.at[_id, 'number_of_errors'] += 1\n",
    "    \n",
    "    acc = 1.0 - p\n",
    "    report.at[k, 'accuracy']=f\"{acc:.1%}\"\n",
    "    mlflow.log_metric(f\"accuracy_{k}\", acc)\n",
    "    \n",
    "    n += 1\n",
    "    avg += acc\n",
    "    \n",
    "avg = avg / n\n",
    "\n",
    "report.at['ALL', 'accuracy']=f\"{avg:.1%}\"\n",
    "mlflow.log_metric(\"accuracy_TAGS\", avg)\n",
    "report.to_csv(training_reports_path /'attributes_accuracy.csv')    \n",
    "mlflow.log_artifact(training_reports_path / 'attributes_accuracy.csv')\n",
    "\n",
    "# 81.6%\n",
    "report    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst docs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdocs[['number_of_errors']].sort_values('number_of_errors', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjects predictions reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = userdocs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh1WFsjprUr1"
   },
   "outputs": [],
   "source": [
    "subj_pred = ev[pd.notna(ev.subject_predicted)][pd.notna(ev.subject_expected)]\n",
    "subj_df = subj_pred[['subject_predicted', 'subject_expected']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = sorted(np.unique(subj_df['subject_expected'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(subj_df.subject_expected, subj_df.subject_predicted, digits=3, output_dict=True)\n",
    "report['weighted avg']\n",
    "mlflow.log_metrics(report['weighted avg'])\n",
    "mlflow.log_metric('subject_F1',report['weighted avg']['f1-score'])\n",
    "mlflow.log_metric('subject_F1_support',report['weighted avg']['support'])\n",
    "mlflow.log_metric('subject_precision',report['weighted avg']['precision'])\n",
    "mlflow.log_metric('subject_recall',report['weighted avg']['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFh8SxP9J081"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_report(umodel, subj_df):\n",
    "  plot_cm(subj_df.subject_expected.values, subj_df.subject_predicted.values, figsize=(12, 12))\n",
    "  \n",
    "  img_path = training_reports_path / f'subjects-confusion-matrix-{umodel.name}.png'\n",
    "  plt.savefig(img_path, bbox_inches='tight')\n",
    "  mlflow.log_artifact(img_path)\n",
    "\n",
    "  report = classification_report(subj_df.subject_expected, subj_df.subject_predicted, digits=3)\n",
    "  print(umodel.name)\n",
    "  print(report)\n",
    "\n",
    "  fn = training_reports_path / f'subjects-classification_report-{umodel.name}.txt'\n",
    "  with open(fn, \"w\") as text_file:\n",
    "    text_file.write(report)\n",
    " \n",
    "  mlflow.log_artifact(fn)\n",
    "\n",
    "\n",
    "make_report(umodel, subj_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sub_mlflow_run is not None:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
