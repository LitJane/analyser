{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger('eva;_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - [%(filename)s:%(lineno)d] - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.debug('--=logging started=--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Dehw0fnKfSBF",
    "outputId": "3bcb61cd-401a-43d7-dccf-9a6022b2a576"
   },
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print ('Running in colab:', IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBqnGnQfKWF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "  nb_dir = os.path.split(os.getcwd())[0]\n",
    "  if nb_dir not in sys.path:\n",
    "      sys.path.append(nb_dir)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import analyser.hyperparams\n",
    "import mlflow\n",
    "\n",
    "print(mlflow.active_run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_mlflow_run = None\n",
    "if mlflow.active_run() is None:\n",
    "    # mlflow.start_run(run_name='fetch trainset from db')\n",
    "    ml_flow_url = os.environ.get('MLFLOW_URL', \"http://192.168.10.38:5000\")\n",
    "    mlflow.set_tracking_uri(ml_flow_url)\n",
    "    print(f'{ml_flow_url=}', 'set MLFLOW_URL env var to re-define')\n",
    "\n",
    "    mlflow.set_experiment(\"Обучение анализатора\")\n",
    "    sub_mlflow_run = mlflow.start_run(nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from analyser.finalizer import get_doc_by_id\n",
    "from analyser.documents import TextMap\n",
    "from analyser.ml_tools import SemanticTag\n",
    "\n",
    "from analyser.contract_parser import nn_get_tag_values\n",
    "from analyser.contract_parser import nn_find_org_names, nn_get_subject, nn_get_contract_number, nn_get_contract_date\n",
    "from analyser.parsing import AuditContext\n",
    "\n",
    "\n",
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "from integration.db import get_doc_by_id\n",
    "from bson import ObjectId\n",
    " \n",
    "from trainsets.retrain_contract_uber_model import UberModelTrainsetManager\n",
    "from tf_support.super_contract_model import semantic_map_keys_contract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from colab_support.renderer import HtmlRenderer\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "class DemoRenderer(HtmlRenderer):\n",
    "  def render_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
    "    html = self.to_color_text(tokens, weights, colormap, print_debug, _range, separator=separator)\n",
    "    display(HTML(html))\n",
    "\n",
    "  def to_color_text(self, tokens, weights, colormap='coolwarm', print_debug=False, _range=None, separator=' '):\n",
    "    return super()._to_color_text(tokens, weights, matplotlib, colormap=colormap, _range=_range, separator=separator)\n",
    "\n",
    "renderer_ = DemoRenderer()\n",
    "\n",
    "\n",
    "# renderer_.render_color_text([\"слово 1\", \"слово 2\"], np.array( [1, 0]), _range=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRQOy7o0uyTv"
   },
   "source": [
    "# Prepare paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "work_dir = Path(analyser.hyperparams.work_dir)\n",
    "# del work_dir\n",
    "# # print(work_dir)\n",
    "\n",
    "training_reports_path = Path(analyser.hyperparams.__file__).parent.parent / 'training_reports/'\n",
    "print(f'{training_reports_path=}')\n",
    "print(f'{analyser.hyperparams.work_dir=}')\n",
    "print(f'{work_dir=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data set meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umtm = UberModelTrainsetManager (work_dir, training_reports_path)\n",
    "umtm.load_contract_trainset_meta()\n",
    "stats = umtm.stats\n",
    "stats['sample_weight']=-1.0 #TODO: WHY?\n",
    "stats['subject_weight']=-1.0\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxEdSGOuq62R"
   },
   "source": [
    "## Validate trainset (take a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import  validate_datapoint\n",
    "\n",
    "# stats['valid'] = True\n",
    "stats['error'] = ''\n",
    "\n",
    "for i in stats.index:\n",
    "  \n",
    "  try:\n",
    "    validate_datapoint(str(i), stats)\n",
    "\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "\n",
    "    stats.at[i, 'valid'] = False\n",
    "    stats.at[i, 'error'] = str(e)\n",
    "    \n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_valid = stats[stats['valid']]\n",
    "# stats_valid = stats_valid[stats_valid.source=='file']\n",
    "len(stats_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bson import json_util\n",
    "\n",
    "fn = work_dir / 'documents.json'\n",
    "with open(fn) as file:\n",
    "    file_data = json.load(file, object_hook=json_util.object_hook)    \n",
    "    print(f'total docs in {fn} is {len(file_data)}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validating data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_datapoint(id: str, meta: DataFrame):\n",
    "  try:\n",
    "    (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = make_xyw(id, meta)\n",
    "    if sm.shape[1] != len(semantic_map_keys_contract):\n",
    "      mxs = f'semantic map shape is {sm.shape[1]}, expected is {len(semantic_map_keys_contract)} source={meta.at[id, \"source\"]}'\n",
    "      raise ValueError(mxs)\n",
    "\n",
    "  except Exception as e:\n",
    "    raise e\n",
    "    \n",
    "# validate_datapoint('5deba9034ddc27bcf92dd383', stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "\n",
    "    def test_date_tags_detector(doc_id):\n",
    "\n",
    "        try:\n",
    "            jd = DbJsonDoc(get_doc_by_id(ObjectId(doc_id)))\n",
    "\n",
    "\n",
    "            (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = make_xyw(doc_id, stats)\n",
    "            sm = pd.DataFrame( sm, columns= semantic_map_keys_contract) \n",
    "\n",
    "            user_date_val = jd.user['attributes_tree']['contract']['date']            \n",
    "            date_tag_____ = nn_get_contract_date     (jd.get_tokens_map_unchaged(), sm)\n",
    "\n",
    "            if date_tag_____.value != user_date_val['value']:\n",
    "\n",
    "                print(f\"{date_tag_____.span=}\\t\\t{date_tag_____.value=} \")\n",
    "\n",
    "                print(f\"{user_date_val['span']=}\\t\\t{user_date_val['value']=}\")\n",
    "                print(f\"{date_tag_____.value == user_date_val['value']}\")\n",
    "                print(doc_id, '_'*50)\n",
    "        except:\n",
    "            pass\n",
    "    #         print(doc_id, 'no date')\n",
    "\n",
    "    for k in range(0,len(stats[stats.source=='db'])):\n",
    "        test_date_tags_detector(stats[stats.source=='db'].index[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analyser.persistence import DbJsonDoc\n",
    "from tf_support.super_contract_model import make_xyw\n",
    "\n",
    "import traceback\n",
    "\n",
    "docs = {}\n",
    "errors = 0\n",
    "for fd in file_data:\n",
    "    try:\n",
    "      validate_datapoint(str(fd['_id']), stats)\n",
    "      docs [fd['_id']] =  DbJsonDoc(fd)\n",
    "#       print (fd['_id'])\n",
    "    except KeyError as e:\n",
    "      errors += 1\n",
    "      logger.error(f'No key in stats: {str(e)}')\n",
    "    except Exception as e:\n",
    "      errors += 1\n",
    "      \n",
    "      logger.error(f'{type(e)}, {str(e)}')\n",
    "      traceback.print_exc()\n",
    "\n",
    "print('errors count:', errors)\n",
    "print(f'total docs in {fn} is {len(list(docs.values()))}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sample doc (DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    a_doc_from_json = list(docs.values())[1]\n",
    "    print(a_doc_from_json.get_tokens_map_unchaged().text[:230])\n",
    "    \n",
    "    a_doc_from_json.get_attributes_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_kJ0L2RnrHl3",
    "outputId": "4863cd27-1459-458a-951d-b054235ffaea"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from colab_support.renderer import plot_embedding, plot_cm\n",
    "from tf_support.super_contract_model import make_xyw\n",
    "\n",
    "if DEBUG:\n",
    "    SAMPLE_DOC_ID = str(a_doc_from_json.get_id())# stats_valid.index[0]\n",
    "\n",
    "\n",
    "    # SAMPLE_DOC_ID = '5fdb2145542ce403c92b460c'\n",
    "    # del a_doc_from_json\n",
    "\n",
    "    print(f'{SAMPLE_DOC_ID=}')\n",
    "\n",
    "    (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = make_xyw(SAMPLE_DOC_ID, stats)\n",
    "\n",
    "\n",
    "    print(f'semantic map shape {sm.shape=}')\n",
    "    _crop = 700\n",
    "    plot_embedding(tok_f[:_crop], title=f'Tokens features {SAMPLE_DOC_ID}') \n",
    "    plot_embedding(emb[:_crop], title=f'Embedding {SAMPLE_DOC_ID}') \n",
    "    plot_embedding(sm[:_crop], title=f'Semantic map {SAMPLE_DOC_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAFmo0sG4H9k"
   },
   "source": [
    "# Init Model 🦖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TEST_FLOW' not in globals():\n",
    "    TEST_FLOW = False\n",
    "print(f'{TEST_FLOW=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'model_factory_fn' in globals():\n",
    "    print ('*'*80)\n",
    "    print('model_factory_fn defined by external process as', model_factory_fn.__name__)\n",
    "else:\n",
    "    from tf_support.super_contract_model import make_att_model\n",
    "    model_factory_fn = make_att_model\n",
    "\n",
    "print(f'{model_factory_fn=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://mlflow.org/docs/latest/models.html#keras-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from tf_support.tools import KerasTrainingContext\n",
    "\n",
    " \n",
    "ctx = KerasTrainingContext(umtm.work_dir, session_index=21)\n",
    "ctx.EVALUATE_ONLY = True\n",
    " \n",
    "    \n",
    "if 'umodel' in globals() and umodel is not None:\n",
    "    print ('*'*80)\n",
    "    print('umodel defined as', umodel.name)\n",
    "else:\n",
    "    # weights = Path(analyser.hyperparams.models_path) / f\"{model_factory_fn.__name__}.h5\"\n",
    "    weights = training_reports_path / f'{model_factory_fn.__name__}.h5'\n",
    "    mlflow.log_param('weights', str(weights))\n",
    "    if weights.is_file():\n",
    "        print (f'LOADING: {model_factory_fn} -- {weights}')\n",
    "    \n",
    "    \n",
    "    umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, weights=weights)\n",
    "    mlflow.log_param('model_name', str(umodel.name))\n",
    "    mlflow.log_param('model_params', umodel.count_params())\n",
    "    \n",
    "    \n",
    "umodel.trainable = False\n",
    "umodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUum89Tdhg-9"
   },
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate single doc (self-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_index = umtm.stats [umtm.stats['value']>0].index[2]\n",
    "if DEBUG:\n",
    "    print(f'{SAMPLE_DOC_ID=}')\n",
    "    prediction = umodel.predict(x=[np.expand_dims(emb, axis=0), np.expand_dims(tok_f, axis=0)], batch_size=1)\n",
    "\n",
    "\n",
    "    tagsmap = pd.DataFrame(prediction[0][0], columns=semantic_map_keys_contract)\n",
    "    tagsmap_e = pd.DataFrame(sm, columns=semantic_map_keys_contract)\n",
    "    delta = tagsmap - tagsmap_e \n",
    "    # .T\n",
    "    plot_embedding(tagsmap[:_crop], f'Predicted Semantic Map {tagsmap.shape}')\n",
    "    plot_embedding(delta[:_crop], title=f'DELTA Semantic map {tagsmap_e.shape}')\n",
    "    plot_embedding(tagsmap_e[:_crop], title=f'EXPECTED Semantic map {tagsmap_e.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(\"mean delta\", delta.abs().sum().sum() / tagsmap_e.sum().sum())\n",
    "    print(\"sum of delatas\", delta.abs().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    av = tagsmap.max(axis=1) #tagsmap['amount-begin'] + tagsmap['vat-begin'] + tagsmap['number-begin'] + tagsmap['org-name-begin']\n",
    "\n",
    "    # av = tagsmap.sum(axis=1)\n",
    "    renderer_.render_color_text(a_doc_from_json.get_tokens_map_unchaged().tokens[:600], av[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting tag values from inferred semantic map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AuditContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    cas = nn_find_org_names(a_doc_from_json.get_tokens_map_unchaged(), tagsmap, ac)\n",
    "    if cas:\n",
    "        if len(cas)>0:\n",
    "\n",
    "            print(cas[0].name)\n",
    "            print(cas[0].type)\n",
    "            print(cas[0].alias)\n",
    "        if len(cas)>1:\n",
    "            print()\n",
    "            print(cas[1].name)\n",
    "            print(cas[1].type)\n",
    "            print(cas[1].alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date/number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    date_tag = nn_get_contract_date     (a_doc_from_json.get_tokens_map_unchaged(), tagsmap) \n",
    "    number_tag = nn_get_contract_number (a_doc_from_json.get_tokens_map_unchaged(), tagsmap)\n",
    "    if date_tag:\n",
    "        print( f'{date_tag.value=}')\n",
    "    if number_tag:\n",
    "        print( f'{number_tag.value=}' )\n",
    "    \n",
    "if DEBUG:\n",
    "    attention = tagsmap['date' + '-begin'].values.copy()\n",
    "\n",
    "    threshold = max(attention.max() * 0.8, 0.1)\n",
    "    print(f'{attention.max()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    textmap = a_doc_from_json.get_tokens_map_unchaged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from analyser.schemas import ContractPrice, merge_spans\n",
    "from analyser.legal_docs import find_value_sign\n",
    "from analyser.transaction_values import ValueSpansFinder\n",
    "from analyser.text_tools import to_float\n",
    "from analyser.contract_parser import nn_find_contract_value\n",
    "if DEBUG:\n",
    "    #---\n",
    "    cps = nn_find_contract_value(textmap, tagsmap)\n",
    "    if cps:\n",
    "      print(str(cps[0].get_span()))\n",
    "      for k in cps[0].list_children():\n",
    "        print(str(k))\n",
    "\n",
    "      print()\n",
    "      print()\n",
    "\n",
    "      print('brutto', str(cps[0].amount_brutto))\n",
    "      print('netto', str(cps[0].amount_netto))\n",
    "      print('amount', str(cps[0].amount))\n",
    "      print('vat', str(cps[0].vat))\n",
    "    else:\n",
    "      print('nothing found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    thresholds = dict(tagsmap.max()*.8)\n",
    "    thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    subject_tag = nn_get_tag_values('subject',  a_doc_from_json.get_tokens_map_unchaged(), tagsmap, max_tokens=200, threshold=0.02, limit=1, return_single=True)\n",
    "\n",
    "    print(subject_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw tags TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if DEBUG:\n",
    "    tags_hl = np.zeros(len(textmap))\n",
    "\n",
    "\n",
    "# def hl(tag):\n",
    "#   try:\n",
    "#     tags_hl [ tag.span[0]:tag.span[1]] +=1\n",
    "#   except:\n",
    "#     pass\n",
    "  \n",
    "\n",
    "# if cps:\n",
    "#   hl(cps[0].amount_brutto)\n",
    "#   hl(cps[0].amount_netto)\n",
    "#   hl(cps[0].amount)\n",
    "#   hl(cps[0].vat)\n",
    "#   hl(cps[0].sign)\n",
    "#   hl(cps[0].currency)\n",
    "#   hl(cps[0])\n",
    "\n",
    "\n",
    "# if cas and len(cas)>0:\n",
    "#     hl(cas[0].name)\n",
    "#     hl(cas[0].type)\n",
    "#     hl(cas[0].alias)\n",
    "\n",
    "    \n",
    "# if cas and len(cas)>1:\n",
    "#     hl(cas[1].name)\n",
    "#     hl(cas[1].type)\n",
    "#     hl(cas[1].alias)\n",
    "\n",
    "# hl(number_tag)\n",
    "# hl(date_tag)\n",
    "\n",
    "\n",
    "# hl(subject_tag)\n",
    "\n",
    "# renderer_.render_color_text(a_doc_from_json.get_tokens_map_unchaged().tokens[:160], tags_hl[:160])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev =   umtm.stats.copy()\n",
    "tags =          pd.DataFrame()\n",
    "errors_report = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdocs = umtm.stats[umtm.stats.unseen==False]\n",
    "userdocs = userdocs[userdocs.source=='db']\n",
    "userdocs = userdocs[userdocs.score < 50000]\n",
    "userdocs = userdocs[userdocs['valid']==True]\n",
    "\n",
    "userdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import t_semantic_map_keys_price, semantic_map_keys, t_semantic_map_keys_common, t_semantic_map_keys_org\n",
    "semantic_keys_numeric = ['amount', 'amount_brutto', 'amount_netto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for v in semantic_map_keys:\n",
    "    for s in ['_expected', '_predicted']:\n",
    "        userdocs[f'{v}{s}'] = ''\n",
    "        userdocs[f'{v}{s}'] = userdocs[f'{v}{s}'].astype(str)\n",
    "        \n",
    "for v in semantic_keys_numeric:\n",
    "    for s in ['_expected', '_predicted']:\n",
    "        userdocs[f'{v}{s}'] = np.NaN\n",
    "        userdocs[f'{v}{s}'] = userdocs[f'{v}{s}'].astype(float)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_keys=['name', 'alias', 'type']\n",
    "org_numbered_keys=[ f'org-1-{v}' for v in org_keys]\n",
    "org_numbered_keys+=[ f'org-2-{v}' for v in org_keys]\n",
    "org_numbered_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analyser.text_tools import to_float, span_len\n",
    "\n",
    "def fix_contract_number(tag: SemanticTag, textmap: TextMap) -> SemanticTag or None:\n",
    "  if tag:\n",
    "    span = [tag.span[0], tag.span[1]]\n",
    "    for i in range(tag.span[0], tag.span[1]):\n",
    "      if i < 0 or i >= len(textmap):\n",
    "        msg = f'{i=} {textmap=} {len(textmap)=} {tag=} {tag.span=}'\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "      t = textmap[i]\n",
    "      t = t.strip().lstrip('№').lstrip().lstrip(':').lstrip('N ').lstrip().rstrip('.')\n",
    "      if t == '':\n",
    "        span[0] = i + 1\n",
    "    tag.span = span\n",
    "  if span_len(tag.span) == 0:\n",
    "    return None\n",
    "\n",
    "  return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "128*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "maxlen = 128*12\n",
    "\n",
    "analyser.contract_parser.fix_contract_number=fix_contract_number\n",
    " \n",
    "def load_doc_as_table_row(_id, df, col_suffix='_expected'):\n",
    "    doc = get_doc_by_id(ObjectId(_id))\n",
    "    if doc is None:\n",
    "        print (f'{_id} not found in db')\n",
    "        return\n",
    "    \n",
    "    jd = DbJsonDoc(doc)\n",
    "#     print(jd.get_attribute_value('number'))\n",
    "#     df.at[ _id, f'analyze_timestamp{col_suffix}']=jd.analysis['analyze_timestamp']\n",
    "    \n",
    "    tree = jd.get_attributes_tree()\n",
    "    df.at[ _id, f'number{col_suffix}'] = jd.get_attribute_value('number')\n",
    "    df.at[ _id, f'date{col_suffix}']   = jd.get_attribute_value('date')\n",
    "    df.at[ _id, f'subject{col_suffix}']= jd.get_attribute_value('subject')\n",
    "    \n",
    "    orgs = tree.get('orgs', [])\n",
    "#     for i, o in enumerate(orgs):\n",
    "    if len(orgs)>0:\n",
    "        o1=orgs[0]\n",
    "    else:\n",
    "        o1={}\n",
    "        \n",
    "    if len(orgs)>1:\n",
    "        o2=orgs[1]\n",
    "    else:\n",
    "        o2={}\n",
    "        \n",
    "    for part in org_keys:\n",
    "        v1 = o1.get(part, {}).get('value', \"\").lower()\n",
    "        v2 = o2.get(part, {}).get('value', \"\").lower()\n",
    "        vv=sorted([v1,v2])\n",
    "        df.at[ _id, f'org-1-{part}{col_suffix}'] = vv[1]\n",
    "        df.at[ _id, f'org-2-{part}{col_suffix}'] = vv[0]\n",
    "\n",
    "    \n",
    "    for v in t_semantic_map_keys_price:\n",
    "        df.at[ _id, f'{v}{col_suffix}'] =  tree.get('price', {}).get(v, {}).get('value')\n",
    "\n",
    "    df.at[ _id, f'analysis.version{col_suffix}'] = jd.analysis['version']\n",
    "    return jd\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def interpret_prediction(_id, tagsmap, df):\n",
    "    col_suffix=\"_predicted\"\n",
    "    \n",
    "    doc = load_doc_as_table_row(_id, df)\n",
    "    if doc is None: \n",
    "        return\n",
    "    \n",
    "    tokens = doc.get_tokens_map_unchaged()\n",
    "    \n",
    "    #---\n",
    "    # ORGS ------------------\n",
    "    orgs = nn_find_org_names(tokens, tagsmap, ac)\n",
    "    if len(orgs)>0:\n",
    "        o1 = orgs[0]\n",
    "    else:\n",
    "        o1={}\n",
    "        \n",
    "    if len(orgs)>1:\n",
    "        o2 = orgs[1]\n",
    "    else:\n",
    "        o2={}\n",
    "        \n",
    "    for part in ['name', 'alias', 'type']:\n",
    "        part_tag1 = getattr(o1, part, {})\n",
    "        part_tag2 = getattr(o2, part, {})\n",
    "        \n",
    "        v1 = getattr(part_tag1, 'value', '').lower()\n",
    "        v2 = getattr(part_tag2, 'value', '').lower()\n",
    "#         vv= [v1,v2]\n",
    "        vv=sorted([v1,v2])\n",
    "        df.at[ _id, f'org-1-{part}{col_suffix}'] = vv[1] \n",
    "        df.at[ _id, f'org-2-{part}{col_suffix}'] = vv[0] \n",
    " \n",
    "    # PRICE ------------------\n",
    "    cps = nn_find_contract_value(tokens, tagsmap)\n",
    "    if cps:\n",
    "        cps=cps[0]\n",
    "        for v in t_semantic_map_keys_price:\n",
    "            part = getattr(cps, v)\n",
    "            if part:\n",
    "                df.at[ _id, f'{v}{col_suffix}'] = part.value\n",
    "                 \n",
    " \n",
    "    # DATE NUMBER ------------------\n",
    "    number_tag = nn_get_contract_number(tokens, tagsmap)\n",
    "    date_tag =   nn_get_contract_date  (tokens, tagsmap) \n",
    "    \n",
    "        \n",
    "    if number_tag:\n",
    "        df.at[ _id, f'number{col_suffix}'] = str(number_tag.value)\n",
    "    else:\n",
    "        df.at[ _id, f'number{col_suffix}'] = ''\n",
    "        \n",
    "        \n",
    "    if date_tag:\n",
    "        df.at[ _id, f'date{col_suffix}'] = str(date_tag.value)\n",
    "    else:\n",
    "        df.at[ _id, f'date{col_suffix}'] = ''\n",
    " \n",
    "\n",
    "validation_set = userdocs\n",
    "if TEST_FLOW:\n",
    "    validation_set = userdocs[0:20]\n",
    "\n",
    "    \n",
    "    \n",
    "for i in range(0, len(validation_set), batch_size):\n",
    "    batch = userdocs[i:i+batch_size]\n",
    "    actual_batch_size=len(batch)\n",
    "    print(f'{actual_batch_size=}')\n",
    "    batch_input_emb=[]\n",
    "    batch_input_token_f=[]\n",
    "    for _id in batch.index.values:\n",
    " \n",
    "        dp = make_xyw(_id, userdocs)\n",
    "        dp = umtm.trim_maxlen( dp, 0, maxlen  )\n",
    "        (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "        batch_input_emb.append(emb)\n",
    "        batch_input_token_f.append(tok_f)\n",
    "        \n",
    "        expected_subj =  decode_subj_prediction(subj)[0]\n",
    "        userdocs.at[_id, 'subject_expected'] = expected_subj.name\n",
    " \n",
    "        # weights_subj.append(subject_weight)\n",
    "\n",
    "    del _id\n",
    "    del dp\n",
    "\n",
    "    \n",
    "    \n",
    "    dps =  [np.array(batch_input_emb), np.array(batch_input_token_f)]\n",
    " \n",
    "    \n",
    "    predictions = umodel.predict(x=dps, batch_size=actual_batch_size)\n",
    "    del batch_input_emb\n",
    "    del batch_input_token_f\n",
    "#     print ('len(predictions)', len(predictions))\n",
    "    \n",
    "    p_tags = predictions[0]\n",
    "    p_subj = predictions[1]\n",
    "    for k,_id in enumerate(batch.index.values):\n",
    "#     for k in range(0, len(p_tags)):\n",
    "#         print (k, p_tags.shape, p_subj.shape)\n",
    "                   \n",
    "        tagsmap = pd.DataFrame(p_tags[k], columns=semantic_map_keys_contract)\n",
    "        interpret_prediction(_id, tagsmap, userdocs)\n",
    "        \n",
    "        subj_1hot = p_subj[k]\n",
    "\n",
    "#         expected =  decode_subj_prediction(subj[k])\n",
    "        predicted = decode_subj_prediction(subj_1hot)[0]\n",
    "        userdocs.at[_id, 'subject_predicted'] = predicted.name\n",
    "        \n",
    "        \n",
    "#         print(predicted)\n",
    "            \n",
    "#         print(_id, trim_dp(x[0]).shape)\n",
    "\n",
    "# userdocs[['org-1-name_expected','subject_expected','subject_predicted','date_expected','date_predicted', 'number_expected','number_predicted']].head(30)\n",
    "userdocs[\n",
    "  ['org-1-type_expected', 'org-1-type_predicted', 'org-1-name_expected', 'org-1-name_predicted', 'org-1-alias_expected', 'org-1-alias_predicted', \n",
    "   'org-2-type_expected', 'org-2-type_predicted', 'org-2-name_expected', 'org-2-name_predicted', 'org-2-alias_expected', 'org-2-alias_predicted', \n",
    "   'subject_expected', 'subject_predicted', 'date_expected',\n",
    "   'date_predicted', 'number_expected', 'number_predicted']].head(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean predicted/expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# userdocs['amount_expected']=userdocs['amount_expected'].replace('',np.NaN).astype(float)\n",
    "# userdocs['amount_predicted']=userdocs['amount_predicted'].replace('',np.NaN).astype(float)\n",
    "\n",
    "# userdocs['amount_brutto_predicted'].replace('',np.NaN).astype(float)\n",
    "# userdocs['amount_brutto_expected'].replace('',np.NaN).astype(float)\n",
    "\n",
    "columns = []\n",
    "for v in semantic_keys_numeric:\n",
    "    for s in ['_expected', '_predicted' ]:\n",
    "        col = f'{v}{s}'\n",
    "        userdocs[col] = userdocs[col].replace('',np.NaN).replace(np.NaN, -1).astype(int)\n",
    "        columns.append(col)\n",
    "        \n",
    "userdocs['date_predicted'] = userdocs['date_predicted'].astype(str)        \n",
    "userdocs['date_expected'] = userdocs['date_expected'].astype(str)        \n",
    "userdocs[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdocs=userdocs.replace('None', '')\n",
    "userdocs=userdocs.replace('none', '')\n",
    "userdocs=userdocs.replace('nan', '')\n",
    "\n",
    "userdocs.date_expected =  userdocs.date_expected.replace(np.NaN, '')\n",
    "userdocs.date_predicted = userdocs.date_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.number_expected =  userdocs.number_expected.replace(np.NaN, '')\n",
    "userdocs.number_predicted = userdocs.number_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.vat_unit_expected =  userdocs.vat_unit_expected.replace(np.NaN, '')\n",
    "userdocs.vat_unit_predicted = userdocs.vat_unit_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.vat_expected =  userdocs.vat_expected.replace(np.NaN, '')\n",
    "userdocs.vat_predicted = userdocs.vat_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.sign_expected =  userdocs.sign_expected.replace(np.NaN, '')\n",
    "userdocs.sign_predicted = userdocs.sign_predicted.replace(np.NaN, '')\n",
    "\n",
    "userdocs.currency_expected =  userdocs.currency_expected.replace(np.NaN, '')\n",
    "userdocs.currency_predicted = userdocs.currency_predicted.replace(np.NaN, '')\n",
    "userdocs[['date_expected', 'date_predicted', 'number_expected', 'number_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_wrong(userdocs, key):\n",
    "    s= userdocs[userdocs[f'{key}_expected'] != userdocs[f'{key}_predicted']][[f'{key}_expected', f'{key}_predicted']]\n",
    "    p=float(len(s)) / len(userdocs)\n",
    "#     print(len(s), p)\n",
    "    return s,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags report\n",
    "# accuracy report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# from mlflow.models import Model\n",
    "# model1 = mlflow.tensorflow.load_model(\"file:///root/artem/analyser/mlruns/0/c9389e6d6a87415c9488079fd46c09d2/artifacts/model\")\n",
    "# model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "userdocs['number_of_errors'] = 0\n",
    "report = pd.DataFrame()\n",
    "\n",
    "n   = 0\n",
    "avg = 0\n",
    "\n",
    "for k in t_semantic_map_keys_common[1:] + t_semantic_map_keys_price + org_numbered_keys :\n",
    "    s, p = select_wrong(userdocs, k)\n",
    "    for _id, _ in s.iterrows():\n",
    "       userdocs.at[_id, 'number_of_errors'] += 1\n",
    "    \n",
    "    acc = 1.0 - p\n",
    "    report.at[k, 'accuracy']=f\"{acc:.1%}\"\n",
    "    mlflow.log_metric(f\"accuracy_{k}\", acc)\n",
    "    \n",
    "    n += 1\n",
    "    avg += acc\n",
    "    \n",
    "avg = avg / n\n",
    "\n",
    "report.at['ALL', 'accuracy']=f\"{avg:.1%}\"\n",
    "mlflow.log_metric(\"accuracy_TAGS\", avg)\n",
    "report.to_csv(umtm.reports_dir /'attributes_accuracy.csv')    \n",
    "mlflow.log_artifact(umtm.reports_dir / 'attributes_accuracy.csv')\n",
    "\n",
    "# 81.6%\n",
    "report    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worst docs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdocs[['number_of_errors']].sort_values('number_of_errors', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,p = select_wrong(userdocs, \"date\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s,p = select_wrong(userdocs, \"currency\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing subject report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t=userdocs[userdocs.subject_predicted != userdocs.subject_expected]\n",
    "\n",
    "for i, row in _t.iterrows():\n",
    "    _t.at[i,'link'] = f'https://gpn-audit.nemosoft.ru/#/pre-audit/edit/{i}'\n",
    "# _t\n",
    "\n",
    "_report = _t[['link','analyze_date']].copy()\n",
    "_report['Предмет, выявленный Шайтан-Арбой']=_t.subject_predicted\n",
    "_report['Предмет, выявленный роевым био-интеллектом']=_t.subject_expected\n",
    "_report.to_csv(umtm.reports_dir /'subjects_to_check.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = userdocs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh1WFsjprUr1"
   },
   "outputs": [],
   "source": [
    "# _cols = [  'wrong' ]\n",
    "# _tmp = ev[cols]\n",
    "# errors_report = _tmp[ _tmp.wrong == True] #.sort_values('subject')\n",
    "# print(len(errors_report), 'wrong subjects of', len(tags))\n",
    "# errors_report \n",
    "\n",
    "subj_pred = ev[pd.notna(ev.subject_predicted)][pd.notna(ev.subject_expected)]\n",
    "subj_df = subj_pred[['subject_predicted', 'subject_expected']].copy()\n",
    "subj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# print(subj_df['predicted_subj'].values)\n",
    "labels = sorted(np.unique(subj_df['subject_expected'].values))\n",
    "print (labels)\n",
    "\n",
    "# cm = confusion_matrix(subj_df['expected_subj'].values, subj_df['predicted_subj'].values, labels=labels)\n",
    "# cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(subj_df.subject_expected, subj_df.subject_predicted, digits=3, output_dict=True)\n",
    "report['weighted avg']\n",
    "mlflow.log_metrics(report['weighted avg'])\n",
    "mlflow.log_metric('subject_F1',report['weighted avg']['f1-score'])\n",
    "mlflow.log_metric('subject_F1_support',report['weighted avg']['support'])\n",
    "mlflow.log_metric('subject_precision',report['weighted avg']['precision'])\n",
    "mlflow.log_metric('subject_recall',report['weighted avg']['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFh8SxP9J081"
   },
   "outputs": [],
   "source": [
    "def make_report(umodel, subj_df):\n",
    "  plot_cm(subj_df.subject_expected.values, subj_df.subject_predicted.values, figsize=(12, 12))\n",
    "  \n",
    "  img_path = umtm.reports_dir / f'subjects-confusion-matrix-{umodel.name}.png'\n",
    "  plt.savefig(img_path, bbox_inches='tight')\n",
    "  mlflow.log_artifact(img_path)\n",
    "\n",
    "  report = classification_report(subj_df.subject_expected, subj_df.subject_predicted, digits=3)\n",
    "  print(umodel.name)\n",
    "  print(report)\n",
    "\n",
    "  fn = umtm.reports_dir / f'subjects-classification_report-{umodel.name}.txt'\n",
    "  with open(fn, \"w\") as text_file:\n",
    "    text_file.write(report)\n",
    " \n",
    "  mlflow.log_artifact(fn)\n",
    "\n",
    "\n",
    "# subj_df = subj_df[['predicted_subj', 'expected_subj']].copy() #ev[~pd.isna(ev['predicted_subj'])]\n",
    "make_report(umodel, subj_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sub_mlflow_run is not None:\n",
    "    print(active_mlflow_run.info)\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
