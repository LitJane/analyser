{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19651a0e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg",
    "papermill": {
     "duration": 1.149707,
     "end_time": "2023-03-03T14:15:50.604451",
     "exception": false,
     "start_time": "2023-03-03T14:15:49.454744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    " \n",
    "\n",
    "logger = logging.getLogger('retrain_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.debug('--=logging started=--')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c78626",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBqnGnQfKWF",
    "papermill": {
     "duration": 0.01392,
     "end_time": "2023-03-03T14:15:50.626530",
     "exception": false,
     "start_time": "2023-03-03T14:15:50.612610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "import analyser.hyperparams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38034de",
   "metadata": {
    "papermill": {
     "duration": 0.007774,
     "end_time": "2023-03-03T14:15:50.642304",
     "exception": false,
     "start_time": "2023-03-03T14:15:50.634530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b9059",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "papermill": {
     "duration": 1.866896,
     "end_time": "2023-03-03T14:15:52.517142",
     "exception": false,
     "start_time": "2023-03-03T14:15:50.650246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from bson import ObjectId\n",
    "\n",
    "import gpn_config\n",
    "import mlflow\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tf_support.super_contract_model import get_semantic_map_new, \\\n",
    "        semantic_map_keys_contract, t_semantic_map_keys_common, t_semantic_map_keys_org, t_semantic_map_keys_price\n",
    "\n",
    "from analyser.persistence import DbJsonDoc\n",
    "\n",
    "    \n",
    "\n",
    "if 'COLLECTION_NAME' in globals():\n",
    "    COLLECTION_NAME = globals()['COLLECTION_NAME']\n",
    "else:\n",
    "    COLLECTION_NAME = 'documents'\n",
    "print(f'documents {COLLECTION_NAME=}')\n",
    "    \n",
    "\n",
    "def get_doc_by_id (objid):\n",
    "    db = get_mongodb_connection()\n",
    "    documents_collection = db[COLLECTION_NAME]\n",
    "    _id = ObjectId(str(objid))\n",
    "    jdata = documents_collection.find_one({'_id': _id})\n",
    "    return jdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844dff1-4cbe-4589-b382-01ea20f1dfeb",
   "metadata": {},
   "source": [
    "# Init ml flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc21c2e",
   "metadata": {
    "papermill": {
     "duration": 0.135732,
     "end_time": "2023-03-03T14:15:52.661228",
     "exception": false,
     "start_time": "2023-03-03T14:15:52.525496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# mlflow.start_run(run_name='fetch trainset from db')\n",
    "ml_flow_url = gpn_config.configured('MLFLOW_URL')\n",
    "mlflow.set_tracking_uri(ml_flow_url)\n",
    "print(f'{ml_flow_url=}', 'set MLFLOW_URL env var to re-define')\n",
    "\n",
    "mlflow.set_experiment(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏\")\n",
    "active_mlflow_run = mlflow.start_run(nested=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb664f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "papermill": {
     "duration": 0.014162,
     "end_time": "2023-03-03T14:15:52.699596",
     "exception": false,
     "start_time": "2023-03-03T14:15:52.685434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "reports_path = analyser.hyperparams.reports_path\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d4ae3",
   "metadata": {
    "papermill": {
     "duration": 0.016429,
     "end_time": "2023-03-03T14:16:30.264833",
     "exception": false,
     "start_time": "2023-03-03T14:16:30.248404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ü§¶ Find markup errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dadde0-e714-40b0-8822-f893de91d424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# del user_docs_ids\n",
    "\n",
    "if 'errors_report_file_prefix' in globals():\n",
    "    errors_report_file_prefix = globals()['errors_report_file_prefix']\n",
    "else:\n",
    "    errors_report_file_prefix = \"user_markup_errors\"\n",
    "\n",
    "    \n",
    "reports_fn = reports_path / f\"{errors_report_file_prefix}.csv\"    \n",
    "\n",
    "\n",
    "    \n",
    "if 'user_docs_ids'in globals():\n",
    "    user_docs_ids = globals()['user_docs_ids']\n",
    "    print('Number of contract IDs for validation (set ouside) is', len(user_docs_ids))\n",
    "else:    \n",
    "    print('Query DB for all user docs')\n",
    "    \n",
    "    from integration.db import get_mongodb_connection\n",
    "    from pymongo import ASCENDING\n",
    "\n",
    "    query = {\n",
    "      '$and': [\n",
    "        {\"parse.documentType\":{ '$in': [\"AGREEMENT\", \"CONTRACT\", \"SUPPLEMENTARY_AGREEMENT\"] }  },      \n",
    "        {\"user.attributes_tree\": {\"$ne\": None}},\n",
    "    #       {'user.updateDate': {'$gt': lastdate}}    \n",
    "      ]\n",
    "    }\n",
    "\n",
    "    db = get_mongodb_connection()\n",
    "    documents_collection = db['documents']\n",
    "    sorting = [('analysis.analyze_timestamp', ASCENDING), ('user.updateDate', ASCENDING)]\n",
    "    res = documents_collection.find(filter=query, \n",
    "                                    sort=sorting,\n",
    "                                    projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "    #                                             'analysis.attributes_tree.version': True,\n",
    "    #                                             'analysis.attributes_tree.contract.subject': True}\n",
    "                                   ).limit(5000)\n",
    "\n",
    "    res = list([i for i in res])\n",
    "    user_docs_ids  = [i[\"_id\"] for i in res]\n",
    "    display(Markdown(f\"#### {len(res)} -- –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–º–µ—á–µ–Ω—ã —á–µ–ª–æ–≤–µ–∫–æ–º\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eea6de",
   "metadata": {
    "papermill": {
     "duration": 0.021219,
     "end_time": "2023-03-03T14:16:30.548722",
     "exception": false,
     "start_time": "2023-03-03T14:16:30.527503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_span_val(jd:DbJsonDoc, tag):\n",
    "    tm = jd.get_tokens_map_unchaged()\n",
    "    \n",
    "    span = tag.get('span', [0,0]) \n",
    "    \n",
    "    if span[1]-span[0]==0:\n",
    "        return None\n",
    "    \n",
    "    quote = tm.text_range(span)\n",
    "    \n",
    "    return quote\n",
    "\n",
    "\n",
    "\n",
    "def add_error(userdocs, _id, error_message_tuple ):    \n",
    "            \n",
    "    _errors = []\n",
    "    \n",
    "    if type(error_message_tuple) == list:\n",
    "        _errors = error_message_tuple\n",
    "    else:\n",
    "        if error_message_tuple:\n",
    "            _errors.append(error_message_tuple)\n",
    "            \n",
    "        \n",
    "    for error_message_tuple in _errors:\n",
    "        try:\n",
    "            if len(error_message_tuple) == 3:\n",
    "                error_message, missing, severity = error_message_tuple\n",
    "            else:\n",
    "                error_message =str(error_message_tuple)\n",
    "                missing = False\n",
    "                severity = 1\n",
    "\n",
    "            userdocs.at[_id,'errors count'] = userdocs.at[_id,'errors count'] + 1\n",
    "            userdocs.at[_id,'errors severity'] = userdocs.at[_id,'errors severity'] + severity\n",
    "\n",
    "            if missing==True:\n",
    "                userdocs.at[_id, 'error missing']=';\\n'.join(  [error_message, userdocs.at[_id,'error missing']] )\n",
    "            else:\n",
    "                userdocs.at[_id, 'error']=';\\n'.join(  [error_message, userdocs.at[_id,'error']] )\n",
    "        except:\n",
    "            print(error_message_tuple)\n",
    "            raise(Exception(_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc827fe",
   "metadata": {
    "papermill": {
     "duration": 0.0339,
     "end_time": "2023-03-03T14:16:30.632126",
     "exception": false,
     "start_time": "2023-03-03T14:16:30.598226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def validate_date(jd:DbJsonDoc, df, i=0) -> str or None:\n",
    "    _id = str(jd._id)\n",
    "    tag = jd.get_attributes_tree().get('date', {})\n",
    "    val = get_span_val(jd, tag)\n",
    "    \n",
    "    if not val:\n",
    "        return f\"–î–∞—Ç–∞ –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "\n",
    "    \n",
    "    df.at[_id, '–î–∞—Ç–∞'] = val \n",
    "    df.at[_id, 'Date val.'] = tag.get('value', None)\n",
    "       \n",
    "    \n",
    "    if '\\n' in val:\n",
    "        return f\"–î–∞—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\", False, 2\n",
    "    \n",
    "    if len(val) > 25:\n",
    "        return f\"–î–∞—Ç–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è\", False, 4\n",
    "    \n",
    "    \n",
    "def validate_number(jd:DbJsonDoc, df, i=0) -> str or None:\n",
    "    _id = str(jd._id)\n",
    "#     span = jd.get_attributes_tree().get('number', {}).get('span', [0,0])\n",
    "#     val = jd.get_tokens_map_unchaged().text_range(span)\n",
    "    \n",
    "    tag = jd.get_attributes_tree().get('number', {})\n",
    "    span = tag.get('span', [0,0]) \n",
    "    val = get_span_val(jd, tag)\n",
    "    \n",
    "    if not val:\n",
    "        return f\"–ù–æ–º–µ—Ä –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    \n",
    "    df.at[_id, f'–ù–æ–º–µ—Ä'] = val \n",
    "    \n",
    "    if '–¥–æ–≥–æ–≤–æ—Ä' in val.lower():\n",
    "        return f\"–ù–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–æ\", False, 3\n",
    "    \n",
    "    if '\\n' in val:\n",
    "        return f\"–ù–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\", False, 2\n",
    "       \n",
    "    if val.strip() !=val:\n",
    "        return f\"–ù–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º\", False, 2\n",
    "    \n",
    "    if val.strip()==\"‚Ññ\":\n",
    "        return f\"–ù–æ–º–µ—Ä –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ–º–µ—Ä–æ–º\", False, 3\n",
    "    \n",
    "    if (span[1]-span[0])>5:\n",
    "        return f\"–ù–æ–º–µ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞ –∫–∞–∫–æ–π-—Ç–æ –¥–ª–∏–Ω–Ω—ã–π\", False, 2\n",
    "\n",
    "#     if val.strip()[0]==\"‚Ññ\":\n",
    "#         return f\"–ù–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞–∫ ‚Ññ\"\n",
    "        \n",
    "#     if len(val) > 30:\n",
    "#         return f\"–ù–æ–º–µ—Ä —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π\"\n",
    "    \n",
    "    \n",
    "def validate_alias(jd:DbJsonDoc, df, i=0) -> str or None:\n",
    "    _id = str(jd._id)\n",
    "    orgs=jd.get_attributes_tree().get('orgs', [{},{}])\n",
    "    \n",
    "    if len(orgs) < i+1:\n",
    "        return f\"–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    tag =  orgs[i].get('alias',{})\n",
    "    val = get_span_val(jd, tag)\n",
    "    \n",
    "    if not val:\n",
    "        return f\"–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    \n",
    "    df.at[_id,f'–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1}'] = val \n",
    "    \n",
    "    if  '¬´' in val or '¬ª' in val:\n",
    "        return f\"–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1} —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞–≤—ã—á–∫–∏\", False, 2\n",
    "    \n",
    "    if '\\n' in val:\n",
    "        return f\"–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\", False, 2\n",
    "    \n",
    "    if len(val) > 25:\n",
    "        return f\"–ü—Å–µ–≤–¥–æ–Ω–∏–º {i+1} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π\", False, 5\n",
    "    \n",
    "    \n",
    "def validate_org_name(jd:DbJsonDoc, df, i=0) -> str or None:\n",
    "    _id = str(jd._id)\n",
    "    _prefix = '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'\n",
    "    errors = list()\n",
    "    \n",
    "    orgs = jd.get_attributes_tree().get('orgs', [{},{}])\n",
    "    \n",
    "    if len(orgs) < i+1:\n",
    "        return f\"{_prefix} {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True,1\n",
    "    \n",
    "    tag = orgs[i].get('name',{})\n",
    "    val = get_span_val(jd, tag)\n",
    "    \n",
    "    if not val:\n",
    "        return f\"{_prefix} {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    df.at[_id,f'{_prefix} {i+1}'] = val \n",
    "    \n",
    "    if '\\n' in val:\n",
    "        errors.append(( f\"{_prefix} {i+1} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\", False, 2))\n",
    "    \n",
    "    if '¬´' in val:\n",
    "        if '¬ª' not in val:\n",
    "            errors.append(( f\"{_prefix} {i+1}: –∫–∞–≤—ã—á–∫–∞ –Ω–µ –∑–∞–∫—Ä—ã—Ç–∞\", False, 3))\n",
    "    \n",
    "    if '¬ª' in val:\n",
    "        if '¬´' not in val:\n",
    "            errors.append(( f\"{_prefix} {i+1}: –∫–∞–≤—ã—á–∫–∞ –Ω–µ –æ—Ç–∫—Ä—ã—Ç–∞\", False, 3))\n",
    "        \n",
    "    if val[0]=='¬´':\n",
    "        errors.append(( f\"{_prefix} {i+1} —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞–≤—ã—á–∫–∏\", False, 2))\n",
    "\n",
    "    return errors\n",
    "    \n",
    "    \n",
    "def validate_org_type(jd:DbJsonDoc, df, i=0) -> str or None:\n",
    "    _id = str(jd._id)\n",
    "    \n",
    "    _prefix = '–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏'\n",
    "    errors = list()\n",
    "        \n",
    "        \n",
    "    orgs=jd.get_attributes_tree().get('orgs', [{},{}])\n",
    "    \n",
    "   \n",
    "    \n",
    "    if len(orgs) < i+1:\n",
    "        return f\"{_prefix} {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    tag = orgs[i].get('type',{})\n",
    "    if not tag:\n",
    "        return f\"{_prefix} {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    \n",
    "    val = tag['value'] #  get_span_val(jd, tag)\n",
    "    \n",
    "    if not val:\n",
    "        errors.append(( f\"{_prefix} {i+1} –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 100))\n",
    "    \n",
    "    df.at[_id, f'{_prefix} {i+1}'] = val \n",
    "    \n",
    "    if '\\n' in val:\n",
    "        errors.append((f\"{_prefix} {i+1}: —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\", False, 2))\n",
    "    \n",
    "    if '¬´' in val:\n",
    "        if '¬ª' not in val:\n",
    "            errors.append(( f\"{_prefix} {i+1}: –∫–∞–≤—ã—á–∫–∞ –Ω–µ –∑–∞–∫—Ä—ã—Ç–∞\", False, 3))\n",
    "    \n",
    "    if '¬ª' in val:\n",
    "        if '¬´' not in val:\n",
    "            errors.append(( f\"{_prefix} {i+1}: –∫–∞–≤—ã—á–∫–∞ –Ω–µ –æ—Ç–∫—Ä—ã—Ç–∞\", False, 3))\n",
    "        \n",
    "    if val[0]=='¬´':\n",
    "        errors.append(( f\"{_prefix} {i+1}: —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞–≤—ã—á–∫–∏\", False, 3))\n",
    "    \n",
    "    return errors\n",
    "    \n",
    "\n",
    "def validate_subject_len(jd:DbJsonDoc, df) -> str or None: \n",
    "    _id = str(jd._id)\n",
    "    \n",
    "    tag = jd.get_attributes_tree().get('subject', {})\n",
    "    val = get_span_val(jd, tag)\n",
    "    if not val:\n",
    "        return f\"–ø—Ä–µ–¥–º–µ—Ç –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 10\n",
    "    \n",
    "    \n",
    "    span = tag.get('span', [0,0]) \n",
    "    \n",
    "    kind = tag.get('value', None) \n",
    "    \n",
    "    \n",
    "    subject_len = span[1]-span[0]\n",
    "    \n",
    "    df.at[_id,'subject len'] = subject_len\n",
    "    df.at[_id,'subject kind'] = kind\n",
    "    \n",
    "    if subject_len > 150:\n",
    "        df.at[_id,'subject'] = val[:200]\n",
    "        return f\"–ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {subject_len} —Å–ª–æ–≤\", False, 5\n",
    "    \n",
    "    if subject_len == 0:\n",
    "        return f\"—Ä–∞–∑–¥–µ–ª –æ –ø—Ä–µ–¥–º–µ—Ç–µ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–µ —É–∫–∞–∑–∞–Ω; –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∫–∞–∫ [{kind}]\", False, 10\n",
    "    \n",
    "    \n",
    "def validate_price(jd:DbJsonDoc, df):    \n",
    "    errors = list()\n",
    "    \n",
    "    _id = str(jd._id)\n",
    "    \n",
    "    tag = jd.get_attributes_tree().get('price', {})\n",
    "    \n",
    "    \n",
    "    span = tag.get('span', [0,0]) \n",
    "        \n",
    "    _len = span[1]-span[0]\n",
    "    \n",
    "#     df.at[_id,'subject len'] = subject_len\n",
    "\n",
    "    \n",
    "    \n",
    "    if not tag:\n",
    "        return f\"—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ –æ—Ç—Å—É—Ç—Å–≤—É–µ—Ç\", True, 1\n",
    "    \n",
    "    price_q = get_span_val(jd, tag)\n",
    "    df.at[_id,'—Å—É–º–º–∞'] = f\"{price_q}\"\n",
    "    \n",
    "    \n",
    "    sentence_span1 = jd.get_tokens_map_unchaged().sentence_at_index( span[0])\n",
    "    sentence_span2 = jd.get_tokens_map_unchaged().sentence_at_index( span[1])\n",
    "    sentence_span = [ sentence_span1[0], sentence_span2[1]]\n",
    "    \n",
    "    \n",
    "    sentence = jd.get_tokens_map_unchaged().text_range(sentence_span)\n",
    "    \n",
    "    amount_name = '—Å—É–º–º–∞ (—Å—Ç–∞—Ä–∞—è)'\n",
    "    price_name = '—Å—É–º–º–∞ –¥–æ–≥–æ–≤–æ—Ä–∞'\n",
    "    \n",
    "    \n",
    "    vat = tag.get('vat')    \n",
    "    if vat:\n",
    "        val = get_span_val(jd, vat)    \n",
    "        df.at[_id, '–Ω–∞–ª–æ–≥'] = f'{val}'\n",
    "        \n",
    "        \n",
    "    vat_unit = tag.get('vat_unit')    \n",
    "    if vat_unit:\n",
    "        val = get_span_val(jd, vat_unit)    \n",
    "        df.at[_id,'vat_unit'] = f'{val}'\n",
    "        \n",
    "\n",
    "    amount_netto = tag.get('amount_netto')    \n",
    "    if amount_netto:\n",
    "        val = get_span_val(jd, amount_netto)    \n",
    "        df.at[_id,'—Å—É–º–º–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞'] = f'{val}'\n",
    "        \n",
    "        \n",
    "    amount = tag.get('amount')  \n",
    "    if amount:\n",
    "        val = get_span_val(jd, amount)    \n",
    "        df.at[_id, amount_name] = f'{val}'\n",
    "     \n",
    "        \n",
    "    amount_brutto = tag.get('amount_brutto')    \n",
    "    if amount_brutto:\n",
    "        val = get_span_val(jd, amount_brutto)    \n",
    "        df.at[_id,'—Å—É–º–º–∞ —Å –Ω–∞–ª–æ–≥–æ–º'] = f'{val}'\n",
    "        \n",
    "        \n",
    "    currency = tag.get('currency')    \n",
    "    if currency:\n",
    "        val = get_span_val(jd, currency)    \n",
    "        df.at[_id,'currency'] = f'{val}'\n",
    "        \n",
    "    ## --- validation:\n",
    "    \n",
    "    if vat:\n",
    "        val = get_span_val(jd, vat)    \n",
    "        if not val.lstrip('-')[0].isdigit():\n",
    "            errors.append((f\"–Ω–∞–ª–æ–≥ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–µ —Å —Ü–∏—Ñ—Ä—ã\", False, 100))\n",
    "     \n",
    "    #----\n",
    "    if amount_netto:\n",
    "        _span = amount_netto.get('span', [0,0])         \n",
    "        if _span[1]-_span[0] > 4:\n",
    "            errors.append((f\"—Å—É–º–º–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ\", False, 10))\n",
    "    \n",
    "    #----\n",
    "    if amount_brutto:\n",
    "        _span = amount_brutto.get('span', [0,0])         \n",
    "        if _span[1]-_span[0] > 4:\n",
    "            errors.append((f\"—Å—É–º–º–∞ c –Ω–∞–ª–æ–≥–æ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ\", False, 10))\n",
    "        \n",
    "    #----\n",
    "    if amount:\n",
    "        _span = amount.get('span', [0,0])         \n",
    "        if _span[1]-_span[0] > 4:\n",
    "            errors.append((f\"{amount_name} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ\", False, 10))\n",
    "    \n",
    "    \n",
    "    if amount_brutto:\n",
    "        if not vat:\n",
    "            errors.append((f\"–Ω–∞–ª–æ–≥ –Ω–µ —É–∫–∞–∑–∞–Ω\", False, 20))\n",
    "    \n",
    "\n",
    "    if '—à—Ç—Ä–∞—Ñ' in sentence.lower() or \"—Å—Ç—Ä–∞—Ö–æ–≤–∞—è —Å—É–º–º–∞\" in sentence.lower() or \" –ø–µ–Ω–∏ \" in sentence.lower() : \n",
    "        print('-'*80)\n",
    "        print(_id, '—Å—É–º–º–∞ –Ω–µ —Ç–∞!', 'sentence_span', sentence_span, jd.get_tokens_map_unchaged().text_range(sentence_span)[:200])\n",
    "        errors.append((f\"—Å—É–º–º–∞ –≤–æ–æ–±—â–µ –Ω–µ —Ç–∞!!\", False, 100))\n",
    "   \n",
    "    if '–Ω–¥—Å –Ω–µ –æ–±–ª–∞–≥–∞–µ—Ç—Å—è' in sentence.lower() or '–Ω–¥—Å –Ω–µ —É–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è' in sentence.lower():\n",
    "        pass\n",
    "    else:\n",
    "        if '–Ω–¥—Å' in sentence.lower():\n",
    "            if not (amount_netto or  amount_brutto):\n",
    "                errors.append((f\"—Å—É–º–º–∞ —Å —É—á–µ—Ç–æ–º –Ω–∞–ª–æ–≥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞\", False, 1))\n",
    "        \n",
    "    if amount_netto or amount_brutto or amount:\n",
    "        if not currency:\n",
    "            errors.append((f\"–≤–∞–ª—é—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞\", False, 30))\n",
    "        \n",
    "    if vat:\n",
    "        if not vat_unit:\n",
    "            errors.append((f\"–≤–∞–ª—é—Ç–∞ –Ω–∞–ª–æ–≥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞\", False, 40))\n",
    "    \n",
    "    if tag:\n",
    "        if not (price_q.strip()[0].isalpha() or price_q.strip()[0].isdigit):\n",
    "            errors.append((f\"—Ä–∞–∑–¥–µ–ª –æ —Ü–µ–Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–µ —Å–æ —Å–ª–æ–≤–∞\", False, 4))\n",
    "        \n",
    "    if _len > 150:\n",
    "        errors.append((f\"{price_name} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è: {_len} —Å–ª–æ–≤\", False, 10))\n",
    "    \n",
    "    if _len < 10:\n",
    "        errors.append((f\"{price_name} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è: {_len} —Å–ª–æ–≤, –Ω—É–∂–Ω–æ: {sentence_span1[1]- sentence_span2[0]}\", False, 10))\n",
    "        \n",
    "    return errors\n",
    "\n",
    "####################################### sentence_at_index\n",
    "\n",
    "def validate_markup(user_docs_ids) -> DataFrame:\n",
    "  userdocs = DataFrame()\n",
    "  userdocs['errors count']=0\n",
    "  userdocs['errors severity']=0\n",
    "  userdocs[\"–¥–∞—Ç–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\"]=None\n",
    "  userdocs['–î–∞—Ç–∞']=None\n",
    "\n",
    "  for k, oid in enumerate(user_docs_ids):\n",
    "# for k, oid in enumerate(['5fe34f64b770574a005553e6']):    \n",
    "    _id = str(oid)\n",
    "    oid = ObjectId(_id)\n",
    "    d = get_doc_by_id(oid)\n",
    "    try:\n",
    "        jd = DbJsonDoc(d)\n",
    "\n",
    "        attr_tree = jd.get_attributes_tree()  \n",
    "    #     print('',attr_tree)\n",
    "\n",
    "        if jd.user:\n",
    "            userdocs.at[_id,'–¥–∞—Ç–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è'] = jd.user['updateDate']\n",
    "        else:\n",
    "            userdocs.at[_id,'–¥–∞—Ç–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è'] = None\n",
    "            \n",
    "        userdocs.at[_id,'–¥–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞'] = jd.analysis['analyze_timestamp']\n",
    "        userdocs.at[_id,'–≤–µ—Ä—Å–∏—è'] = jd.analysis['version']\n",
    "            \n",
    "        userdocs.at[_id,'filename']=jd.filename\n",
    "\n",
    "        userdocs.at[_id,'link'] = f'https://gpn-audit.nemosoft.ru/#/audit/edit/{_id}'\n",
    "        userdocs.at[_id,'error'] = ''\n",
    "        userdocs.at[_id,'error missing'] = ''\n",
    "        userdocs.at[_id,'errors count'] = 0\n",
    "        userdocs.at[_id,'errors severity'] = 0\n",
    "\n",
    "\n",
    "\n",
    "        if not jd.analysis:\n",
    "            userdocs.at[_id, 'error'] = \"–ù–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!!\"\n",
    "\n",
    "        else:\n",
    "            add_error(userdocs, _id, validate_subject_len(jd, userdocs) )\n",
    "\n",
    "\n",
    "            add_error(userdocs, _id, validate_org_type(jd, userdocs, 0) )\n",
    "            add_error(userdocs, _id, validate_org_name(jd, userdocs, 0) )\n",
    "            add_error(userdocs, _id, validate_alias(jd, userdocs, 0) )\n",
    "\n",
    "            add_error(userdocs, _id, validate_org_type(jd, userdocs, 1) )\n",
    "            add_error(userdocs, _id, validate_org_name(jd, userdocs, 1) )\n",
    "            add_error(userdocs, _id, validate_alias(jd, userdocs, 1) )\n",
    "\n",
    "            add_error(userdocs, _id, validate_date(jd, userdocs) )\n",
    "            add_error(userdocs, _id, validate_number(jd, userdocs) )\n",
    "\n",
    "            add_error(userdocs, _id, validate_price(jd, userdocs) )\n",
    "    except Exception as e:\n",
    "        add_error(userdocs, _id, str(e) )\n",
    "        \n",
    "  return userdocs.sort_values(['errors severity'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bab14",
   "metadata": {
    "papermill": {
     "duration": 7.447322,
     "end_time": "2023-03-03T14:16:38.096339",
     "exception": false,
     "start_time": "2023-03-03T14:16:30.649017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "userdocs = validate_markup(user_docs_ids)\n",
    "\n",
    "_s = f\"#### {userdocs['errors count'].sum()} -- –≤—Å–µ–≥–æ –æ—à–∏–±–æ–∫/–Ω–µ–¥–æ—á–µ—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏ –≤ {len(userdocs)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö -- ({userdocs['errors count'].sum()/len(userdocs)} –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)\"\n",
    "display(Markdown(_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6a2f0-8bd5-4954-b8b3-48f5bbd08f9c",
   "metadata": {},
   "source": [
    "# Finding statistical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc09f6-6327-4745-9d93-b370f6dffe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE_STATS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1a682",
   "metadata": {
    "papermill": {
     "duration": 0.141446,
     "end_time": "2023-03-03T14:16:44.477814",
     "exception": false,
     "start_time": "2023-03-03T14:16:44.336368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if VALIDATE_STATS:\n",
    "    # userdocs = validate_markup(user_docs_ids)\n",
    "\n",
    "    def find_statistical_errors(userdocs):\n",
    "        l = list(userdocs['vat_unit'].values) + list( userdocs['currency'].values)\n",
    "        l= [str(k).lower() for k in l]\n",
    "        c = Counter(l)\n",
    "        rare_currency=[]\n",
    "        for k in c:\n",
    "            if c[k] < 2:\n",
    "                rare_currency.append(k)\n",
    "\n",
    "\n",
    "        l = list(userdocs['–ü—Å–µ–≤–¥–æ–Ω–∏–º 1'].values) + list( userdocs['–ü—Å–µ–≤–¥–æ–Ω–∏–º 2'].values)\n",
    "        l= [str(k).lower() for k in l]\n",
    "        c = Counter(l)\n",
    "        rare_aliases=[]\n",
    "        for k in c:\n",
    "            if c[k] < 2:\n",
    "                rare_aliases.append(k)\n",
    "        # rare_aliases\n",
    "\n",
    "\n",
    "        l = list(userdocs['–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 1'].values) + list( userdocs['–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 2'].values)\n",
    "        l= [str(k).lower() for k in l]\n",
    "        c = Counter(l)\n",
    "        rare_forms=[]\n",
    "        for k in c:\n",
    "            if c[k] < 2:\n",
    "                rare_forms.append(k)\n",
    "        # print(rare_forms)        \n",
    "\n",
    "\n",
    "\n",
    "        for i, row in userdocs.iterrows():\n",
    "            fs1 = str(row['–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 1']).lower()\n",
    "            fs2 = str(row['–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 2']).lower()\n",
    "\n",
    "            as1 = str(row['–ü—Å–µ–≤–¥–æ–Ω–∏–º 1']).lower()\n",
    "            as2 = str(row['–ü—Å–µ–≤–¥–æ–Ω–∏–º 2']).lower()\n",
    "\n",
    "\n",
    "            cs1 = str(row['vat_unit']).lower()\n",
    "            if cs1 in rare_currency:\n",
    "                print(i, f'[{cs1=}]')\n",
    "                add_error(userdocs, i, \"–í–∞–ª—é—Ç–∞ –Ω–∞–ª–æ–≥–∞ –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω–∞—è\" )\n",
    "\n",
    "            cs2 = str(row['currency']).lower()\n",
    "            if cs2 in rare_currency:\n",
    "                print(i, f'[{cs2=}]')\n",
    "                add_error(userdocs, i, \"–í–∞–ª—é—Ç–∞ –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω–∞—è\" )\n",
    "\n",
    "            if fs1 in rare_forms:\n",
    "                print(i, f'[{fs1=}]')\n",
    "                add_error(userdocs, i, \"–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 1 –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω–∞—è\" )\n",
    "            if fs2 in rare_forms:\n",
    "                print(i, f'[{fs2=}]')\n",
    "                add_error(userdocs, i, \"–§–æ—Ä–º–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ 2 –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω–∞—è\" )\n",
    "\n",
    "            if as1 in rare_aliases:\n",
    "                print(i, f'[{as1=}]')\n",
    "                add_error(userdocs, i, \"–ü—Å–µ–≤–¥–æ–Ω–∏–º 1 –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω—ã–π\" )\n",
    "            if as2 in rare_aliases:\n",
    "                print(i, f'[{as2=}]')\n",
    "                add_error(userdocs, i, \"–ü—Å–µ–≤–¥–æ–Ω–∏–º 2 –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω—ã–π\" )\n",
    "\n",
    "    find_statistical_errors(userdocs)            \n",
    "    userdocs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b005a8",
   "metadata": {
    "papermill": {
     "duration": 0.409652,
     "end_time": "2023-03-03T14:16:44.962206",
     "exception": false,
     "start_time": "2023-03-03T14:16:44.552554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if VALIDATE_STATS:\n",
    "\n",
    "    userdocs_subj = userdocs[ ['subject kind', '–ü—Å–µ–≤–¥–æ–Ω–∏–º 1', '–ü—Å–µ–≤–¥–æ–Ω–∏–º 2']]\n",
    "    # userdocs_subj\n",
    "\n",
    "    def get_alias_pair(v):\n",
    "        return ' -vs- '.join(   np.sort( list([ str(v['–ü—Å–µ–≤–¥–æ–Ω–∏–º 1']).lower(), str(v['–ü—Å–µ–≤–¥–æ–Ω–∏–º 2']).lower()])) ) \n",
    "\n",
    "\n",
    "\n",
    "    c=Counter([get_alias_pair(v) for i, v in userdocs_subj.iterrows()])\n",
    "    rare_aliases_pairs=[k for k in c if c[k] < 2]\n",
    "    for i, row in userdocs.iterrows():\n",
    "        d=get_alias_pair(row)\n",
    "        if d in rare_aliases_pairs:\n",
    "            print(i, f'[{d}]')\n",
    "            userdocs.at[i, 'strange alias combo'] =f\"–ù–µ–æ–±—ã—á–Ω–∞—è –ø–∞—Ä–∞ –ø—Å–µ–≤–¥–æ–Ω–∏–º–æ–≤: {d}\"\n",
    "\n",
    "    print('-'*20)\n",
    "    del rare_aliases_pairs\n",
    "    # # rare_aliases_pairs\n",
    "\n",
    "    # del d\n",
    "\n",
    "    def get_alias_subject_pair(v, i):\n",
    "        return ' / '.join(    list([ str(v[f'–ü—Å–µ–≤–¥–æ–Ω–∏–º {i}']).lower(), str(v['subject kind']).lower()])) \n",
    "\n",
    "    p1 = [get_alias_subject_pair(v, 1) for i, v in userdocs_subj.iterrows()]\n",
    "\n",
    "\n",
    "    c=Counter(p1+[get_alias_subject_pair(v, 2) for i, v in userdocs_subj.iterrows()])\n",
    "    rare_alias_subj_pairs=[k for k in c if c[k] < 3]\n",
    "\n",
    "    for i, row in userdocs.iterrows():\n",
    "        d1 = get_alias_subject_pair(row, 1)\n",
    "        d2 = get_alias_subject_pair(row, 2)\n",
    "\n",
    "        if d1 in rare_alias_subj_pairs:\n",
    "            print(i, f'[{d1}]')\n",
    "            userdocs.at[i, 'strange alias-subject'] =f\"–ù–µ–æ–±—ã—á–Ω–∞—è –ø–∞—Ä–∞ –ø—Å–µ–≤–¥–æ–Ω–∏–º-–ø—Ä–µ–¥–º–µ—Ç: {d1}\"\n",
    "\n",
    "        if d2 in rare_alias_subj_pairs :\n",
    "            print(i, f'[{d2}]')\n",
    "            userdocs.at[i, 'strange alias-subject'] =f\"–ù–µ–æ–±—ã—á–Ω–∞—è –ø–∞—Ä–∞ –ø—Å–µ–≤–¥–æ–Ω–∏–º-–ø—Ä–µ–¥–º–µ—Ç: {d2}\"\n",
    "\n",
    "    print('-'*20)        \n",
    "    rare_alias_subj_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5437f",
   "metadata": {
    "papermill": {
     "duration": 0.024092,
     "end_time": "2023-03-03T14:16:45.005801",
     "exception": false,
     "start_time": "2023-03-03T14:16:44.981709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if VALIDATE_STATS:\n",
    "    _s = f\"#### {datetime.today().strftime('%d.%m.%Y')} *–í—Å–µ–≥–æ* –æ—à–∏–±–æ–∫, –Ω–µ–¥–æ—á–µ—Ç–æ–≤ –∏ –ø—Ä–æ—á.: {userdocs['errors count'].sum()} –≤ {len(userdocs)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö -- ({userdocs['errors count'].sum()/len(userdocs):0.2f} –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)\"\n",
    "    display(Markdown(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ae900",
   "metadata": {
    "papermill": {
     "duration": 0.031429,
     "end_time": "2023-03-03T14:16:45.056464",
     "exception": false,
     "start_time": "2023-03-03T14:16:45.025035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if VALIDATE_STATS:\n",
    "    userdocs_subj = userdocs[ ['link','subject kind', '–ü—Å–µ–≤–¥–æ–Ω–∏–º 1', '–ü—Å–µ–≤–¥–æ–Ω–∏–º 2', 'strange alias combo', 'strange alias-subject' ]]\n",
    "\n",
    "    errors_subjects = userdocs_subj[userdocs_subj['strange alias combo'].notnull() | userdocs_subj['strange alias-subject'].notnull() ]\n",
    "    errors_subjects.to_csv(reports_path / \"errors_subjects.csv\", index=True)         \n",
    "    errors_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8541e",
   "metadata": {
    "papermill": {
     "duration": 0.039815,
     "end_time": "2023-03-03T14:16:45.115871",
     "exception": false,
     "start_time": "2023-03-03T14:16:45.076056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "very_bad_docs = userdocs[userdocs['errors severity']>10]\n",
    "very_bad_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da06ab-11df-4cbf-9dd1-e1c483aed34a",
   "metadata": {},
   "source": [
    "# Save reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb6663",
   "metadata": {
    "papermill": {
     "duration": 0.268868,
     "end_time": "2023-03-03T14:16:45.404870",
     "exception": false,
     "start_time": "2023-03-03T14:16:45.136002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# yseterday_userdocs = validate_markup(yseterday_ids)\n",
    "\n",
    "# yseterday_userdocs['errors severity'] = yseterday_userdocs['errors severity'].astype('int')\n",
    "# yseterday_userdocs['errors count'] = yseterday_userdocs['errors count'].astype('int')\n",
    "\n",
    "\n",
    "userdocs['errors severity'] = userdocs['errors severity'].astype('int')\n",
    "userdocs['errors count'] = userdocs['errors count'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d5da1",
   "metadata": {
    "papermill": {
     "duration": 0.064538,
     "end_time": "2023-03-03T14:16:45.489799",
     "exception": false,
     "start_time": "2023-03-03T14:16:45.425261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'errors_report_metric_prefix' in globals():\n",
    "    errors_report_metric_prefix = globals()['errors_report_metric_prefix']\n",
    "else:\n",
    "    errors_report_metric_prefix = \"user\"\n",
    " \n",
    "userdocs=userdocs.sort_values([\"errors severity\", \"errors count\", \"–¥–∞—Ç–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\"], ascending=False)\n",
    "userdocs.to_csv(reports_fn, index=True)    \n",
    "# userdocs\n",
    "\n",
    "mlflow.log_metric(f'severity', userdocs['errors severity'].sum())\n",
    "mlflow.log_metric(f'severity per doc', userdocs['errors severity'].sum()/len(userdocs))\n",
    "\n",
    "mlflow.log_metric(f'errors', userdocs['errors count'].sum())\n",
    "mlflow.log_metric(f'errors per doc', userdocs['errors count'].sum()/len(userdocs))\n",
    "\n",
    "mlflow.log_metric(f'docs count',  len(userdocs))\n",
    "mlflow.log_metric(f'severe docs',  len(very_bad_docs))\n",
    "mlflow.log_param('subset code',  str(errors_report_metric_prefix))\n",
    "mlflow.log_artifact(reports_fn)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86906d63-f0af-46a2-83ed-5e6e73ac2ebc",
   "metadata": {},
   "source": [
    "# End mlflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6bfae2",
   "metadata": {
    "papermill": {
     "duration": 0.02263,
     "end_time": "2023-03-03T14:16:59.528625",
     "exception": false,
     "start_time": "2023-03-03T14:16:59.505995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(active_mlflow_run.info)\n",
    "\n",
    "print('see results at')\n",
    "print(f'{mlflow.get_registry_uri()}/#/experiments/{active_mlflow_run.info.experiment_id}/runs/{active_mlflow_run.info.run_id}')\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 72.402502,
   "end_time": "2023-03-03T14:17:00.814723",
   "environment_variables": {},
   "exception": null,
   "input_path": "trainsets/export_trainset.ipynb",
   "output_path": "trainsets/export_trainset-result.ipynb",
   "parameters": {},
   "start_time": "2023-03-03T14:15:48.412221",
   "version": "2.3.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
