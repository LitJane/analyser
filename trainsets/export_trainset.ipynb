{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c0d6d5",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ",
    "papermill": {
     "duration": 0.009014,
     "end_time": "2023-01-27T06:24:52.535392",
     "exception": false,
     "start_time": "2023-01-27T06:24:52.526378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports\\\n",
    "\n",
    "\n",
    "TODO:\n",
    "1) check value/vat/no vat\n",
    "2) check headlines, fix it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849953b",
   "metadata": {
    "papermill": {
     "duration": 0.008724,
     "end_time": "2023-01-27T06:24:52.548646",
     "exception": false,
     "start_time": "2023-01-27T06:24:52.539922",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# !export GPN_DB_HOST=192.168.10.36\n",
    "LIMIT = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd7439-2bb4-469c-9bc9-5abe7fd75ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    LIMIT = int(LIMIT)\n",
    "except:\n",
    "    LIMIT = 5000\n",
    "        \n",
    " \n",
    "print(f'{LIMIT=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ad3a0",
   "metadata": {
    "papermill": {
     "duration": 0.008091,
     "end_time": "2023-01-27T06:24:52.561670",
     "exception": false,
     "start_time": "2023-01-27T06:24:52.553579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!ls -la ../../work\n",
    "# #!mv ../../../documents.json.zip ../../work\n",
    "# # !unzip '../../work/documents.json.zip' '../../work/documents.json'\n",
    "# !mv documents.json ../../work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59202d3f",
   "metadata": {
    "papermill": {
     "duration": 0.010537,
     "end_time": "2023-01-27T06:24:52.576917",
     "exception": false,
     "start_time": "2023-01-27T06:24:52.566380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMPORT_FRESH_ONLY = True # re-import all if False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19651a0e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg",
    "papermill": {
     "duration": 1.174238,
     "end_time": "2023-01-27T06:24:53.755874",
     "exception": false,
     "start_time": "2023-01-27T06:24:52.581636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logger = logging.getLogger('retrain_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.debug('--=logging started=--')\n",
    "\n",
    "print(\"tf\",tf.__version__)\n",
    "CPU = platform.processor()\n",
    "print (f'Running on CPU:{CPU}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c78626",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBqnGnQfKWF",
    "papermill": {
     "duration": 0.010097,
     "end_time": "2023-01-27T06:24:53.770893",
     "exception": false,
     "start_time": "2023-01-27T06:24:53.760796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "import analyser.hyperparams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38034de",
   "metadata": {
    "papermill": {
     "duration": 0.004731,
     "end_time": "2023-01-27T06:24:53.780533",
     "exception": false,
     "start_time": "2023-01-27T06:24:53.775802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b9059",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "papermill": {
     "duration": 1.917095,
     "end_time": "2023-01-27T06:24:55.702410",
     "exception": false,
     "start_time": "2023-01-27T06:24:53.785315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "from analyser.finalizer import get_doc_by_id\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from integration.db import get_mongodb_connection\n",
    "\n",
    "from datetime import datetime\n",
    "from math import log1p\n",
    "from pandas import DataFrame\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from colab_support.renderer import plot_embedding\n",
    "\n",
    "from analyser.structures import DocumentState\n",
    "\n",
    "from pathlib import Path\n",
    "from bson import ObjectId\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "from pymongo import ASCENDING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc21c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    " \n",
    "\n",
    "ml_flow_url = os.environ.get('MLFLOW_URL', \"http://192.168.10.38:5000\")\n",
    "mlflow.set_tracking_uri(ml_flow_url)\n",
    "print(f'{ml_flow_url=}', 'set MLFLOW_URL env var to re-define')\n",
    "mlflow.set_experiment(\"analyzer-export-ts\")\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac038c",
   "metadata": {
    "papermill": {
     "duration": 0.008546,
     "end_time": "2023-01-27T06:24:55.730011",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.721465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb664f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "papermill": {
     "duration": 0.009415,
     "end_time": "2023-01-27T06:24:55.744596",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.735181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_work_dir_default = Path(analyser.hyperparams.__file__).parent.parent.parent / 'work'\n",
    "work_dir = os.environ.get('GPN_WORK_DIR', _work_dir_default)\n",
    "\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.mkdir(work_dir)\n",
    "\n",
    "analyser.hyperparams.work_dir = work_dir\n",
    "reports_path = Path(analyser.hyperparams.__file__).parent.parent / 'training_reports/'\n",
    "\n",
    "\n",
    "assert os.path.isdir(analyser.hyperparams.work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4972a3-5cc1-449f-bc2d-688d0f80020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOKS_DIR = str(Path(analyser.__file__).parent.parent/'trainsets')\n",
    "print(f'{LIMIT=}')\n",
    "print(f'{NOTEBOOKS_DIR=}')\n",
    "print(f'{analyser.hyperparams.work_dir=}', )\n",
    "print(f'{reports_path=}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282ca87",
   "metadata": {
    "papermill": {
     "duration": 0.004965,
     "end_time": "2023-01-27T06:24:55.754721",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.749756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Query DB for contact IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256b8ee",
   "metadata": {
    "papermill": {
     "duration": 0.004863,
     "end_time": "2023-01-27T06:24:55.764548",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.759685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load meta data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d6d1a-f34a-4784-824f-5a4dea36ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_trainset_meta_fn = 'contract_trainset_meta.temp.csv'\n",
    "export_fn = str(Path(work_dir) / contract_trainset_meta_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082368b3-bfe1-44ec-b638-ef6a6742ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!touch {export_fn}\n",
    "!tail {export_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe54db",
   "metadata": {
    "papermill": {
     "duration": 0.012031,
     "end_time": "2023-01-27T06:24:55.971601",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.959570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stats[[\"user_correction_date\", 'analyze_date']].max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a6b9d",
   "metadata": {
    "papermill": {
     "duration": 0.074671,
     "end_time": "2023-01-27T06:24:56.051866",
     "exception": false,
     "start_time": "2023-01-27T06:24:55.977195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# stats = pd.read_csv(export_fn, index_col=0)\n",
    "mlflow.log_param('metafile',  export_fn)\n",
    "try:\n",
    "    stats = pd.read_csv(export_fn, index_col=0)\n",
    "    if not 'analyze_date' in stats:\n",
    "        stats['analyze_date'] = None\n",
    "        \n",
    "    if not 'documentType' in stats:\n",
    "        stats['documentType'] = 'Unknown'\n",
    "    \n",
    "    stats['user_correction_date'] = pd.to_datetime(stats['user_correction_date'], utc=True)\n",
    "    stats['analyze_date'] = pd.to_datetime(stats['analyze_date'], utc=True)\n",
    "\n",
    "    lastdate = stats[[\"user_correction_date\", 'analyze_date']].max().max()\n",
    "    \n",
    "    mlflow.log_param('last_doc_export_date',  lastdate)\n",
    "    mlflow.log_metric('docs_in_metafile_start',  len(stats))\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(f'ERROR: cannot load {export_fn}')\n",
    "    logger.exception(ex)\n",
    "    logger.error(f'cannot load {export_fn}')\n",
    "\n",
    "    lastdate = datetime(1900, 1, 1)\n",
    "    stats = DataFrame()\n",
    "    # stats = stats.astype({'currency':str})\n",
    "    stats['currency']=''\n",
    "    \n",
    "if not IMPORT_FRESH_ONLY:\n",
    "    lastdate = datetime(1900, 1, 1)\n",
    "    \n",
    "print(f'lastdate={lastdate} ; export_fn={export_fn}')\n",
    "stats['source'] = 'db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "display(Markdown(f\"#### {lastdate:%d.%m.%Y} -- дата последнего экспортированного документа\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a8fa8",
   "metadata": {
    "papermill": {
     "duration": 0.005487,
     "end_time": "2023-01-27T06:24:59.001630",
     "exception": false,
     "start_time": "2023-01-27T06:24:58.996143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00191302",
   "metadata": {
    "papermill": {
     "duration": 0.013761,
     "end_time": "2023-01-27T06:24:59.020888",
     "exception": false,
     "start_time": "2023-01-27T06:24:59.007127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_amount(attr_tree):\n",
    "  _value_tag = attr_tree.get('price')\n",
    "  amount = None\n",
    "  if _value_tag is not None:\n",
    "    amount = _value_tag.get('amount_netto')\n",
    "    if amount is None:\n",
    "      amount = _value_tag.get('amount_brutto')\n",
    "    if amount is None:\n",
    "      amount = _value_tag.get('amount')\n",
    "  return amount\n",
    "\n",
    "        \n",
    "    \n",
    "def add_stats_record(d: DbJsonDoc, stats: DataFrame, source='db'):\n",
    "  _id = str(d.get_id())\n",
    "    \n",
    "  attr_tree = d.get_attributes_tree()\n",
    " \n",
    "\n",
    "  stats.at[_id, 'checksum'] = d.get_tokens_for_embedding().get_checksum()\n",
    "  stats.at[_id, 'version'] = d.get_version_string()\n",
    "  stats.at[_id, 'documentType'] = d.documentType\n",
    "    \n",
    " \n",
    "  stats.at[_id, 'source'] = source\n",
    "  stats.at[_id, 'export_date'] = datetime.now()\n",
    "  stats.at[_id, 'len'] = len(d)\n",
    "  stats.at[_id, 'analyze_date'] = pd.to_datetime( d.analysis['analyze_timestamp'] , utc=True)\n",
    "\n",
    "\n",
    "  n_headers = len(d.analysis.get('headers', []))\n",
    "  stats.at[_id, 'headers'] = n_headers\n",
    "    \n",
    "  _value_tag = attr_tree.get('price')\n",
    "  \n",
    "  stats.at[_id, 'value'] = None\n",
    "  stats.at[_id, 'value_log1p'] = None\n",
    "  stats.at[_id, 'value_span'] = None\n",
    "  stats.at[_id, 'currency'] = None\n",
    "    \n",
    "  if _value_tag is not None:\n",
    "    amount = get_amount(attr_tree) \n",
    "    if amount:\n",
    "        \n",
    "        stats.at[_id, 'value'] = amount.get('value') \n",
    "        stats.at[_id, 'value_log1p'] = log1p(amount.get('value') )\n",
    "        stats.at[_id, 'value_span'] = amount.get('span', [0,0]) [0]\n",
    "    \n",
    "#         print( stats.at[_id, 'value'])\n",
    "    \n",
    "    stats.at[_id, 'currency'] = _value_tag.get('currency', {}).get('value')\n",
    "  \n",
    "  _orgs = attr_tree.get('orgs', [{},{}]) \n",
    "  if len(_orgs)>0:\n",
    "      if _orgs[0]:\n",
    "          stats.at[_id, 'org-1-name'] = _orgs[0].get('name', {}).get('value')\n",
    "          stats.at[_id, 'org-1-alias'] = _orgs[0].get('alias', {}).get('value')\n",
    "          stats.at[_id, 'org-1-type'] = _orgs[0].get('type', {}).get('value')\n",
    "\n",
    "      if len(_orgs)>1:\n",
    "          if _orgs[1]:\n",
    "            stats.at[_id, 'org-2-name'] = _orgs[1].get('name', {}).get('value')\n",
    "            stats.at[_id, 'org-2-alias'] = _orgs[1].get('alias', {}).get('value')\n",
    "            stats.at[_id, 'org-2-type'] = _orgs[1].get('type', {}).get('value')\n",
    "\n",
    "  stats.at[_id, 'subject'] = attr_tree.get('subject', {}).get('value')\n",
    "  stats.at[_id, 'subject confidence'] = attr_tree.get('subject', {}).get('confidence')\n",
    "    \n",
    "\n",
    "  span = attr_tree.get('subject', {}).get('span',[0,0])\n",
    "  subject_len = span[1] - span[0]\n",
    "  stats.at[_id, 'subj_len'] = subject_len  \n",
    "\n",
    "  stats.at[_id, 'user_correction_date'] = None\n",
    "  if d.user is not None:\n",
    "    # if 'attributes_tree' in d.user and 'creation_date' in d.user['attributes_tree']:\n",
    "    # stats.at[_id, 'user_correction_date'] = d.user['attributes_tree']['creation_date']\n",
    "    stats.at[_id, 'user_correction_date'] = pd.to_datetime(d.user[ 'updateDate'], utc=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # find_in_dict('attributes_tree.creation_date', d.user)\n",
    "\n",
    "  valid_state = (DocumentState.Excluded.value==d.state or DocumentState.Done.value==d.state)\n",
    "  if not valid_state:\n",
    "    print(_id, 'invalid state: ', d.state)\n",
    "  valid_struct = ('contract' in jd.analysis['attributes_tree'])\n",
    "  if not valid_struct:\n",
    "    print(_id, 'invalid structure: ', d.state)\n",
    "  stats.at[_id, 'valid'] = valid_state and valid_struct and subject_len>0 and subject_len<=300 and n_headers>0\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "d = get_doc_by_id(ObjectId('5fe34f62b770574a005553be'))\n",
    "jd = DbJsonDoc(d)\n",
    "add_stats_record(jd, stats)\n",
    "stats.loc['5fe34f62b770574a005553be']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf23b1",
   "metadata": {
    "papermill": {
     "duration": 0.005445,
     "end_time": "2023-01-27T06:24:56.078257",
     "exception": false,
     "start_time": "2023-01-27T06:24:56.072812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# load old json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69920a65",
   "metadata": {
    "papermill": {
     "duration": 2.906678,
     "end_time": "2023-01-27T06:24:58.990346",
     "exception": false,
     "start_time": "2023-01-27T06:24:56.083668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bson import json_util\n",
    "\n",
    "_DEBUG = False\n",
    "file_data=None\n",
    "\n",
    "try:\n",
    "    fn = work_dir / 'documents.json'\n",
    "    with open(fn) as file:\n",
    "        file_data = json.load(file, object_hook=json_util.object_hook)\n",
    "\n",
    "        print(f'total docs in {fn} is {len(file_data)}')    \n",
    "        _DEBUG = True\n",
    "except Exception as ex:\n",
    "    logger.exception(ex)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if _DEBUG:\n",
    "    a_doc_from_json=DbJsonDoc(file_data[3])\n",
    "\n",
    "    print(a_doc_from_json.get_attributes_tree())\n",
    "    print(a_doc_from_json.get_version_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d38cd",
   "metadata": {
    "papermill": {
     "duration": 1.598081,
     "end_time": "2023-01-27T06:25:00.624795",
     "exception": false,
     "start_time": "2023-01-27T06:24:59.026714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: import!!!\n",
    "\n",
    "from tf_support.super_contract_model import get_semantic_map_new, \\\n",
    "        semantic_map_keys_contract, t_semantic_map_keys_common, t_semantic_map_keys_org, t_semantic_map_keys_price\n",
    "\n",
    "# print(\"semantic_map_keys\", semantic_map_keys[8:14])\n",
    "# print(\"semantic_map_keys\", semantic_map_keys[4:7])\n",
    "# print(\"semantic_map_keys all\", semantic_map_keys)\n",
    "print(\"semantic_map_keys_contract\", semantic_map_keys_contract)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# del get_semantic_map_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if _DEBUG:\n",
    "    sm_test = get_semantic_map_new(a_doc_from_json)\n",
    "    plot_embedding(sm_test[0:150], f'get_semantic_map_new: semantic map')\n",
    "    print(sm_test['date-begin'].max())\n",
    "    print(sm_test['date-begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86305c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sm_test)\n",
    "# trimmed = sm_test[0:1200].values.T.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ece3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_map_keys[4:7]\n",
    "# position_enc = np.arange( 1,0, -0.0005)\n",
    "# # position_enc\n",
    "# max_pos = min(len(trimmed), len(position_enc))\n",
    "# trimmed[0:max_pos][-1] = position_enc[0:max_pos]\n",
    "# plot_embedding(trimmed , f'get_semantic_map_new: semantic map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428e91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8140e71",
   "metadata": {
    "papermill": {
     "duration": 1.837228,
     "end_time": "2023-01-27T06:25:02.468539",
     "exception": false,
     "start_time": "2023-01-27T06:25:00.631311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test one\n",
    "from analyser.headers_detector import get_tokens_features\n",
    "\n",
    "d = get_doc_by_id(ObjectId('636dfd1473925c8dae26f910'))\n",
    "if d is not None:\n",
    "    jd = DbJsonDoc(d)\n",
    "    semantic_map: DataFrame = get_semantic_map_new(jd)\n",
    "\n",
    "    tokens_features = get_tokens_features(jd.get_tokens_map_unchaged().tokens)\n",
    "\n",
    "    plot_embedding(semantic_map, f'get_semantic_map_new: semantic map')\n",
    "    plot_embedding(tokens_features, f'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e665fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e2ef0",
   "metadata": {},
   "source": [
    "### Adding data from old json file to the stats table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfb256",
   "metadata": {
    "papermill": {
     "duration": 21.195799,
     "end_time": "2023-01-27T06:25:23.697544",
     "exception": false,
     "start_time": "2023-01-27T06:25:02.501745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fff\n",
    "files_dict = {}\n",
    "files_cnt = 0\n",
    "if file_data:\n",
    "    k=0\n",
    "    for d in file_data:\n",
    "        k+=1 \n",
    "        jd = DbJsonDoc(d)\n",
    "    #     print( jd.analysis['analyze_timestamp'])\n",
    "        files_dict[jd.get_id()] = jd\n",
    "        try:            \n",
    "            add_stats_record(jd, stats, source = 'file')\n",
    "            files_cnt+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(jd.get_id())\n",
    "#             raise (e)\n",
    "\n",
    "        if k % 100 == 0:\n",
    "#             stats['user_correction_date'] = pd.to_datetime(stats['user_correction_date'], utc=False)\n",
    "#             stats['analyze_date'] = pd.to_datetime(stats['analyze_date'], utc=False)\n",
    "#             stats['export_date'] = pd.to_datetime(stats['export_date'], utc=False)\n",
    "            \n",
    "    \n",
    "            print(f'{k} of {len(file_data)}')\n",
    "            stats.to_csv(export_fn, index=True)\n",
    "            print(f'stats saved to {export_fn}')\n",
    "\n",
    "            \n",
    "mlflow.log_param('files_in_json',  files_cnt)            \n",
    "stats.to_csv(export_fn, index=True)\n",
    "# stats.to_csv('tmp.csv', index=True)\n",
    "\n",
    "print(f'stats saved to {export_fn}')\n",
    "\n",
    "# del file_data\n",
    "stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44010ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = stats.drop('63c506afe2456d59975e0fcd')\n",
    "stats[stats.index.duplicated()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05d74a",
   "metadata": {
    "papermill": {
     "duration": 0.007382,
     "end_time": "2023-01-27T06:25:23.712748",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.705366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "invalid = stats[stats['valid']==False]\n",
    "print(f'{len(invalid)}: number of invalid records')\n",
    "mlflow.log_metric('invalid records',  len(invalid))     \n",
    "invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7f066",
   "metadata": {
    "papermill": {
     "duration": 0.007444,
     "end_time": "2023-01-27T06:25:23.746476",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.739032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fetch fresh docs from Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9424da5",
   "metadata": {
    "papermill": {
     "duration": 0.011901,
     "end_time": "2023-01-27T06:25:23.765673",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.753772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from datetime import timedelta\n",
    "# if True:\n",
    "\n",
    "lastdate  += relativedelta(days=-15)  \n",
    "# lastdate = datetime.combine(lastdate, datetime.min.time())\n",
    "# lastdate = pd.to_datetime( lastdate , utc=True)\n",
    "lastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_date = datetime.now() + relativedelta(days=-2)  \n",
    "report_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cb158",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = datetime.now() + relativedelta(days=-1)  \n",
    "# {'analysis.analyze_timestamp': {'$gt': yesterday}},\n",
    "\n",
    "query = {'user.updateDate': {'$gt': report_date}}\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db['documents']\n",
    "sorting = [('analysis.analyze_timestamp', ASCENDING), ('user.updateDate', ASCENDING)]\n",
    "res = documents_collection.find(filter=query, \n",
    "                                sort=sorting,\n",
    "                                projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "                               ).limit(LIMIT)\n",
    "\n",
    "_r = list([i for i in res])\n",
    "# print(res[0])\n",
    "_s = f\"#### {len(_r)} -- всего размечено документов после {report_date:%d.%m.%Y}\"\n",
    "mlflow.log_metric('fresh_user_docs_in_db',  len(_r))     \n",
    "display(Markdown(_s))\n",
    "yseterday_ids=[i[\"_id\"] for i in _r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a716a1",
   "metadata": {
    "papermill": {
     "duration": 0.01798,
     "end_time": "2023-01-27T06:25:23.791502",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.773522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING\n",
    "\n",
    "query = {\n",
    "  '$and': [\n",
    "    {\"parse.documentType\":{ '$in': [\"AGREEMENT\", \"CONTRACT\", \"SUPPLEMENTARY_AGREEMENT\"] }  },\n",
    "      \n",
    "#     {\"state\": 15},\n",
    "    {'$or': [\n",
    "          {\"analysis.attributes_tree\": {\"$ne\": None}},\n",
    "          {\"user.attributes_tree\": {\"$ne\": None}}\n",
    "        ]},\n",
    "#       {'$and': [\n",
    "#           {\"user.updateDate\": {\"$ne\": None}},\n",
    "#           {'user.updateDate': {'$gt': lastdate}}\n",
    "#       ]}\n",
    "#     {\"user.attributes_tree.contract.people\": {\"$ne\": None}}\n",
    "\n",
    "    #     {'$or': [\n",
    "    #         {\"user.attributes_tree.contract.price.amount_netto\": {\"$ne\": None}},\n",
    "    #         {\"user.attributes_tree.contract.price.amount_brutto\": {\"$ne\": None}}\n",
    "    #     ]}\n",
    "    {'$or': [\n",
    "      {'analysis.analyze_timestamp': {'$gt': lastdate}}, \n",
    "      {'user.updateDate': {'$gt': lastdate}}\n",
    "    ]}\n",
    "  ]\n",
    "}\n",
    "\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db['documents']\n",
    "sorting = [('analysis.analyze_timestamp', ASCENDING), ('user.updateDate', ASCENDING)]\n",
    "res = documents_collection.find(filter=query, \n",
    "                                sort=sorting,\n",
    "                                projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "#                                             'analysis.attributes_tree.version': True,\n",
    "#                                             'analysis.attributes_tree.contract.subject': True}\n",
    "                               ).limit(LIMIT)\n",
    "\n",
    "res = list([i for i in res])\n",
    "mlflow.log_metric('fresh_docs_in_db',  len(res))     \n",
    "# print(res[0])\n",
    "_s = f\"#### {len(res)} -- всего проанализировано новых документов после {lastdate:%d.%m.%Y}\"\n",
    "display(Markdown(_s))\n",
    "\n",
    "res[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cddc9",
   "metadata": {
    "papermill": {
     "duration": 0.007579,
     "end_time": "2023-01-27T06:25:23.806807",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.799228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c0987",
   "metadata": {
    "papermill": {
     "duration": 0.011715,
     "end_time": "2023-01-27T06:25:23.860299",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.848584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_docs_ids = docs_ids = [i[\"_id\"] for i in res]\n",
    "print(f\"{len(new_docs_ids)=}\")\n",
    "# sample_id = ObjectId('637f7bf0e712cc2ff2e943d3')\n",
    "\n",
    "# sample_id = docs_ids[1]\n",
    "# sample_id\n",
    "# d = get_doc_by_id(sample_id)\n",
    "\n",
    "# jd = DbJsonDoc(d)\n",
    "# jd.get_id()\n",
    "# print(d['_id'])\n",
    "\n",
    "\n",
    "#==========================Test 1 doc\n",
    "    \n",
    "# print(jd.get_version_string())\n",
    " \n",
    "# add_stats_record(jd, stats)\n",
    "# stats.loc[str(jd._id)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e401da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in (1,2):\n",
    "#     for key in ['alias', 'name', 'type']:\n",
    "#         k=f'org-{i}-{key}'\n",
    "#         stats2[k] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_db_docs_ids = stats[stats.source=='db'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b91a78",
   "metadata": {
    "papermill": {
     "duration": 0.30268,
     "end_time": "2023-01-27T06:25:24.225089",
     "exception": false,
     "start_time": "2023-01-27T06:25:23.922409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_to_stats_list(docs_ids, stats):\n",
    "    for k, oid in enumerate(docs_ids):\n",
    "        d = get_doc_by_id(ObjectId(str(oid)))\n",
    "        try:\n",
    "            jd = DbJsonDoc(d)            \n",
    "        except Exception as e:\n",
    "            print(f'missing {oid=}')\n",
    "            stats = stats.drop(index=oid)\n",
    "            print (e)\n",
    "        \n",
    "        if jd:\n",
    "            add_stats_record(jd, stats)\n",
    "            \n",
    "        if k % 100 == 0:\n",
    "            print(f'{k} of {len(docs_ids)}')\n",
    "            stats.to_csv(export_fn, index=True)\n",
    "            print(f'stats saved to {export_fn}')\n",
    "        \n",
    "#--\n",
    "# add_to_stats_list(all_db_docs_ids, stats)     \n",
    "\n",
    "\n",
    "add_to_stats_list(new_docs_ids, stats)     \n",
    "\n",
    "stats.to_csv(export_fn, index=True)\n",
    "print(f'stats saved to {export_fn}')\n",
    "mlflow.log_metric('docs_in_metafile_result',  len(stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e578920-4056-4e2f-9a24-5448178d3200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34b676",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ids = list(\n",
    "#     stats[stats['source']=='db']\n",
    "#     [stats['documentType']=='undefined'][ stats['user_correction_date'].notna()].index)\n",
    "\n",
    "\n",
    "# add_to_stats_list(_ids)        \n",
    "# stats.to_csv(export_fn, index=True)\n",
    "\n",
    "s = stats[ stats['source']=='db'] \n",
    "s =s[s['headers'].isna()]\n",
    "# s = s[s['documentType']=='CONTRACT']\n",
    "# s = s[s['subj_len'].isna()]\n",
    "# s = s[s['user_correction_date'].notna()]\n",
    "# s = s[s['value']>0]\n",
    "_ids = list(s.index)\n",
    "print(len(_ids))\n",
    "# add_to_stats_list(_ids, stats)     \n",
    "# print(f'saving to {export_fn}; {len(stats)} records')\n",
    "# stats.to_csv(export_fn, index=True)\n",
    "del _ids\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc8934",
   "metadata": {
    "papermill": {
     "duration": 0.0148,
     "end_time": "2023-01-27T06:25:24.248320",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.233520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats = stats[stats.valid!=False]\n",
    "len(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80457a7",
   "metadata": {},
   "source": [
    "### Put all to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505ff2b",
   "metadata": {
    "papermill": {
     "duration": 0.035991,
     "end_time": "2023-01-27T06:25:24.293122",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.257131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2 = stats.copy()\n",
    "\n",
    "if False:\n",
    "    for i in (1,2):\n",
    "        for key in ['alias', 'name', 'type']:\n",
    "            k=f'org-{i}-{key}'\n",
    "            stats2[k] = stats2[k].str.lower()\n",
    "        \n",
    "        \n",
    "# stats2['org-1-alias'] = stats2['org-1-alias'].str.lower()\n",
    "# stats2['org-1-name'] = stats2['org-1-name'].str.lower()\n",
    "# stats2['org-2-name'] = stats2['org-2-name'].str.lower()\n",
    "\n",
    "# stats2['org-1-type'] = stats2['org-1-type'].str.lower()\n",
    "# stats2['org-2-type'] = stats2['org-2-type'].str.lower()\n",
    "stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44f5e4",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "### Collecting org types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20b110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b6cb1",
   "metadata": {
    "papermill": {
     "duration": 0.014158,
     "end_time": "2023-01-27T06:25:24.316132",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.301974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76791914",
   "metadata": {
    "papermill": {
     "duration": 0.014652,
     "end_time": "2023-01-27T06:25:24.339512",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.324860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['org-1-alias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eca769",
   "metadata": {
    "papermill": {
     "duration": 0.01456,
     "end_time": "2023-01-27T06:25:24.362839",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.348279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['org-2-alias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_column_name = \"score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811f524",
   "metadata": {
    "papermill": {
     "duration": 7.638486,
     "end_time": "2023-01-27T06:25:32.010191",
     "exception": false,
     "start_time": "2023-01-27T06:25:24.371705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calc_no_value_importance(stats2, column_name, score_k=2.):\n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        if pd.isna(row[column_name]):\n",
    "            stats2.at[i, score_column_name] *= score_k\n",
    "                \n",
    "def calc_user_importance(stats2):\n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        if not pd.isna(row['unseen']):\n",
    "\n",
    "            if row['unseen'] == True:\n",
    "                stats2.at[i, score_column_name] *= 2\n",
    "                \n",
    "def calc_val_importance(stats2):\n",
    " \n",
    "    for i, row in stats2.iterrows():\n",
    "        if not pd.isna(row['value']):\n",
    "            val = row['value']\n",
    "            if val >= 2000:\n",
    "                stats2.at[i, score_column_name] *= 1./log1p(val)\n",
    "            if val < 2000:\n",
    "                stats2.at[i, score_column_name] *= 4.\n",
    "        else:\n",
    "            stats2.at[i, score_column_name] *= 10.0 #error\n",
    "            \n",
    "def calc_column_val_importance(stats2, column_name, k=1.):\n",
    "    counts = stats2[column_name].value_counts()\n",
    " \n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        val = row[column_name]\n",
    "        if val in counts:\n",
    "            count = counts[val]\n",
    "            stats2.at[i, score_column_name] *= k * log1p(count)\n",
    "#             print(count, log1p(count),  1./log1p(count))\n",
    "        else:\n",
    "            stats2.at[i, score_column_name] *= 10.0 #error\n",
    "\n",
    "            \n",
    "for i, row in stats2.iterrows():        \n",
    "  stats2.at[i, 'url'] = f'http://gpn-audit.nemosoft.ru/#/audit/edit/{i}'\n",
    "\n",
    "    \n",
    "stats2 ['unseen'] = pd.isna( stats2['user_correction_date'])\n",
    "\n",
    "stats2[score_column_name] = 1.\n",
    " \n",
    "\n",
    "calc_no_value_importance(stats2, 'org-1-alias', 2.)\n",
    "calc_no_value_importance(stats2, 'org-2-alias', 2.)\n",
    "calc_no_value_importance(stats2, 'org-1-name', 3.)\n",
    "calc_no_value_importance(stats2, 'org-2-name', 3.)\n",
    "calc_no_value_importance(stats2, 'user_correction_date', 10.)\n",
    "calc_no_value_importance(stats2, 'subject', 4.)\n",
    "calc_no_value_importance(stats2, 'value', 5.)\n",
    "\n",
    "## calc_column_val_importance(stats2, 'org-1-name')\n",
    "## calc_column_val_importance(stats2, 'org-2-name')\n",
    "\n",
    "calc_column_val_importance(stats2, 'org-1-alias')\n",
    "calc_column_val_importance(stats2, 'org-2-alias')\n",
    "calc_column_val_importance(stats2, 'subject', k = 3.)\n",
    "\n",
    "calc_user_importance(stats2)\n",
    "calc_val_importance(stats2)\n",
    "\n",
    "# drop_duplicates(subset=['checksum']).\n",
    "stats2 = stats2.sort_values([score_column_name], ascending=True)\n",
    "print(len(stats2))\n",
    "stats2.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ec979",
   "metadata": {
    "papermill": {
     "duration": 0.013174,
     "end_time": "2023-01-27T06:25:32.033325",
     "exception": false,
     "start_time": "2023-01-27T06:25:32.020151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = stats2.sort_values(['score'], ascending=True).copy()\n",
    "# sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c6189",
   "metadata": {
    "papermill": {
     "duration": 0.009373,
     "end_time": "2023-01-27T06:25:32.052236",
     "exception": false,
     "start_time": "2023-01-27T06:25:32.042863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Find and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cba52",
   "metadata": {
    "papermill": {
     "duration": 0.045557,
     "end_time": "2023-01-27T06:25:32.107600",
     "exception": false,
     "start_time": "2023-01-27T06:25:32.062043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['len'] //= 10 #similar lens\n",
    "stats2['len'] *= 10 #similar lens\n",
    "stats2.sort_values(['len', 'org-1-name', 'org-2-name', 'org-1-alias','org-2-alias', 'value', 'subject'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cff7f",
   "metadata": {
    "papermill": {
     "duration": 0.777279,
     "end_time": "2023-01-27T06:25:32.895028",
     "exception": false,
     "start_time": "2023-01-27T06:25:32.117749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, row in stats2.iterrows():\n",
    "  _str = ':'.join(sorted([str(x) for x in\n",
    "                          [row['len'], row['org-1-alias'], row['org-2-alias'], row['org-1-name'], row['org-2-name'],\n",
    "                           row['value'], row['subject'], row['headers']  ]]))\n",
    "  stats2.at[i, 'hash'] = hash(_str)\n",
    "\n",
    "stats2 = stats2.sort_values(['hash'])\n",
    "print(len(stats2))\n",
    "stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3ddce",
   "metadata": {
    "papermill": {
     "duration": 0.685095,
     "end_time": "2023-01-27T06:25:33.590699",
     "exception": false,
     "start_time": "2023-01-27T06:25:32.905604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unks = stats2['hash'].value_counts()\n",
    "\n",
    "for i, row in stats2.iterrows():   \n",
    "    stats2.at[i, 'dups'] = unks[row['hash']]\n",
    "    \n",
    "dups = stats2.sort_values(['dups', 'hash'], ascending=False)\n",
    "# dups.to_csv(Path(work_dir) / 'contract_trainset_meta.duplicates.csv', index=True)\n",
    "\n",
    "# dups[40:].head(40)\n",
    "len(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af60db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dups.duplicated(subset=['hash'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e46b81",
   "metadata": {
    "papermill": {
     "duration": 0.01003,
     "end_time": "2023-01-27T06:25:33.631361",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.621331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0448d0",
   "metadata": {
    "papermill": {
     "duration": 0.022505,
     "end_time": "2023-01-27T06:25:33.663779",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.641274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s = dups.sort_values([score_column_name], ascending=True).copy()\n",
    "\n",
    "dups_s = dups_s.drop_duplicates(subset=['hash'])\n",
    "# dups_s = dups_s.sort_values([score_column_name], ascending=True)\n",
    "# dups_s = dups_s.sort_values([score_column_name], ascending=True)\n",
    "# dups_s = dups_s[dups_s[score_column_name] < 5000]\n",
    "print(len(dups_s))\n",
    "\n",
    "# dups_s [dups_s['len'] > 30000].head(40)\n",
    "\n",
    "mlflow.log_metric('duplicates',  len(stats) - len(dups_s))\n",
    "mlflow.log_metric('unique_docs',  len(dups_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79575d",
   "metadata": {
    "papermill": {
     "duration": 0.010055,
     "end_time": "2023-01-27T06:25:33.684339",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.674284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove rare subjects and very long docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7058571",
   "metadata": {
    "papermill": {
     "duration": 0.124581,
     "end_time": "2023-01-27T06:25:33.819081",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.694500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "ax = dups_s[dups_s['len'] < 30000] ['len'].plot.hist(bins=20, alpha=0.8)\n",
    "ax.set_title('Text Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbb927",
   "metadata": {
    "papermill": {
     "duration": 0.124476,
     "end_time": "2023-01-27T06:25:33.954310",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.829834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dups_s[score_column_name].min())\n",
    "plt.figure(figsize=(12, 4))\n",
    "ax = dups_s[score_column_name].plot.hist(bins=28, alpha=0.8)\n",
    "ax.set_title('Distibution by score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c643a1a",
   "metadata": {
    "papermill": {
     "duration": 0.417471,
     "end_time": "2023-01-27T06:25:34.382676",
     "exception": false,
     "start_time": "2023-01-27T06:25:33.965205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dups_s['valid'] = True\n",
    "\n",
    "print(\"excluding too long docs\")\n",
    "for i, row in dups_s.iterrows():   \n",
    "    if row['len'] > 30000:\n",
    "        print ('excluding', i, row[0], row['subject'], row['len'])\n",
    "        dups_s.at[i, 'valid'] = False  \n",
    "        \n",
    "dups_s = dups_s[dups_s['valid'] == True]\n",
    "\n",
    "print(\"excluding docs with rare subj\")\n",
    "cnts = dups_s ['subject'].value_counts()\n",
    "for i, row in dups_s.iterrows():   \n",
    "    if not pd.isna(row['subject']):\n",
    "        if cnts[row['subject']] < 3:\n",
    "            print ('excluding', i, row[0], row['subject'])\n",
    "            dups_s.at[i, 'valid'] = False\n",
    "\n",
    "dups_s = dups_s[dups_s['valid'] == True]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb790a25",
   "metadata": {
    "papermill": {
     "duration": 0.15943,
     "end_time": "2023-01-27T06:25:34.553400",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.393970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    cnt = dups_s['subject'].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(12, 6 ))\n",
    "    sns.barplot(x=cnt.values, y=cnt.index)\n",
    "\n",
    "    print(  cnt )\n",
    "\n",
    "    plt.title('Frequency Distribution of subjects')\n",
    "    plt.xlabel('Number of Occurrences')\n",
    "    plt.show()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845fad77",
   "metadata": {
    "papermill": {
     "duration": 0.092832,
     "end_time": "2023-01-27T06:25:34.657792",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.564960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s.index.name = '_id'\n",
    "dups_s.to_csv(Path(work_dir) / 'contract_trainset_meta.csv', index=True)\n",
    "\n",
    "\n",
    "_s = f\"### {len(dups_s)} - Всего новых документов в обучающем наборе на {lastdate:%d.%m.%Y}\"\n",
    "display(Markdown(_s))\n",
    "\n",
    "\n",
    "dups_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0caeec7",
   "metadata": {
    "papermill": {
     "duration": 0.050796,
     "end_time": "2023-01-27T06:25:34.720462",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.669666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s[dups_s['unseen']].to_csv(Path(work_dir) / 'contract_trainset_meta.to_markup.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522c2bb",
   "metadata": {
    "papermill": {
     "duration": 0.037327,
     "end_time": "2023-01-27T06:25:34.769709",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.732382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s[dups_s['unseen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pymongo import ASCENDING\n",
    "\n",
    "query = {\n",
    "  '$and': [\n",
    "    {\"parse.documentType\":{ '$in': [\"AGREEMENT\", \"CONTRACT\", \"SUPPLEMENTARY_AGREEMENT\"] }  },      \n",
    "    {\"user.attributes_tree\": {\"$ne\": None}},\n",
    "#       {'user.updateDate': {'$gt': lastdate}}    \n",
    "  ]\n",
    "}\n",
    "\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db['documents']\n",
    "sorting = [('analysis.analyze_timestamp', ASCENDING), ('user.updateDate', ASCENDING)]\n",
    "res = documents_collection.find(filter=query, \n",
    "                                sort=sorting,\n",
    "                                projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "#                                             'analysis.attributes_tree.version': True,\n",
    "#                                             'analysis.attributes_tree.contract.subject': True}\n",
    "                               ).limit(5000)\n",
    "\n",
    "res = list([i for i in res])\n",
    "\n",
    "_s = f\"#### {len(res)} -- Всего документов, которые размечены человеком\"\n",
    "display(Markdown(_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d4ae3",
   "metadata": {},
   "source": [
    "# 🤦 Find markup errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_docs_ids  = [i[\"_id\"] for i in res]\n",
    "print('user_docs_ids', len(user_docs_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781045a8-b38f-4589-a522-ef1b1577d2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors_report_file_prefix = \"user_markup_errors\"\n",
    "errors_report_metric_prefix = \"user\"\n",
    "user_docs_ids = user_docs_ids\n",
    "%run -i -t {NOTEBOOKS_DIR}/validate_markup.ipynb\n",
    "del user_docs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00e063-fbed-4ac2-bcec-d604473c7a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    ax = userdocs.groupby(\n",
    "        [userdocs[\"дата редактирования\"].dt.year, userdocs[\"дата редактирования\"].dt.month ])[\n",
    "            ['errors count']].count().plot(kind=\"bar\", figsize=(16,3))\n",
    "\n",
    "    plt.show()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39459e68-91fc-419b-8749-dacbe1a99300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# auto_docs = validate_markup(auto_docs_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e16cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistics\n",
    "### Collecting org types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c74e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "statistics = {}\n",
    "\n",
    "\n",
    "l = list(userdocs['Форма собственности 1'].values) + list( userdocs['Форма собственности 2'].values)\n",
    "l= [str(k) for k in l]\n",
    "c = Counter(l)\n",
    "\n",
    "\n",
    "statistics['org_types_cased'] = pd.DataFrame(c.items(),  columns = ['type', 'count']).sort_values(['count', 'type'], ascending=False)\n",
    "statistics['org_types_cased'].to_csv(\"statistics.org_types_cased.csv\")\n",
    "statistics['org_types_cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb903d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = list(userdocs['Форма собственности 1'].values) + list( userdocs['Форма собственности 2'].values)\n",
    "l= [str(k).lower() for k in l]\n",
    "c = Counter(l)\n",
    "\n",
    "\n",
    "statistics['org_types'] = pd.DataFrame(c.items(),  columns = ['type', 'count']).sort_values(['count', 'type'], ascending=False)\n",
    "statistics['org_types'].to_csv(\"statistics.org_types.csv\")\n",
    "statistics['org_types']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba49ae",
   "metadata": {},
   "source": [
    "### Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userdocs['subject kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefde1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_alias_pairs(cased:bool):\n",
    "    _df = pd.DataFrame()\n",
    "    for i, v in userdocs.iterrows():\n",
    "        if cased:\n",
    "            key = ' & '.join(   np.sort( list([ str(v['Псевдоним 1']), str(v['Псевдоним 2'])])) ) \n",
    "        else:\n",
    "            key = ' & '.join(   np.sort( list([ str(v['Псевдоним 1']).lower(), str(v['Псевдоним 2']).lower()])) ) \n",
    "        key2 = ' & '.join( [str(v['subject kind']), key ] )\n",
    "        _df.at[i, 'count'] = key2\n",
    "\n",
    "\n",
    "\n",
    "    _t = pd.DataFrame(_df['count'].value_counts())\n",
    "    for i, v in _t.iterrows():\n",
    "        s, a1, a2 = str(i).split('&')\n",
    "    #     print(a2)\n",
    "        _t.at[i, 'alias 1'] = a1\n",
    "        _t.at[i, 'alias 2'] = a2\n",
    "        _t.at[i, 'subject'] = s\n",
    "#     statistics['alias_pairs']=_t\n",
    "#     statistics['alias_pairs'].to_csv(\"statistics.alias_pair.csv\")\n",
    "#     _t\n",
    "    return _t\n",
    "\n",
    "\n",
    "statistics['alias_pairs']=count_alias_pairs(cased=False)\n",
    "statistics['alias_pairs'].to_csv(\"statistics.alias_pair.csv\")\n",
    "\n",
    "statistics['alias_pairs_cased']=count_alias_pairs(cased=True)\n",
    "statistics['alias_pairs_cased'].to_csv(\"statistics.alias_pair_cased.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044eda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics['alias_pairs_cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433ad01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics['alias_pairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707ed40",
   "metadata": {},
   "source": [
    "## Validate auto markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86078a9b-8ec2-430c-a91a-c4d2ef4a0963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc05aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del userdocs\n",
    "auto_docs_ids = stats[ (stats.user_correction_date.isnull()) & (stats.source=='db') & (stats['documentType']=='CONTRACT') ].index\n",
    "errors_report_file_prefix = \"ai_markup_errors\"\n",
    "errors_report_metric_prefix = \"ai\"\n",
    "user_docs_ids = auto_docs_ids\n",
    "%run -i -t {NOTEBOOKS_DIR}/validate_markup.ipynb\n",
    "del user_docs_ids\n",
    "\n",
    "auto_docs = userdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4f301",
   "metadata": {},
   "source": [
    "### Invalidate no-date docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f538e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_docs = auto_docs[ (auto_docs['Дата'].isnull()) | (auto_docs['errors severity']>=10)]\n",
    "bad_docs.index\n",
    "\n",
    "for i, row in stats.iterrows():\n",
    "    if i in bad_docs.index:\n",
    "        stats.at[i, 'valid'] = False\n",
    "# stats.loc[bad_docs.index].valid=False\n",
    "stats.loc[bad_docs.index].valid\n",
    "# for i, row in stats.iterrows():\n",
    "#     if i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46f76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_docs = auto_docs[ (~auto_docs['Дата'].isnull()) & (auto_docs['errors severity']<10)]\n",
    "good_docs\n",
    "# auto_docs[auto_docs['errors count']<3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc422f21",
   "metadata": {
    "papermill": {
     "duration": 0.011648,
     "end_time": "2023-01-27T06:25:34.793158",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.781510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f2c83",
   "metadata": {
    "papermill": {
     "duration": 0.015985,
     "end_time": "2023-01-27T06:25:34.820833",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.804848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17dcbb",
   "metadata": {
    "papermill": {
     "duration": 0.017036,
     "end_time": "2023-01-27T06:25:34.849754",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.832718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analyser.headers_detector import get_tokens_features\n",
    "from tf_support.embedder_elmo import ElmoEmbedder\n",
    "from analyser.legal_docs import embedd_tokens\n",
    "import os\n",
    "\n",
    "embedder = ElmoEmbedder.get_instance('elmo')  # lazy init\n",
    "\n",
    "def _dp_fn(doc_id, suffix):\n",
    "    return os.path.join(work_dir, 'datasets', f'{doc_id}-datapoint-{suffix}.npy')\n",
    "\n",
    "\n",
    "def save_contract_data_arrays(db_json_doc: DbJsonDoc):\n",
    "    # TODO: trim long documens according to contract parser\n",
    "\n",
    "    id_ = db_json_doc.get_id()\n",
    "\n",
    "    tokens_map: TextMap = db_json_doc.get_tokens_for_embedding()\n",
    "\n",
    "    # 1) EMBEDDINGS\n",
    "    print(len(tokens_map))\n",
    "    embeddings = embedd_tokens(tokens_map,\n",
    "                               embedder,\n",
    "                               log_key=f'id={id_} chs={tokens_map.get_checksum()}')\n",
    "\n",
    "    # 2) TOKEN FEATURES\n",
    "    token_features: DataFrame = get_tokens_features(db_json_doc.get_tokens_map_unchaged().tokens)\n",
    "\n",
    "    # 3) SEMANTIC MAP\n",
    "    semantic_map: DataFrame = get_semantic_map_new(db_json_doc)\n",
    "    #####\n",
    " \n",
    "    np.save(_dp_fn(id_, 'token_features'), token_features)\n",
    "    np.save(_dp_fn(id_, 'semantic_map'), semantic_map)\n",
    "    _embeddings_file = _dp_fn(id_, 'embeddings')\n",
    "    np.save(_embeddings_file, embeddings)\n",
    "    print(f'embeddings saved to {_embeddings_file} {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadf03f",
   "metadata": {
    "papermill": {
     "duration": 0.016668,
     "end_time": "2023-01-27T06:25:34.878280",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.861612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_docs_ids = [str(i) for i in new_docs_ids]\n",
    "intersection_set = set.intersection(set(list(dups_s.index)), set(_docs_ids)) \n",
    "len(intersection_set)\n",
    "# intersection_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_docs.index\n",
    "\n",
    "# intersection_set = set.intersection(set(list(dups_s.index)), set(_docs_ids)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f636a65",
   "metadata": {
    "papermill": {
     "duration": 0.011815,
     "end_time": "2023-01-27T06:25:34.902223",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.890408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainset fixtures\n",
    "#### Invalidate long subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e0c80",
   "metadata": {
    "papermill": {
     "duration": 12.542763,
     "end_time": "2023-01-27T06:25:47.456825",
     "exception": false,
     "start_time": "2023-01-27T06:25:34.914062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ids = list(dups_s.index)\n",
    "\n",
    "\n",
    "\n",
    "if not 'subject_pos' in stats:\n",
    "    stats['subject_pos'] = None\n",
    "        \n",
    "ids_of_long_subject_docs=[]\n",
    "\n",
    "for k, oid in enumerate(ids):\n",
    "\n",
    "    _id = str(oid)\n",
    "    iiid= ObjectId(_id)\n",
    "    \n",
    "    src = dups_s.loc[_id]['source']\n",
    "    jd = None\n",
    "    if src == 'db':\n",
    "#       if False:\n",
    "        d = get_doc_by_id(iiid)\n",
    "        if d is not None:\n",
    "            jd = DbJsonDoc(d)\n",
    "            \n",
    "            if jd.state not in [15,12]:\n",
    "                print(jd.state, iiid)\n",
    "            \n",
    "            stats.at[_id, 'documentType'] = jd.documentType\n",
    "            dups_s.at[_id, 'documentType'] = jd.documentType\n",
    "            \n",
    "            attr_tree = jd.get_attributes_tree()  \n",
    "            span = attr_tree.get('subject', {}).get('span',[0,0])\n",
    "            subject_len = span[1]-span[0]\n",
    "            stats.at[_id, 'subj_len'] = subject_len\n",
    "            dups_s.at[_id, 'subj_len'] = subject_len  \n",
    "            \n",
    "            stats.at[_id, 'subject_pos'] = span[0]\n",
    "            dups_s.at[_id, 'subject_pos'] = span[0]  \n",
    "                \n",
    "            if span[0]==0:\n",
    "                stats.at[_id, 'valid'] = False\n",
    "                dups_s.at[_id, 'valid'] = False\n",
    "                \n",
    "            if subject_len >= 200:\n",
    "                \n",
    "                stats.at[_id, 'valid'] = False\n",
    "                dups_s.at[_id, 'valid'] = False\n",
    "                \n",
    "                \n",
    "                \n",
    "                ids_of_long_subject_docs.append(oid)\n",
    "                \n",
    "                tm = jd.get_tokens_map_unchaged()\n",
    "                quote = tm.text_range(span)\n",
    "                print(k, iiid, span, span[1]-span[0])\n",
    "#                 print('-'*100)\n",
    "                sentence_span = tm.sentence_at_index(span[0])\n",
    "                quote2 = tm.text_range(sentence_span)\n",
    "                print(quote2[:50])\n",
    "                print('='*100)\n",
    "# stats[stats['documentType']=='CONTRACT']\n",
    "\n",
    "dups_s.to_csv(Path(work_dir) / 'contract_trainset_meta.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd569cc5",
   "metadata": {},
   "source": [
    "#### Fix missing headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168bc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from analyser.legal_docs import LegalDocument, PARAGRAPH_DELIMITER, GenericDocument, Paragraph\n",
    "from analyser.ml_tools import SemanticTag\n",
    "from integration.word_document_parser import join_paragraphs\n",
    "\n",
    "\n",
    "def save_analysis(db_document: DbJsonDoc, doc_dummie: LegalDocument) -> DbJsonDoc:\n",
    "  documents_collection = get_mongodb_connection()['documents']\n",
    "  \n",
    "  _analyse_json_obj: dict = doc_dummie.to_json_obj()\n",
    "  print(\"--analyse_json_obj['headers'] len=\", len(_analyse_json_obj['headers']))\n",
    "\n",
    "#   to_save = db_document.as_dict()\n",
    "#   to_save['headers'] = _analyse_json_obj['headers']\n",
    "\n",
    "  documents_collection.update_one({'_id': doc_dummie.get_id()}, {'$set': {'analysis.headers': _analyse_json_obj['headers']}})\n",
    " \n",
    "    \n",
    "  print(\"updated\", doc_dummie.get_id())\n",
    "\n",
    "\n",
    "def restore_headers (response, oid) :\n",
    "  doc = join_paragraphs(response, oid)\n",
    "  return doc\n",
    "   \n",
    "\n",
    "\n",
    "def restore_headlines(k, oid):    \n",
    "\n",
    "    iiid= ObjectId(oid)\n",
    "    src = dups_s.loc[oid]['source']\n",
    "    jd = None\n",
    "    if src == 'db':\n",
    "#       if False:\n",
    "        d = get_doc_by_id(iiid)\n",
    "        if d is not None:\n",
    "            jd = DbJsonDoc(d)\n",
    "            \n",
    "            if jd.state in [15,12]:\n",
    "                if len(jd.analysis.get('headers', [])) > 0:\n",
    "                    #  all is ok\n",
    "#                     print(oid)\n",
    "                    pass\n",
    "                else:\n",
    "                     doc_with_paragraphs = restore_headers(jd.parse, iiid)\n",
    "                     save_analysis(jd,  doc_with_paragraphs)\n",
    "                     print ('--', k, oid, len(doc_with_paragraphs.paragraphs))\n",
    "\n",
    "                    \n",
    "for k, oid in enumerate(ids):\n",
    "    restore_headlines(k,oid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a077b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore_headlines(1,'62d149474ae369b4976e31fc'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef4b62",
   "metadata": {
    "papermill": {
     "duration": 0.016802,
     "end_time": "2023-01-27T06:25:47.486145",
     "exception": false,
     "start_time": "2023-01-27T06:25:47.469343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ids_of_long_subject_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782260ac",
   "metadata": {
    "papermill": {
     "duration": 0.038847,
     "end_time": "2023-01-27T06:25:47.537145",
     "exception": false,
     "start_time": "2023-01-27T06:25:47.498298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211bb0f",
   "metadata": {
    "papermill": {
     "duration": 0.112493,
     "end_time": "2023-01-27T06:25:47.662379",
     "exception": false,
     "start_time": "2023-01-27T06:25:47.549886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats.to_csv(export_fn, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af0f64",
   "metadata": {},
   "source": [
    "# Main sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3187db1",
   "metadata": {
    "papermill": {
     "duration": 711.334283,
     "end_time": "2023-01-27T06:37:39.009670",
     "exception": false,
     "start_time": "2023-01-27T06:25:47.675387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# raise('test mode!')\n",
    "\n",
    "for k, oid in enumerate(list(intersection_set)): #enumerate(ids): \n",
    "# for k, oid in enumerate(list(ids)):\n",
    "    if k % 20 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "# \n",
    "    iiid= ObjectId(oid)\n",
    "    src = dups_s.loc[oid]['source']\n",
    "    jd = None\n",
    "    if src == 'db':\n",
    "#       if False:\n",
    "        d = get_doc_by_id(iiid)\n",
    "        if d is not None:\n",
    "          jd = DbJsonDoc(d)\n",
    "    else:\n",
    "#         if False:\n",
    "        print(oid, k, 'of', len(ids), src)\n",
    "        jd = files_dict[iiid]\n",
    "        \n",
    "    print(oid, k, 'of', len(ids), src)\n",
    "\n",
    "    if jd is not None:\n",
    "        if jd.state in [15,12]:                \n",
    "            save_contract_data_arrays(jd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3efe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('see results at')\n",
    "print(f'{mlflow.get_registry_uri()}/#/experiments/{mlflow.last_active_run().info.experiment_id}/runs/{mlflow.last_active_run().info.run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac57aa-972c-49bb-8fc8-2ddadf2c2269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 776.413935,
   "end_time": "2023-01-27T06:37:48.026538",
   "environment_variables": {},
   "exception": null,
   "input_path": "trainsets/export_trainset.ipynb",
   "output_path": "trainsets/export_trainset.ipynb",
   "parameters": {},
   "start_time": "2023-01-27T06:24:51.612603",
   "version": "2.3.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
