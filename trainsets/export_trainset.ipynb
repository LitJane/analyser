{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84416aa0",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ",
    "papermill": {
     "duration": 0.02715,
     "end_time": "2021-08-03T13:00:10.959150",
     "exception": false,
     "start_time": "2021-08-03T13:00:10.932000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d3c58-2d3a-4f27-a017-c9acd65e4806",
   "metadata": {
    "papermill": {
     "duration": 0.021947,
     "end_time": "2021-08-03T13:00:10.998129",
     "exception": false,
     "start_time": "2021-08-03T13:00:10.976182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMPORT_FRESH_ONLY = True # re-import all if False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d79173",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg",
    "papermill": {
     "duration": 1.861673,
     "end_time": "2021-08-03T13:00:12.870686",
     "exception": false,
     "start_time": "2021-08-03T13:00:11.009013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logger = logging.getLogger('retrain_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.debug('--=logging started=--')\n",
    "\n",
    "print(tf.__version__)\n",
    "CPU = platform.processor()\n",
    "print (f'Running on CPU:{CPU}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e198c1d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBqnGnQfKWF",
    "papermill": {
     "duration": 0.016512,
     "end_time": "2021-08-03T13:00:12.895142",
     "exception": false,
     "start_time": "2021-08-03T13:00:12.878630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import analyser.hyperparams \n",
    "analyser.hyperparams.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae495501",
   "metadata": {
    "papermill": {
     "duration": 0.007981,
     "end_time": "2021-08-03T13:00:12.911919",
     "exception": false,
     "start_time": "2021-08-03T13:00:12.903938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1029623",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "papermill": {
     "duration": 0.011883,
     "end_time": "2021-08-03T13:00:12.931780",
     "exception": false,
     "start_time": "2021-08-03T13:00:12.919897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "from analyser.finalizer import get_doc_by_id\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from integration.db import get_mongodb_connection\n",
    "\n",
    "from datetime import datetime\n",
    "from math import log1p\n",
    "from pandas import DataFrame\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from colab_support.renderer import plot_embedding\n",
    "\n",
    "from analyser.structures import DocumentState\n",
    "\n",
    "from pathlib import Path\n",
    "from bson import ObjectId\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from tf_support.super_contract_model import seq_labels_contract, seq_labels_contract_swap_orgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55a33b",
   "metadata": {
    "papermill": {
     "duration": 0.008023,
     "end_time": "2021-08-03T13:00:12.947888",
     "exception": false,
     "start_time": "2021-08-03T13:00:12.939865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cde660",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "papermill": {
     "duration": 1.475714,
     "end_time": "2021-08-03T13:00:14.431563",
     "exception": false,
     "start_time": "2021-08-03T13:00:12.955849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_work_dir_default = Path(analyser.hyperparams.__file__).parent.parent.parent / 'work'\n",
    "work_dir = os.environ.get('GPN_WORK_DIR', _work_dir_default)\n",
    "\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.mkdir(work_dir)\n",
    "\n",
    "analyser.hyperparams.work_dir = work_dir\n",
    " \n",
    "\n",
    "print('work_dir=', analyser.hyperparams.work_dir)\n",
    "assert os.path.isdir(analyser.hyperparams.work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed5139",
   "metadata": {
    "papermill": {
     "duration": 0.008357,
     "end_time": "2021-08-03T13:00:14.449100",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.440743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Query DB for contact IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589992da-6190-42a9-a056-ef015966620c",
   "metadata": {
    "papermill": {
     "duration": 0.00879,
     "end_time": "2021-08-03T13:00:14.466682",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.457892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load meta data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1f699",
   "metadata": {
    "papermill": {
     "duration": 0.036079,
     "end_time": "2021-08-03T13:00:14.510860",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.474781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_fn = str(Path(work_dir) / 'contract_trainset_meta.temp.csv')\n",
    "# stats = pd.read_csv(export_fn, index_col=0)\n",
    "\n",
    "try:\n",
    "    stats = pd.read_csv(export_fn, index_col=0)\n",
    "    if not 'analyze_date' in stats:\n",
    "        stats['analyze_date'] = None\n",
    "    stats['analyze_date'] = pd.to_datetime(stats['analyze_date'])\n",
    "    stats['user_correction_date'] = pd.to_datetime(stats['user_correction_date'])\n",
    "\n",
    "    lastdate = stats[[\"user_correction_date\", 'analyze_date']].max().max()\n",
    "    \n",
    "except Exception as ex:\n",
    "    logger.exception(ex)\n",
    "    logger.error(f'cannot load {export_fn}')\n",
    "\n",
    "    lastdate = datetime(1900, 1, 1)\n",
    "    stats = DataFrame()\n",
    "    \n",
    "if not IMPORT_FRESH_ONLY:\n",
    "    lastdate = datetime(1900, 1, 1)\n",
    "    \n",
    "print(lastdate, export_fn)\n",
    "\n",
    "stats['source'] = 'db'\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4783cd-1b41-49b0-afa1-9b7f8fdeb7ba",
   "metadata": {},
   "source": [
    "# load old json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce657108-2e07-4fe4-bc7a-025022ebd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bson import json_util\n",
    "\n",
    "fn = work_dir / 'documents.json'\n",
    "with open(fn) as file:\n",
    "    file_data = json.load(file, object_hook=json_util.object_hook)\n",
    "    \n",
    "    print(f'total docs in {fn} is {len(file_data)}')    \n",
    "    \n",
    "    \n",
    "\n",
    "_DEBUG = True\n",
    "\n",
    "\n",
    "if _DEBUG:\n",
    "    a_doc_from_json=DbJsonDoc(file_data[3])\n",
    "\n",
    "a_doc_from_json.get_attributes_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d590aa-b23a-4bb0-89f4-32dd2aaffd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22e876-76a8-4007-bcd7-88f101e9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc_from_json.get_version_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9812c-72a3-483d-a518-e6ee7c80bb47",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8269ba3-e9bf-48b0-90d9-20c2259c7354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_amount(attr_tree):\n",
    "  _value_tag = attr_tree.get('price')\n",
    "  amount = None\n",
    "  if _value_tag is not None:\n",
    "    amount = _value_tag.get('amount_netto')\n",
    "    if amount is None:\n",
    "      amount = _value_tag.get('amount_brutto')\n",
    "    if amount is None:\n",
    "      amount = _value_tag.get('amount')\n",
    "  return amount\n",
    "\n",
    "        \n",
    "    \n",
    "def add_stats_record(d: DbJsonDoc, stats: DataFrame, source='db'):\n",
    "  _id = str(d.get_id())\n",
    "    \n",
    "  attr_tree = d.get_attributes_tree()\n",
    " \n",
    "\n",
    "  stats.at[_id, 'checksum'] = d.get_tokens_for_embedding().get_checksum()\n",
    "  stats.at[_id, 'version'] = d.get_version_string()\n",
    "    \n",
    " \n",
    "  stats.at[_id, 'source'] = source\n",
    "  stats.at[_id, 'export_date'] = datetime.now()\n",
    "  stats.at[_id, 'len'] = len(d)\n",
    "  stats.at[_id, 'analyze_date'] = d.analysis['analyze_timestamp']\n",
    "\n",
    "\n",
    "  _value_tag = attr_tree.get('price')\n",
    "  \n",
    "  if _value_tag is not None:\n",
    "    amount = get_amount(attr_tree) \n",
    "    if amount:\n",
    "        \n",
    "        stats.at[_id, 'value'] = amount.get('value') \n",
    "        stats.at[_id, 'value_log1p'] = log1p(amount.get('value') )\n",
    "        stats.at[_id, 'value_span'] = amount.get('span', [0,0]) [0]\n",
    "#         print( stats.at[_id, 'value'])\n",
    "    \n",
    "    stats.at[_id, 'currency'] = _value_tag.get('currency', {}).get('value')\n",
    "  \n",
    "  _orgs = attr_tree.get('orgs', [{},{}]) \n",
    "  if len(_orgs)>0:\n",
    "      stats.at[_id, 'org-1-name'] = _orgs[0].get('name', {}).get('value')\n",
    "      stats.at[_id, 'org-1-alias'] = _orgs[0].get('alias', {}).get('value')\n",
    "\n",
    "      if len(_orgs)>1:\n",
    "        stats.at[_id, 'org-2-name'] = _orgs[1].get('name', {}).get('value')\n",
    "        stats.at[_id, 'org-2-alias'] = _orgs[1].get('alias', {}).get('value')\n",
    "    \n",
    "  stats.at[_id, 'subject'] = attr_tree.get('subject', {}).get('value')\n",
    "  stats.at[_id, 'subject confidence'] = attr_tree.get('subject', {}).get('confidence')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  if d.user is not None:\n",
    "    # if 'attributes_tree' in d.user and 'creation_date' in d.user['attributes_tree']:\n",
    "    # stats.at[_id, 'user_correction_date'] = d.user['attributes_tree']['creation_date']\n",
    "    stats.at[_id, 'user_correction_date'] = d.user[\n",
    "      'updateDate']  # find_in_dict('attributes_tree.creation_date', d.user)\n",
    "\n",
    "  stats.at[_id, 'valid'] = (DocumentState.Excluded.value==d.state or DocumentState.Done.value==d.state) and ('contract' in jd.analysis['attributes_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4eb8d-b6d5-41d9-8723-d82b7ac7524b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "_DEBUG=False\n",
    "def get_semantic_map(doc: DbJsonDoc):\n",
    "    attr_tree = doc.get_attributes_tree()\n",
    "    _len = len(doc)\n",
    "    df = DataFrame()\n",
    "    for sl in seq_labels_contract:    \n",
    "        df[sl] = np.zeros(_len)\n",
    "        \n",
    "    av = np.zeros(_len)\n",
    "    headers = doc.analysis['headers']\n",
    "    for h in headers:\n",
    "        av[h['span'][0]:h['span'][1]] = 1.0\n",
    "    \n",
    "    df['headline_h1'] = av\n",
    "    \n",
    "    def add_av(name, span, av=None):\n",
    "        if av is None:            \n",
    "            av = np.zeros(_len)\n",
    "        if span:\n",
    "            av[span[0]:span[1]] = 1.0\n",
    "        df[name] = av\n",
    " \n",
    "    \n",
    "    for n in ['subject', 'date', 'number']:\n",
    "        add_av(n, attr_tree.get(n, {}).get('span'))\n",
    "    \n",
    "    order = seq_labels_contract\n",
    "    for i in [1, 2]:\n",
    "        for n in ['name', 'alias', 'type']:\n",
    "            _key = f'org-{i}-{n}'\n",
    "            try:                \n",
    "                add_av(_key, attr_tree['orgs'][i-1][n]['span'])\n",
    "            except Exception as e:\n",
    "                print(e, _key)\n",
    "                if _DEBUG:\n",
    "                    print(json.dumps( attr_tree['orgs'][i-1], sort_keys=True, indent=4))\n",
    "                    print('ERROR', i, n, doc.get_id(), e, attr_tree.get('orgs'))\n",
    "\n",
    "#     print(df['org-1-name'].argmax())\n",
    "#     print(df['org-2-name'].argmax())\n",
    "    \n",
    "    if df['org-2-name'].argmax() < df['org-1-name'].argmax():\n",
    "        order = seq_labels_contract_swap_orgs\n",
    " \n",
    "    _value_tag = attr_tree.get('price')\n",
    "#     print('_value_tag',_value_tag)\n",
    "#     print()\n",
    "  \n",
    "    if _value_tag is not None:\n",
    "        amount = get_amount(attr_tree) \n",
    "#         print('amount', amount)\n",
    "#         print()\n",
    "#         print('currency')\n",
    "#         print(_value_tag.get('currency',{}).get('span'))\n",
    "        \n",
    "        if amount:\n",
    "            add_av('sign_value_currency/value', amount.get('span'))\n",
    "            \n",
    "        add_av('sign_value_currency/currency', _value_tag.get('currency',{}).get('span'))\n",
    "        add_av('sign_value_currency/sign', _value_tag.get('sign',{}).get('span'))\n",
    "            \n",
    "    return df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fdd9f-88c4-4c10-9c75-c558c4791246",
   "metadata": {},
   "outputs": [],
   "source": [
    " del get_semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a37f29-1632-466e-8481-8e4b0f55362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "_DEBUG=False\n",
    "\n",
    "\n",
    "semantic_map_keys = [\n",
    "    'headline',\n",
    "    'subject', \n",
    "    'date', \n",
    "    'number',\n",
    "    'org-name', \n",
    "    'org-alias', \n",
    "    'org-type'\n",
    "]\n",
    "\n",
    "semantic_map_keys += ['amount', 'amount_brutto', 'amount_netto', 'vat', 'sign', 'currency', 'vat_unit', 'value']\n",
    "\n",
    "semantic_map_keys_contract = []\n",
    "for _name in semantic_map_keys:\n",
    "    semantic_map_keys_contract.append(_name + \"-begin\")\n",
    "    semantic_map_keys_contract.append(_name + \"-end\")\n",
    "\n",
    "print(semantic_map_keys[8:14])\n",
    "print(semantic_map_keys[4:7])\n",
    "\n",
    "def get_semantic_map_new(doc: DbJsonDoc):\n",
    "    \n",
    "    _len = len(doc)\n",
    "    df = DataFrame()\n",
    "    \n",
    "    for sl in semantic_map_keys_contract:    \n",
    "        df[sl] = np.zeros(_len)\n",
    "    \n",
    "    attr_tree = doc.get_attributes_tree()\n",
    "    \n",
    "    def get_av(name):\n",
    "        if name in df:\n",
    "            return df[name]\n",
    "        else:\n",
    "            av = np.zeros(_len, np.float)\n",
    "            df[name] = av\n",
    "            return av\n",
    "\n",
    "            \n",
    "    def add_span_vectors(_name, span):\n",
    "#         print('add_span_vectors',span)\n",
    "        bn = _name + \"-begin\"\n",
    "        en = _name + \"-end\"\n",
    "        b = get_av(bn)\n",
    "        e = get_av(en)\n",
    "        if not span is None:\n",
    "            df[bn][ span[0] ]=1.\n",
    "            df[en][ span[1] ]=1.\n",
    " \n",
    "        \n",
    "\n",
    "    # Headers\n",
    "    headers = doc.analysis['headers']\n",
    "    for h in headers:\n",
    "        add_span_vectors('headline', h['span'])\n",
    " \n",
    " \n",
    "    for n in semantic_map_keys[1:4]:\n",
    "        span = attr_tree.get(n, {}).get('span')\n",
    "        add_span_vectors(n, span)\n",
    "\n",
    " \n",
    "    for i in [1, 2]:\n",
    "        for n in semantic_map_keys[4:7]:\n",
    "            try:                \n",
    "                _nm = n.replace('org-', f'')\n",
    "                span = attr_tree['orgs'][i-1][_nm]['span']\n",
    "                add_span_vectors(n, span)\n",
    "            except Exception as e:\n",
    "                logger.exception(e)\n",
    "                print('ERROR (sp)', e, i, n,  _nm)\n",
    "                if _DEBUG:\n",
    "                    print(json.dumps( attr_tree['orgs'][i-1], sort_keys=True, indent=4))\n",
    "                    print('ERROR', i, n, doc.get_id(), e, attr_tree.get('orgs'))\n",
    " \n",
    "\n",
    "    _value_tag = attr_tree.get('price', {})\n",
    "    add_span_vectors(\"value\", _value_tag.get('span'))\n",
    " \n",
    "    if _value_tag is not None:\n",
    "        amount = get_amount(attr_tree) \n",
    "        if amount:\n",
    "            add_span_vectors('amount', amount.get('span'))\n",
    "        for n in semantic_map_keys[8:14]:\n",
    "            add_span_vectors(n, _value_tag.get(n,{}).get('span'))\n",
    "           \n",
    "    return df [semantic_map_keys_contract]\n",
    "\n",
    "sm_test = get_semantic_map_new(a_doc_from_json)\n",
    "plot_embedding(sm_test[0:400], f'get_semantic_map_new: semantic map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fbd09-8cf5-47ca-8ef7-b69c6611253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_test['date-begin'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597112b-9601-4e69-868a-2d27e2800cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_doc_from_json=DbJsonDoc(file_data[3])\n",
    "# from bson import BSON\n",
    "# decode_doc = BSON.decode(file_data[0])\n",
    "# print(decode_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2575bf4-9224-43af-953d-278516e4d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {}\n",
    "k=0\n",
    "for d in file_data:\n",
    "    k+=1 \n",
    "    jd = DbJsonDoc(d)\n",
    "#     print( jd.analysis['analyze_timestamp'])\n",
    "    files_dict[jd.get_id()] = jd\n",
    "    try:\n",
    "        add_stats_record(jd, stats, 'file')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(jd.get_id())\n",
    "#         raise (e)\n",
    "    \n",
    "    if k % 20 == 0:\n",
    "        print(f'{k} of {len(file_data)}')\n",
    "        stats.to_csv(export_fn, index=True)\n",
    "        print(f'stats saved to {export_fn}')\n",
    "        \n",
    "stats.to_csv(export_fn, index=True)\n",
    "print(f'stats saved to {export_fn}')\n",
    "stats\n",
    "\n",
    "\n",
    "del file_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1db37-2acc-4ead-bdf0-49e115e3c066",
   "metadata": {
    "papermill": {
     "duration": 0.008442,
     "end_time": "2021-08-03T13:00:14.527916",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.519474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fetch fresh docs from Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1f82a-a1b6-4d34-abe3-5264f4e5e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if True:\n",
    "    lastdate = date.today() + relativedelta(days=-10)\n",
    "    lastdate = datetime.combine(lastdate, datetime.min.time())\n",
    "\n",
    "lastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c3cda",
   "metadata": {
    "papermill": {
     "duration": 0.153979,
     "end_time": "2021-08-03T13:00:14.690371",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.536392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING\n",
    "\n",
    "query = {\n",
    "  '$and': [\n",
    "#     {\"parse.documentType\": \"CONTRACT\"},\n",
    "      \n",
    "#     {\"state\": 15},\n",
    "    {'$or': [\n",
    "          {\"analysis.attributes_tree\": {\"$ne\": None}},\n",
    "          {\"user.attributes_tree\": {\"$ne\": None}}\n",
    "        ]},\n",
    "#       {'$and': [\n",
    "#           {\"user.updateDate\": {\"$ne\": None}},\n",
    "#           {'user.updateDate': {'$gt': lastdate}}\n",
    "#       ]}\n",
    "#     {\"user.attributes_tree.contract.people\": {\"$ne\": None}}\n",
    "\n",
    "    #     {'$or': [\n",
    "    #         {\"user.attributes_tree.contract.price.amount_netto\": {\"$ne\": None}},\n",
    "    #         {\"user.attributes_tree.contract.price.amount_brutto\": {\"$ne\": None}}\n",
    "    #     ]}\n",
    "    {'$or': [\n",
    "      {'analysis.analyze_timestamp': {'$gt': lastdate}},\n",
    "      {'user.updateDate': {'$gt': lastdate}}\n",
    "    ]}\n",
    "  ]\n",
    "}\n",
    "\n",
    "db = get_mongodb_connection()\n",
    "documents_collection = db['documents']\n",
    "sorting = [('analysis.analyze_timestamp', ASCENDING), ('user.updateDate', ASCENDING)]\n",
    "res = documents_collection.find(filter=query, \n",
    "                                sort=sorting,\n",
    "                                projection={'_id': True, 'user.updateDate':True, 'state':True, 'parse.documentType':True}\n",
    "#                                             'analysis.attributes_tree.version': True,\n",
    "#                                             'analysis.attributes_tree.contract.subject': True}\n",
    "                               ).limit(1000)\n",
    "\n",
    "res = list([i for i in res])\n",
    "# print(res[0])\n",
    "_s = f\"#### Всего новых документов после {lastdate} : {len(res)}\"\n",
    "display(Markdown(_s))\n",
    "\n",
    "res[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e5226-9de4-46c2-a4d1-725904358ec3",
   "metadata": {},
   "source": [
    "# Prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3e9cd-e51a-4541-a219-73e584847f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq_labels_contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec573ccb-97cb-477f-9379-52403a186899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845f6b5-e4b9-4021-ae36-611d68296d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs_ids = docs_ids = [i[\"_id\"] for i in res]\n",
    "print(len(new_docs_ids))\n",
    "sample_id = ObjectId('5fb278c1d8c9df1ed1236d47')\n",
    "\n",
    "# sample_id = docs_ids[1]\n",
    "# sample_id\n",
    "d = get_doc_by_id(sample_id)\n",
    "\n",
    "jd = DbJsonDoc(d)\n",
    "# jd.get_id()\n",
    "print(d['_id'])\n",
    "\n",
    "\n",
    "#==========================Test 1 doc\n",
    "    \n",
    "print(jd.get_version_string())\n",
    " \n",
    "add_stats_record(jd, stats)\n",
    "stats.loc[str(jd._id)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef128113-6be4-456c-9918-0859a339da37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fc45b-4bb0-4264-936a-021d4e2ca288",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(get_semantic_map_new(jd)[:300], f'semantic map {sample_id}')\n",
    "print(len(docs_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb9529-2b78-47dd-a886-cf854dde0dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd23d",
   "metadata": {
    "papermill": {
     "duration": 0.38132,
     "end_time": "2021-08-03T13:00:15.130505",
     "exception": false,
     "start_time": "2021-08-03T13:00:14.749185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, oid in enumerate(docs_ids):\n",
    "    d = get_doc_by_id(oid)\n",
    "    jd = DbJsonDoc(d)\n",
    "    \n",
    "    try:\n",
    "        add_stats_record(jd, stats)\n",
    "    except Exception as e:\n",
    "        print(jd.get_id())\n",
    "        raise (e)\n",
    "    \n",
    "    if k % 20 == 0:\n",
    "        print(f'{k} of {len(docs_ids)}')\n",
    "        stats.to_csv(export_fn, index=True)\n",
    "        print(f'stats saved to {export_fn}')\n",
    "        \n",
    "stats.to_csv(export_fn, index=True)\n",
    "print(f'stats saved to {export_fn}')\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f0881-7ab9-4655-b37b-f69e24891757",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats[stats.valid!=False]\n",
    "len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60b51d",
   "metadata": {
    "papermill": {
     "duration": 0.025526,
     "end_time": "2021-08-03T13:00:15.166217",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.140691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2 = stats.copy()\n",
    "stats2['org-2-alias'] = stats2['org-2-alias'].str.lower()\n",
    "stats2['org-1-alias'] = stats2['org-1-alias'].str.lower()\n",
    "stats2['org-1-name'] = stats2['org-1-name'].str.lower()\n",
    "stats2['org-2-name'] = stats2['org-2-name'].str.lower()\n",
    "stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05733268-ba37-4545-aa59-e45c7f11ff6c",
   "metadata": {
    "papermill": {
     "duration": 0.01442,
     "end_time": "2021-08-03T13:00:15.190797",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.176377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891e080",
   "metadata": {
    "papermill": {
     "duration": 0.013939,
     "end_time": "2021-08-03T13:00:15.214522",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.200583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['org-1-alias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26686529",
   "metadata": {
    "papermill": {
     "duration": 0.0139,
     "end_time": "2021-08-03T13:00:15.238324",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.224424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['org-2-alias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ae79a",
   "metadata": {
    "papermill": {
     "duration": 0.724846,
     "end_time": "2021-08-03T13:00:15.973278",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.248432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_column_name = \"score\"\n",
    "\n",
    "def calc_no_value_importance(stats2, column_name, score_k=2.):\n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        if pd.isna(row[column_name]):\n",
    "            stats2.at[i, score_column_name] *= score_k\n",
    "                \n",
    "def calc_user_importance(stats2):\n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        if not pd.isna(row['unseen']):\n",
    "\n",
    "            if row['unseen'] == True:\n",
    "                stats2.at[i, score_column_name] *= 2\n",
    "                \n",
    "def calc_val_importance(stats2):\n",
    " \n",
    "    for i, row in stats2.iterrows():\n",
    "        if not pd.isna(row['value']):\n",
    "            val = row['value']\n",
    "            if val >= 2000:\n",
    "                stats2.at[i, score_column_name] *= 1./log1p(val)\n",
    "            if val < 2000:\n",
    "                stats2.at[i, score_column_name] *= 4.\n",
    "        else:\n",
    "            stats2.at[i, score_column_name] *= 10.0 #error\n",
    "            \n",
    "def calc_column_val_importance(stats2, column_name, k=1.):\n",
    "    counts = stats2[column_name].value_counts()\n",
    " \n",
    "        \n",
    "    for i, row in stats2.iterrows():\n",
    "        val = row[column_name]\n",
    "        if val in counts:\n",
    "            count = counts[val]\n",
    "            stats2.at[i, score_column_name] *= k * log1p(count)\n",
    "#             print(count, log1p(count),  1./log1p(count))\n",
    "        else:\n",
    "            stats2.at[i, score_column_name] *= 10.0 #error\n",
    "\n",
    "            \n",
    "for i, row in stats2.iterrows():        \n",
    "  stats2.at[i, 'url'] = f'http://gpn-audit.nemosoft.ru/#/audit/edit/{i}'\n",
    "\n",
    "    \n",
    "stats2 ['unseen'] = pd.isna( stats2['user_correction_date'])\n",
    "\n",
    "stats2[score_column_name] = 1.\n",
    " \n",
    "\n",
    "calc_no_value_importance(stats2, 'org-1-alias', 2.)\n",
    "calc_no_value_importance(stats2, 'org-2-alias', 2.)\n",
    "calc_no_value_importance(stats2, 'org-1-name', 3.)\n",
    "calc_no_value_importance(stats2, 'org-2-name', 3.)\n",
    "calc_no_value_importance(stats2, 'user_correction_date', 10.)\n",
    "calc_no_value_importance(stats2, 'subject', 4.)\n",
    "calc_no_value_importance(stats2, 'value', 5.)\n",
    "\n",
    "## calc_column_val_importance(stats2, 'org-1-name')\n",
    "## calc_column_val_importance(stats2, 'org-2-name')\n",
    "\n",
    "calc_column_val_importance(stats2, 'org-1-alias')\n",
    "calc_column_val_importance(stats2, 'org-2-alias')\n",
    "calc_column_val_importance(stats2, 'subject', k = 3.)\n",
    "\n",
    "calc_user_importance(stats2)\n",
    "calc_val_importance(stats2)\n",
    "\n",
    "# drop_duplicates(subset=['checksum']).\n",
    "stats2 = stats2.sort_values([score_column_name], ascending=True)\n",
    "print(len(stats2))\n",
    "stats2.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ec184-15b0-4808-abc6-b3d7e729adc0",
   "metadata": {
    "papermill": {
     "duration": 0.013603,
     "end_time": "2021-08-03T13:00:15.997961",
     "exception": false,
     "start_time": "2021-08-03T13:00:15.984358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = stats2.sort_values(['score'], ascending=True).copy()\n",
    "# sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8cc3c-1899-4eb0-beab-967ed8148316",
   "metadata": {
    "papermill": {
     "duration": 0.011337,
     "end_time": "2021-08-03T13:00:16.020379",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.009042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Find and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec7282-caa4-4eee-8289-6961e0e24eb1",
   "metadata": {
    "papermill": {
     "duration": 0.0276,
     "end_time": "2021-08-03T13:00:16.059913",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.032313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats2['len'] //= 10 #similar lens\n",
    "stats2['len'] *= 10 #similar lens\n",
    "stats2.sort_values(['len', 'org-1-name', 'org-2-name', 'org-1-alias','org-2-alias', 'value', 'subject'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5701fce-1bf6-4b15-86dc-8aee903e160d",
   "metadata": {
    "papermill": {
     "duration": 0.093955,
     "end_time": "2021-08-03T13:00:16.165294",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.071339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, row in stats2.iterrows():\n",
    "  _str = ':'.join(sorted([str(x) for x in\n",
    "                          [row['len'], row['org-1-alias'], row['org-2-alias'], row['org-1-name'], row['org-2-name'],\n",
    "                           row['value'], row['subject']]]))\n",
    "  stats2.at[i, 'hash'] = hash(_str)\n",
    "\n",
    "stats2 = stats2.sort_values(['hash'])\n",
    "print(len(stats2))\n",
    "stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23073dff-1fbc-4bef-aeae-cdd9fb3dfe0d",
   "metadata": {
    "papermill": {
     "duration": 0.076868,
     "end_time": "2021-08-03T13:00:16.253656",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.176788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unks = stats2['hash'].value_counts()\n",
    "\n",
    "for i, row in stats2.iterrows():   \n",
    "    stats2.at[i, 'dups'] = unks[row['hash']]\n",
    "    \n",
    "dups = stats2.sort_values(['dups', 'hash'], ascending=False)\n",
    "# dups.to_csv(Path(work_dir) / 'contract_trainset_meta.duplicates.csv', index=True)\n",
    "\n",
    "# dups[40:].head(40)\n",
    "len(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf72526-1ad7-4699-91ac-abaeff5c23f7",
   "metadata": {
    "papermill": {
     "duration": 0.015314,
     "end_time": "2021-08-03T13:00:16.280531",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.265217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02726c5-9781-43a6-8b5d-5c18f108be97",
   "metadata": {
    "papermill": {
     "duration": 0.011486,
     "end_time": "2021-08-03T13:00:16.303604",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.292118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b8c5e-1123-4bdb-a493-e4687e28a7ac",
   "metadata": {
    "papermill": {
     "duration": 0.019318,
     "end_time": "2021-08-03T13:00:16.335109",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.315791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s = dups.sort_values([score_column_name], ascending=True).copy()\n",
    "\n",
    "dups_s = dups_s.drop_duplicates(subset=['hash'])\n",
    "dups_s = dups_s.sort_values([score_column_name], ascending=True)\n",
    "\n",
    "\n",
    "dups_s = dups_s.sort_values([score_column_name], ascending=True)\n",
    "# dups_s = dups_s[dups_s[score_column_name] < 5000]\n",
    "print(len(dups_s))\n",
    "\n",
    "# dups_s [dups_s['len'] > 30000].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fcb98-2de6-4518-8746-fd436042a6fa",
   "metadata": {
    "papermill": {
     "duration": 0.011468,
     "end_time": "2021-08-03T13:00:16.358146",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.346678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove rare subjects and very long docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a1a76-b672-46e9-abf9-319b52f77c46",
   "metadata": {
    "papermill": {
     "duration": 0.060113,
     "end_time": "2021-08-03T13:00:16.429594",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.369481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "ax = dups_s[dups_s['len'] < 30000] ['len'].plot.hist(bins=20, alpha=0.8)\n",
    "ax.set_title('Text Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294dfac-cdc5-4167-a64d-9216f2ead251",
   "metadata": {
    "papermill": {
     "duration": 0.038499,
     "end_time": "2021-08-03T13:00:16.479798",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.441299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dups_s[score_column_name].min())\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = dups_s[score_column_name].plot.hist(bins=28, alpha=0.8)\n",
    "ax.set_title('Distibution by score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087ee9d-297f-4d28-8fe2-c460bed597ed",
   "metadata": {
    "papermill": {
     "duration": 0.050659,
     "end_time": "2021-08-03T13:00:16.542195",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.491536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dups_s['valid'] = True\n",
    "\n",
    "print(\"excluding too long docs\")\n",
    "for i, row in dups_s.iterrows():   \n",
    "    if row['len'] > 30000:\n",
    "        print ('excluding', row[0], row['subject'], row['len'])\n",
    "        dups_s.at[i, 'valid'] = False  \n",
    "        \n",
    "dups_s = dups_s[dups_s['valid'] == True]\n",
    "\n",
    "print(\"excluding docs with rare subj\")\n",
    "cnts = dups_s ['subject'].value_counts()\n",
    "for i, row in dups_s.iterrows():   \n",
    "    if not pd.isna(row['subject']):\n",
    "        if cnts[row['subject']] < 4:\n",
    "            print ('excluding', row[0], row['subject'])\n",
    "            dups_s.at[i, 'valid'] = False\n",
    "\n",
    "dups_s = dups_s[dups_s['valid'] == True]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f3f82-a6bc-4644-9623-d74e84ae94b3",
   "metadata": {
    "papermill": {
     "duration": 0.087943,
     "end_time": "2021-08-03T13:00:16.642370",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.554427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "cnt = dups_s['subject'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6 ))\n",
    "sns.barplot(x=cnt.values, y=cnt.index)\n",
    "\n",
    "print(  cnt )\n",
    "\n",
    "plt.title('Frequency Distribution of subjects')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedaa50-a9aa-4eb6-b238-be22cb52718b",
   "metadata": {
    "papermill": {
     "duration": 0.033856,
     "end_time": "2021-08-03T13:00:16.688321",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.654465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s.index.name = '_id'\n",
    "dups_s.to_csv(Path(work_dir) / 'contract_trainset_meta.csv', index=True)\n",
    "\n",
    "\n",
    "_s = f\"### {len(dups_s)} - Всего новых документов в обучающем наборе на {lastdate}\"\n",
    "display(Markdown(_s))\n",
    "\n",
    "\n",
    "dups_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be4347-cc76-4885-9691-e5edf14f1c55",
   "metadata": {
    "papermill": {
     "duration": 0.021286,
     "end_time": "2021-08-03T13:00:16.722326",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.701040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s[dups_s['unseen']].to_csv(Path(work_dir) / 'contract_trainset_meta.to_markup.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d76c06-1f5b-4626-95b8-216a550c8989",
   "metadata": {
    "papermill": {
     "duration": 0.025695,
     "end_time": "2021-08-03T13:00:16.761300",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.735605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dups_s[dups_s['unseen']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0857d1-e99a-4ac3-acce-2325f9c99608",
   "metadata": {
    "papermill": {
     "duration": 0.012798,
     "end_time": "2021-08-03T13:00:16.787093",
     "exception": false,
     "start_time": "2021-08-03T13:00:16.774295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944db2d-ebf9-4446-8e69-5c7e13f76966",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f51800-6a9c-42a1-9811-85a12005b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyser.headers_detector import get_tokens_features\n",
    "from tf_support.embedder_elmo import ElmoEmbedder\n",
    "from analyser.legal_docs import embedd_tokens\n",
    "import os\n",
    "\n",
    "embedder = ElmoEmbedder.get_instance('elmo')  # lazy init\n",
    "\n",
    "def _dp_fn(doc_id, suffix):\n",
    "    return os.path.join(work_dir, 'datasets', f'{doc_id}-datapoint-{suffix}.npy')\n",
    "\n",
    "\n",
    "def save_contract_data_arrays(db_json_doc: DbJsonDoc):\n",
    "    # TODO: trim long documens according to contract parser\n",
    "\n",
    "    id_ = db_json_doc.get_id()\n",
    "\n",
    "    tokens_map: TextMap = db_json_doc.get_tokens_for_embedding()\n",
    "\n",
    "    # 1) EMBEDDINGS\n",
    "    print(len(tokens_map))\n",
    "    embeddings = embedd_tokens(tokens_map,\n",
    "                               embedder,\n",
    "                               log_key=f'id={id_} chs={tokens_map.get_checksum()}')\n",
    "\n",
    "    # 2) TOKEN FEATURES\n",
    "    token_features: DataFrame = get_tokens_features(db_json_doc.get_tokens_map_unchaged().tokens)\n",
    "\n",
    "    # 3) SEMANTIC MAP\n",
    "    semantic_map: DataFrame = get_semantic_map_new(db_json_doc)\n",
    "    #####\n",
    " \n",
    "    np.save(_dp_fn(id_, 'token_features'), token_features)\n",
    "    np.save(_dp_fn(id_, 'semantic_map'), semantic_map)\n",
    "    _embeddings_file = _dp_fn(id_, 'embeddings')\n",
    "    np.save(_embeddings_file, embeddings)\n",
    "    print(f'embeddings saved to {_embeddings_file} {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a53658-72c2-495d-827e-dcdd74bdee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "_docs_ids=[str(i) for i in new_docs_ids]\n",
    "intersection_set = set.intersection(set(list(dups_s.index)), set(_docs_ids)) \n",
    "len(intersection_set)\n",
    "# intersection_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c72fc4-9745-41ec-8ab8-aa1eb27fa947",
   "metadata": {},
   "outputs": [],
   "source": [
    "oid = '5edbadd9da3678279fbcab15'\n",
    "dups_s.loc[oid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67f126-fbe4-442b-bf2d-93bfac5030b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a98e4-4b79-4472-b370-065118afc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(dups_s.index)\n",
    "\n",
    "for k, oid in enumerate(ids):\n",
    "    iiid= ObjectId(oid)\n",
    "    src = dups_s.loc[oid]['source']\n",
    "    jd = None\n",
    "    if src == 'db':\n",
    "#       if False:\n",
    "        d = get_doc_by_id(iiid)\n",
    "        jd = DbJsonDoc(d)\n",
    "    else:\n",
    "#         if False:\n",
    "        print(oid, k, 'of', len(ids), src)\n",
    "        jd = files_dict[iiid]\n",
    "        print(oid, k, 'of', len(ids), src)\n",
    "\n",
    "    if jd is not None:\n",
    "        save_contract_data_arrays(jd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba97f59-b646-4b80-89ec-5ca6ae7024da",
   "metadata": {},
   "outputs": [],
   "source": [
    "25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aefe53-1876-4d8b-b239-230133e634a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.184245,
   "end_time": "2021-08-03T13:00:20.283835",
   "environment_variables": {},
   "exception": null,
   "input_path": "trainsets/export_trainset.ipynb",
   "output_path": "trainsets/export_trainset.ipynb",
   "parameters": {},
   "start_time": "2021-08-03T13:00:10.099590",
   "version": "2.3.3"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
