{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c31f5",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoHJkn9yQIgg",
    "papermill": {
     "duration": 0.020788,
     "end_time": "2023-01-26T10:09:21.502764",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.481976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('retrain_ipynb')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.debug('--=logging started=--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb704b",
   "metadata": {
    "papermill": {
     "duration": 0.006423,
     "end_time": "2023-01-26T10:09:21.516481",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.510058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d003e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Dehw0fnKfSBF",
    "outputId": "3bcb61cd-401a-43d7-dccf-9a6022b2a576",
    "papermill": {
     "duration": 0.010629,
     "end_time": "2023-01-26T10:09:21.533624",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.522995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "TRAIN = True\n",
    "TRAIN_MODEL_3 = False\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print ('Running in colab:', IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cf43f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBqnGnQfKWF",
    "papermill": {
     "duration": 0.010891,
     "end_time": "2023-01-26T10:09:21.551366",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.540475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not IN_COLAB:\n",
    "  nb_dir = os.path.split(os.getcwd())[0]\n",
    "  if nb_dir not in sys.path:\n",
    "      sys.path.append(nb_dir)\n",
    "else:\n",
    "  %tensorflow_version 1.x\n",
    "  import tensorflow as tf\n",
    "  print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae225f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5cLtueagMru",
    "papermill": {
     "duration": 0.011499,
     "end_time": "2023-01-26T10:09:21.569535",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.558036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install --upgrade pip\n",
    "  !pip --version\n",
    "  !pip install --no-deps --upgrade git+https://www.github.com/nemoware/analyser.git@uber-models\n",
    "  !pip install -q pyjarowinkler overrides\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836a55c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT7451NXspdw",
    "papermill": {
     "duration": 0.01449,
     "end_time": "2023-01-26T10:09:21.616569",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.602079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import analyser.hyperparams\n",
    "\n",
    "training_reports_path = Path(analyser.hyperparams.__file__).parent.parent / 'training_reports/'\n",
    "\n",
    "if not IN_COLAB:\n",
    "    _work_dir_default = os.path.realpath(os.path.join(  analyser.hyperparams.__file__, '..', '..', '..', 'work'))\n",
    "    work_dir = os.environ.get('GPN_WORK_DIR', _work_dir_default)\n",
    "\n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "\n",
    "    analyser.hyperparams.work_dir = work_dir\n",
    "else:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.environ['GPN_WORK_DIR']='/content/drive/My Drive/GazpromOil/trainsets/uber_2'\n",
    "    analyser.hyperparams.work_dir = os.environ['GPN_WORK_DIR']\n",
    "\n",
    "print('work_dir=', analyser.hyperparams.work_dir)\n",
    "\n",
    "\n",
    "assert os.path.isdir(analyser.hyperparams.work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcf406",
   "metadata": {
    "colab_type": "text",
    "id": "JbsxFAqC6pjQ",
    "papermill": {
     "duration": 0.006596,
     "end_time": "2023-01-26T10:09:21.643082",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.636486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pexvDJbAtZM0",
    "outputId": "4f7e6e34-d675-423d-d102-1020d49d854f",
    "papermill": {
     "duration": 5.563081,
     "end_time": "2023-01-26T10:09:27.212769",
     "exception": false,
     "start_time": "2023-01-26T10:09:21.649688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from colab_support.renderer import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bson import json_util\n",
    "\n",
    "from analyser.legal_docs import LegalDocument, make_headline_attention_vector\n",
    "from analyser.headers_detector import make_predicted_headline_attention_vector, get_tokens_features\n",
    "from analyser.hyperparams import models_path\n",
    "from analyser.text_tools import find_top_spans\n",
    "from analyser.persistence import DbJsonDoc\n",
    "\n",
    "from trainsets.trainset_tools import TrainsetBalancer, SubjectTrainsetManager\n",
    "from trainsets.retrain_contract_uber_model import UberModelTrainsetManager\n",
    "\n",
    "from tf_support import super_contract_model\n",
    "from tf_support.super_contract_model import get_base_model, semantic_map_keys_contract\n",
    "from tf_support.super_contract_model import uber_detection_model_005_1_1\n",
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "from tf_support.tools import KerasTrainingContext\n",
    "# from tf_support.embedder_elmo import ElmoEmbedder\n",
    "\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv1D, LSTM, GRU, BatchNormalization, TimeDistributed, Dense, Bidirectional, Input, Dropout, Lambda\n",
    "from keras.layers import concatenate, SpatialDropout1D, ActivityRegularization\n",
    "from keras.layers import MaxPooling1D, Activation, ThresholdedReLU, GaussianNoise\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "# from keras.utils import plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020024c",
   "metadata": {
    "papermill": {
     "duration": 0.011125,
     "end_time": "2023-01-26T10:09:27.231055",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.219930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('training_reports_path=', training_reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd532a3d",
   "metadata": {
    "papermill": {
     "duration": 0.644355,
     "end_time": "2023-01-26T10:09:27.882346",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.237991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# for device in gpu_devices:\n",
    "#     print(device)\n",
    "#     tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83a908",
   "metadata": {
    "colab_type": "text",
    "id": "HRQOy7o0uyTv",
    "papermill": {
     "duration": 0.006884,
     "end_time": "2023-01-26T10:09:27.896593",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.889709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799262bb",
   "metadata": {
    "papermill": {
     "duration": 0.010796,
     "end_time": "2023-01-26T10:09:27.928015",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.917219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "  from integration.db import get_mongodb_connection\n",
    "  from bson.objectid import ObjectId\n",
    "\n",
    "  def get_doc(objid):\n",
    "    db = get_mongodb_connection()\n",
    "    documents_collection = db['documents']\n",
    "    jdata = documents_collection.find_one({'_id': ObjectId(objid)})\n",
    "    return DbJsonDoc(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d96088",
   "metadata": {
    "papermill": {
     "duration": 0.011207,
     "end_time": "2023-01-26T10:09:27.946296",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.935089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    jdoc = get_doc('5df1f44dec85d861954efc47')\n",
    "    doc = jdoc.asLegalDoc()\n",
    "    doc.tokens\n",
    "    \n",
    "\n",
    "work_dir = Path(analyser.hyperparams.work_dir)\n",
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aa970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = stats.loc[stats['user_correction_date'].notnull()]\n",
    "# xx = xx[ (xx.version=='3.0.0') | (xx.version=='22.12.7')].sort_values(['score'])\n",
    "# xx.to_csv('contracts_to_re-analyze.csv')\n",
    "# xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78386e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = stats.loc[stats['user_correction_date'].notnull()]\n",
    "# xx['user_correction_date'] = pd.to_datetime(xx['user_correction_date'], utc=True)\n",
    "\n",
    "\n",
    "# xx['user_correction_date'] = xx['user_correction_date'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "# xx.to_datetime(df_res['DateTime'], utc=True)\n",
    "\n",
    "# xx['user_correction_date'] = xx['user_correction_date'].dt.tz_localize(None)\n",
    "\n",
    "# xx.sort_values(['user_correction_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy = xx.sort_values(['user_correction_date'])[xx.version!='22.12.7'][xx.source=='db'].sort_values(['score'])\n",
    "# yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262604f",
   "metadata": {
    "papermill": {
     "duration": 0.006939,
     "end_time": "2023-01-26T10:09:27.960342",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.953403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stats['user_correction_date'] = stats['user_correction_date'].dt.tz_localize(None)\n",
    "# stats.sort_values(['user_correction_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy.to_csv('contracts_to_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8c160",
   "metadata": {
    "papermill": {
     "duration": 1.744057,
     "end_time": "2023-01-26T10:09:29.711406",
     "exception": false,
     "start_time": "2023-01-26T10:09:27.967349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "umtm = UberModelTrainsetManager (work_dir, reports_dir=training_reports_path)\n",
    "\n",
    "umtm.load_contract_trainset_meta() # 'contract_trainset_meta.csv'\n",
    "stats = umtm.stats\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats[ ['org-1-alias', 'org-2-alias'] ]\n",
    "\n",
    "user_dataset = stats[ stats['unseen']==False]\n",
    "len(user_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78d32f",
   "metadata": {
    "papermill": {
     "duration": 0.011549,
     "end_time": "2023-01-26T10:09:29.730569",
     "exception": false,
     "start_time": "2023-01-26T10:09:29.719020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d9c96",
   "metadata": {
    "papermill": {
     "duration": 0.011182,
     "end_time": "2023-01-26T10:09:29.761289",
     "exception": false,
     "start_time": "2023-01-26T10:09:29.750107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_dataset[user_dataset.subj_len>=150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db3a05",
   "metadata": {
    "papermill": {
     "duration": 0.007218,
     "end_time": "2023-01-26T10:09:29.776017",
     "exception": false,
     "start_time": "2023-01-26T10:09:29.768799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Weights: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ samples  \n",
    "\n",
    "—á—Ç–æ–±—ã –≤—Å–µ—Ö –∑–∞–ø—É—Ç–∞—Ç—å, \n",
    " - –≤–µ—Å –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω –ª–æ–≥–æ—Ä–∏—Ñ–º—É —Ü–µ–Ω—ã –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (—á—Ç–æ–±—ã –±—ã–ª–æ –º–µ–Ω—å—à–µ –æ—à–∏–±–æ–∫ –≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö –Ω–∞ –±–æ–ª—å—à–∏–µ —Å—É–º–º—ã)\n",
    " - more weight for user-corrected datapoints\n",
    " - normalize weights, so the sum == Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_file = Path(analyser.__file__).parent.parent /'trainsets' / 'errors.csv'\n",
    "errors_file\n",
    "errors_df = pd.read_csv(errors_file, index_col=0)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3f2c8",
   "metadata": {
    "papermill": {
     "duration": 0.007249,
     "end_time": "2023-01-26T10:09:29.790575",
     "exception": false,
     "start_time": "2023-01-26T10:09:29.783326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainsets.trainset_tools import get_feature_log_weights\n",
    "stats = umtm.stats\n",
    "stats = stats[stats.documentType != 'ANNEX']\n",
    "stats = stats[stats.documentType != 'undefined']\n",
    "\n",
    "print(len(stats))\n",
    "get_feature_log_weights(stats, 'documentType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trainsets.trainset_tools import get_feature_log_weights\n",
    "# _w=get_feature_log_weights(umtm.stats, 'subject')\n",
    "# _w*_w*_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f097425",
   "metadata": {
    "papermill": {
     "duration": 0.565472,
     "end_time": "2023-01-26T10:09:30.363404",
     "exception": false,
     "start_time": "2023-01-26T10:09:29.797932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "subject_weights = get_feature_log_weights(stats, 'subject')\n",
    "subject_weights = subject_weights * subject_weights * subject_weights\n",
    "\n",
    "for i, row in stats.iterrows():\n",
    "  subj_name = row['subject']\n",
    "\n",
    "  #error weight\n",
    "  error_weight = 1.0 \n",
    "  if i in errors_df.index:\n",
    "      error_weight = 1.0 + errors_df.at[i, 'errors count']\n",
    "\n",
    "\n",
    "  sample_weight = 0.5 \n",
    "  if not pd.isna(row['user_correction_date']):  # more weight for user-corrected datapoints\n",
    "    sample_weight = 5.0   # TODO: must be estimated anyhow smartly    \n",
    "\n",
    "  value_weight = 1.0\n",
    "  if not pd.isna(row['value_log1p']):\n",
    "    # —á—Ç–æ–±—ã –≤—Å–µ—Ö –∑–∞–ø—É—Ç–∞—Ç—å, –≤–µ—Å –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω –ª–æ–≥–æ—Ä–∏—Ñ–º—É —Ü–µ–Ω—ã –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞\n",
    "    # (—á—Ç–æ–±—ã –±—ã–ª–æ –º–µ–Ω—å—à–µ –æ—à–∏–±–æ–∫ –≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö –Ω–∞ –±–æ–ª—å—à–∏–µ —Å—É–º–º—ã)\n",
    "    value_weight = 1.0 + row['value_log1p']\n",
    "\n",
    "  sample_weight *=  value_weight \n",
    "  subject_weight =  subject_weights[subj_name] \n",
    "    \n",
    "  sample_weight /= error_weight  \n",
    "\n",
    "  stats.at[i, 'subject_weight'] = subject_weight + random.random()\n",
    "  stats.at[i, 'sample_weight'] = sample_weight + random.random()\n",
    "\n",
    "# normalize weights, so the sum == Number of samples\n",
    "# stats.sample_weight /= stats.sample_weight.mean()\n",
    "# stats.subject_weight /= stats.subject_weight.mean()\n",
    "\n",
    "print(stats.sample_weight.mean())\n",
    "print(stats.subject_weight.mean())\n",
    "print(stats.sample_weight.min())\n",
    "print(stats.subject_weight.min())\n",
    "print(stats.sample_weight.max())\n",
    "print(stats.subject_weight.max())\n",
    "\n",
    "stats.to_csv( work_dir / 'contract_trainset_meta.csv', index=True)\n",
    "\n",
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761ad08",
   "metadata": {
    "papermill": {
     "duration": 0.011512,
     "end_time": "2023-01-26T10:09:30.382599",
     "exception": false,
     "start_time": "2023-01-26T10:09:30.371087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import make_xyw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891fe44",
   "metadata": {
    "papermill": {
     "duration": 0.007515,
     "end_time": "2023-01-26T10:09:30.397876",
     "exception": false,
     "start_time": "2023-01-26T10:09:30.390361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Validating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed33bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "8pe_gZIK3JFh",
    "outputId": "7b923b06-f592-4b11-bb35-d215566e017d",
    "papermill": {
     "duration": 9.602735,
     "end_time": "2023-01-26T10:09:40.008113",
     "exception": false,
     "start_time": "2023-01-26T10:09:30.405378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "\n",
    "from tf_support.super_contract_model import  validate_datapoint\n",
    "\n",
    "stats['valid'] = True\n",
    "stats['error'] = ''\n",
    "\n",
    "for i in stats.index:\n",
    "  \n",
    "  try:\n",
    "    validate_datapoint(str(i), stats)\n",
    "\n",
    "  except Exception as e:\n",
    "    logger.error(e)\n",
    "\n",
    "    stats.at[i, 'valid'] = False\n",
    "    stats.at[i, 'error'] = str(e)\n",
    "    \n",
    "stats\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "umtm.stats = umtm.stats[  pd.isna(umtm.stats.value_span) + (umtm.stats.value_span < 10000) ] #remove big docs from TS\n",
    "stats_valid = stats[stats['valid']]\n",
    "\n",
    "del stats\n",
    "print(len(stats_valid))\n",
    "stats = stats_valid\n",
    "umtm.stats = stats_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbde79f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "len(stats_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d5a7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(stats.sample_weight.mean())\n",
    "print(stats.subject_weight.mean())\n",
    "\n",
    "stats.sample_weight /= stats.sample_weight.mean()\n",
    "stats.subject_weight /= stats.subject_weight.mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73865c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.subject_weight.mean())\n",
    "print(stats.subject_weight.min())\n",
    "\n",
    "print('\\n\\nsample_weight')\n",
    "print('MIN\\t', stats.sample_weight.min())\n",
    "print('MAX\\t', stats.sample_weight.max())\n",
    "print('MEAN\\t', stats.sample_weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b0325",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "ouUjO_T7xf8A",
    "outputId": "17f85a9a-b0c1-4395-886f-af66e6e5fb12",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "subj_count = stats_valid['subject'].value_counts()\n",
    "\n",
    "#plot distribution---------------------\n",
    "sns.barplot(subj_count.values, subj_count.index)\n",
    "plt.title('Frequency Distribution of subjects')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print ('\\nmin', min (subj_count.values))\n",
    "print ('max', max (subj_count.values))\n",
    "print ('total', sum (subj_count.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac952e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "Ld_KZaHwqf_G",
    "outputId": "c974398a-62a1-44e8-b73c-6589766b05c7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from trainsets.trainset_tools import get_feature_log_weights\n",
    "\n",
    "_classes = stats_valid['subject'].unique().tolist()\n",
    "\n",
    "print(f'classes: {_classes}')\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', _classes, umtm.stats['subject'])\n",
    "# class_weights = dict(zip(_classes, class_weights))\n",
    "\n",
    "\n",
    "class_weights = get_feature_log_weights(stats_valid, 'subject')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea83f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "colab_type": "code",
    "id": "Cs6cZtPy6Je1",
    "outputId": "f0a39fff-f3c5-4418-b087-0eef915b7a81",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainsets.trainset_tools import get_feature_log_weights\n",
    " \n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "stats_valid['subject_weight'].hist(bins=40, alpha=0.5)\n",
    "stats_valid['sample_weight'].hist(bins=40, alpha=0.5)\n",
    "\n",
    "plt.xscale('linear') # log?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29d2c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "_DjNv8UQ956S",
    "outputId": "6ded6bae-e1f0-4a77-9024-64da80c6e47f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"subject_weight\", y=\"sample_weight\", data=stats_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2ee1c",
   "metadata": {
    "colab_type": "text",
    "id": "IxEdSGOuq62R",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# look into trainset (take a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03af92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CRb_AJUliUft",
    "outputId": "0369b8f3-7423-45f3-a0ed-3459408ccec0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# umtm.calculate_samples_weights()\n",
    "SAMPLE_DOC_ID = '5fe34f62b770574a005553be' #stats_valid.index[1]\n",
    "\n",
    "print('SAMPLE_DOC_ID', SAMPLE_DOC_ID)\n",
    "(emb, tok_f), (sm, subj), (sample_weight, subject_weight) = make_xyw(SAMPLE_DOC_ID, stats)\n",
    " \n",
    "    \n",
    "print('semantic map shape is:', sm.shape)\n",
    "_crop = 500\n",
    "plot_embedding(tok_f[:_crop], title=f'Tokens features {SAMPLE_DOC_ID}') \n",
    "plot_embedding(emb[:_crop], title=f'Embedding {SAMPLE_DOC_ID}') \n",
    "plot_embedding(sm[:_crop], title=f'Semantic map {SAMPLE_DOC_ID}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonzerozz = np.where(sm > 0)[0]\n",
    "# nonzerozz = list(set(nonzerozz))\n",
    "# nonzerozz\n",
    "\n",
    "# c=random.choice(nonzerozz)\n",
    "# sm[c-1:c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sm[1].argmax()\n",
    "# for i in range(0,10):\n",
    "#     print(random.random() < 0.8)\n",
    "# #     random_row = random.randint(0, sm.shape[-1])\n",
    "# #     focus_pos = sm[random_row].argmax()\n",
    "# #     print(random_row, focus_pos, sm[random_row][focus_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02564c32",
   "metadata": {
    "colab_type": "text",
    "id": "niaa4g6g2Q7g",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Batch generator & TODOs üôè\n",
    "\n",
    "\n",
    "- [X] TODO: add outliers to the trainset ?\n",
    "- [ ] TODO: try sparse_categorical_entropy instead of one-hot encodings\n",
    "- [ ] TODO: model 5.2, 5.1: bipolar concat layer is wrong because we concatenate thongs of different magnitudes. Add a Sigmoid activation layer\n",
    "- [ ] TODO: chechk what is better: to pad with zeros or to pad with means\n",
    "- [X] TODO: add weights to samples\n",
    "- [ ] TODO: sum semantic map alongside vertical axis, and mutiply it (as a mask) by the subject detection seq\n",
    "- [ ] TODO: introduce individual per tag threshosholds, also, the current 0.3 threshold is strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2072a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41186a83",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CFzuOP4w9mB",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def make_generator(self, indices: [int], batch_size: int, augment_samples=False):\n",
    "  #   np.random.seed(43)\n",
    "\n",
    "  while True:\n",
    "    # next batch\n",
    "    batch_indices = np.random.choice(a=indices, size=batch_size)\n",
    "\n",
    "    max_len = 1536\n",
    "    start_from = 0\n",
    "\n",
    "    if augment_samples:\n",
    "      max_len = random.randint(300, max_len + 100)\n",
    "\n",
    "    batch_input_emb = []\n",
    "    batch_input_token_f = []\n",
    "    batch_output_sm = []\n",
    "    batch_output_subj = []\n",
    "\n",
    "    weights = []\n",
    "    weights_subj = []\n",
    "\n",
    "    # Read in each input, perform preprocessing and get labels\n",
    "    for doc_id in batch_indices:\n",
    "\n",
    "      dp = make_xyw(doc_id, stats)\n",
    "      (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "      #       print(dp)\n",
    "\n",
    "      subject_weight_K = 1.0\n",
    "      if augment_samples:\n",
    "        start_from = 0\n",
    "\n",
    "        # row = stats_valid.loc[doc_id]\n",
    "        if random.random() < 0.6:  # 60% of samples\n",
    "          nonzerozz = np.where(sm > 0)[0]\n",
    "#           nonzerozz = nonzerozz\n",
    "          \n",
    "          segment_center = random.choice(nonzerozz)\n",
    "          if len(nonzerozz)==0:\n",
    "             segment_center=0\n",
    "\n",
    "\n",
    "          # segment_center = random.randint(0, len(emb) - 1)  ##select random token as a center\n",
    "\n",
    "          # if not pd.isna(row['value_span']) and random.random() < 0.7:  ##select value token as a center\n",
    "          #   segment_center = int(row['value_span'])\n",
    "\n",
    "          # _off = random.randint(max_len // 4, max_len // 2)\n",
    "          _off = random.randint(-100, 100)\n",
    "          start_from = segment_center - _off\n",
    "          if start_from < 0:\n",
    "            start_from = 0\n",
    "          if start_from >=len(emb):\n",
    "            start_from = len(emb)-1\n",
    "#           print('start_from', start_from)\n",
    "#           if random_row != 1:#subject row, see semantic_map_keys\n",
    "#               subject_weight_K = 0.1  # lower subject weight because there migh–µ be no information about subject around doc. value\n",
    "\n",
    "      dp = self.trim_maxlen(dp, start_from, max_len)\n",
    "\n",
    "      # TODO: find samples maxlen\n",
    "\n",
    "      (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "      #       print((sample_weight, subject_weight))\n",
    "      subject_weight *= subject_weight_K\n",
    "\n",
    "      batch_input_emb.append(emb)\n",
    "      batch_input_token_f.append(tok_f)\n",
    "\n",
    "      batch_output_sm.append(sm)\n",
    "      batch_output_subj.append(subj)\n",
    "\n",
    "      if np.isnan(sample_weight):\n",
    "        raise ValueError()\n",
    "\n",
    "      if np.isnan(subject_weight):\n",
    "        raise ValueError()\n",
    "\n",
    "      weights.append(sample_weight)\n",
    "      weights_subj.append(subject_weight)\n",
    "      # end if emb\n",
    "    # end for loop\n",
    "\n",
    "    # Returns a tuple of (input, output, weights) to feed the network\n",
    "    #     print('batch_output_subj', len(batch_output_subj))\n",
    "    #     print('batch_output_sm', len(batch_output_subj))\n",
    "\n",
    "    yield ([np.array(batch_input_emb), np.array(batch_input_token_f)],\n",
    "           [np.array(batch_output_sm), np.array(batch_output_subj)],\n",
    "           [np.array(weights), np.array(weights_subj)])\n",
    "    \n",
    "\n",
    "    \n",
    "_train, _test = train_test_split(stats_valid, test_size=0.10, stratify=stats_valid[['subject']], random_state=11)\n",
    "\n",
    "train_indices = list(_train.index)\n",
    "test_indices = list(_test.index)\n",
    "\n",
    "    \n",
    "####---test\n",
    "_gen = make_generator(umtm, train_indices, batch_size=230, augment_samples=True)\n",
    "\n",
    "sample = next(_gen)\n",
    "# print(len(sample))\n",
    "del _gen\n",
    " \n",
    "\n",
    "\n",
    "(emb, tok_f), (sm, subj), (sample_weight, subject_weight) = sample\n",
    " \n",
    "    \n",
    "print('semantic map shape is:', sm.shape)\n",
    "_crop = 1500\n",
    "plot_embedding(tok_f[0][:_crop], title=f'Tokens features') \n",
    "# plot_embedding(emb[:_crop],   title=f'Embedding {SAMPLE_DOC_ID}') \n",
    "plot_embedding(sm[0][:_crop],    title=f'Semantic map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DOC_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = make_xyw('5fe34f62b770574a005553be', stats_valid)\n",
    "\n",
    "(emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "plot_embedding(sm[:500],    title=f'Semantic map ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146323f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "zrKMJ2bsn6yx",
    "outputId": "f39d06b8-8af2-4570-fa72-481633dfe61c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 96\n",
    "EMB =  1024\n",
    " \n",
    "_SELFTEST = True\n",
    "\n",
    "\n",
    "print('train_indices[0]:', train_indices[0])\n",
    "print('test_indices[0]:', test_indices[0])\n",
    "\n",
    "\n",
    "def plot_subject_distr(df, title):  \n",
    "  target='subject'\n",
    "  plt.figure(figsize=(16,4))   \n",
    "  sns.set(style=\"whitegrid\")\n",
    "  chart = sns.countplot(data=df, y=target)\n",
    "  plt.title(f'Frequency Distribution of subjects :{title}')\n",
    "\n",
    " \n",
    "plot_subject_distr(stats_valid, 'ALL')\n",
    "plot_subject_distr(stats_valid[stats_valid.index.isin(train_indices)], 'train')\n",
    "plot_subject_distr(stats_valid[stats_valid.index.isin(test_indices)], 'test')\n",
    "\n",
    "\n",
    "if _SELFTEST:\n",
    "  # test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True)\n",
    "  \n",
    "  x, y, w = next(train_gen)\n",
    "  \n",
    "#   print('X:', len(x), 'X[0]=', x[0].shape, 'X[1]=', x[1].shape)\n",
    "#   print('Y:', len(y), 'Y[0]=', y[0].shape, 'Y[1]=', y[1].shape)\n",
    "  \n",
    "\n",
    "#   plot_embedding(x[0][0], 'X2: Token Embeddings')\n",
    "#   plot_embedding(x[1][0], 'X1: Token Features')\n",
    "#   plot_embedding(y[0][0], 'Y: Semantic Map')\n",
    "  \n",
    "#   print(y[0][1])\n",
    "\n",
    "#   del x\n",
    "#   del w\n",
    "#   del y\n",
    "#   del train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf76fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "RIRaSgxCP3Jt",
    "outputId": "4f253294-c07c-481f-ca0f-5cbea0d31d97",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ctx = KerasTrainingContext(checkpoints_path=umtm.reports_dir, session_index=1)\n",
    "\n",
    "ctx.set_batch_size_and_trainset_size(BATCH_SIZE, \n",
    "                                     len(test_indices), \n",
    "                                     4 * len(train_indices))\n",
    "\n",
    "DEFAULT_TRAIN_CTX = ctx\n",
    "CLASSES = 43\n",
    "FEATURES = 14\n",
    "\n",
    "metrics = ['kullback_leibler_divergence', 'mse', 'binary_crossentropy']\n",
    "\n",
    "\n",
    "def train(umodel):\n",
    "  test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "  ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen)\n",
    "\n",
    "def overtrain(umodel):\n",
    "  test_gen = make_generator(umtm, list(train_indices) + list(test_indices), BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, list(train_indices) + list(test_indices), BATCH_SIZE, augment_samples=True) \n",
    "  ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152baa78",
   "metadata": {
    "colab_type": "text",
    "id": "gAFmo0sG4H9k",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models ü¶ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c66ec3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights_filename(model_factory_fn):\n",
    "    weights = ctx.model_checkpoint_path / f'{model_factory_fn.__name__}.h5'\n",
    "    print(weights.is_file(), weights)\n",
    "    if not weights.is_file():\n",
    "        weights = Path(analyser.hyperparams.models_path) / f'{model_factory_fn.__name__}.h5'\n",
    "        print(weights.is_file(), weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# get_weights_filename(uber_detection_model_005_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e026a10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train from 0 uber_detection_model_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedbd36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL_3:\n",
    "    from tf_support.super_contract_model import uber_detection_model_003\n",
    "    \n",
    "    model_factory_fn = uber_detection_model_003\n",
    "    umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, load_weights=False )\n",
    "\n",
    "    umodel.summary()\n",
    "\n",
    "    if TRAIN:\n",
    "      test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "      train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "\n",
    "      ctx.EPOCHS = 10\n",
    "      ctx.EVALUATE_ONLY = False\n",
    "      ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915537bf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Continue training 003 from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70b5a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyser.hyperparams.models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ea35b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL_3:\n",
    "    \n",
    "    model_factory_fn = uber_detection_model_003\n",
    "\n",
    "    weights = get_weights_filename(model_factory_fn)\n",
    "    umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, weights=weights)\n",
    "\n",
    "    if TRAIN:\n",
    "      test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "      train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "\n",
    "      ctx.EPOCHS = 25\n",
    "      ctx.EVALUATE_ONLY = False\n",
    "      ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d5177",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overtrain 003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3a081",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL_3:\n",
    "    if TRAIN:\n",
    "      ctx.unfreezeModel(umodel)\n",
    "      umodel.compile(loss=super_contract_model.losses, optimizer='Adam', metrics=super_contract_model.metrics)\n",
    "      print(super_contract_model.losses)\n",
    "      umodel.summary()\n",
    "\n",
    "      ctx.EPOCHS = 12\n",
    "      ctx.EVALUATE_ONLY = False\n",
    "\n",
    "      test_gen = make_generator(umtm, train_indices, BATCH_SIZE)\n",
    "      train_gen = make_generator(umtm, train_indices + test_indices, BATCH_SIZE, augment_samples=True) \n",
    "\n",
    "      ctx.train_and_evaluate_model(umodel, train_gen, test_generator=test_gen, retrain=False, lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b4aa3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 005 model\n",
    "- 0.0233: last val subject loss\n",
    "- 0.0016: last tagging loss\n",
    "- 0.0248: last val loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485527d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL_3:\n",
    "    model_factory_fn = uber_detection_model_005_1_1\n",
    "    umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, load_weights=False )\n",
    "\n",
    "    umodel.summary()\n",
    "\n",
    "    if TRAIN:\n",
    "      test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "      train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "\n",
    "      ctx.EPOCHS = 20\n",
    "      ctx.EVALUATE_ONLY = False\n",
    "      ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen, retrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1fd7d",
   "metadata": {
    "colab_type": "text",
    "id": "eF7Ktdfo9aa7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.1.1 üíïüíï uber_detection_model_005_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83159e85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import FEATURES \n",
    "from analyser.headers_detector import TOKEN_FEATURES\n",
    "\n",
    "from tf_support.super_contract_model import sigmoid_focal_crossentropy, losses\n",
    "\n",
    "import numpy as np\n",
    "from pandas import DataFrame \n",
    "\n",
    "\n",
    "\n",
    "model_factory_fn = uber_detection_model_005_1_1\n",
    "weights = get_weights_filename(model_factory_fn)\n",
    "umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, weights=weights)\n",
    "# umodel = ctx.init_model(model_factory_fn, trained=True, trainable=True, load_weights=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833bc89",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpgUW39m9abE",
    "outputId": "6febaa3a-bf87-4af6-d415-7db086f454a2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Phase I retraining\n",
    "# ‚ùÑÔ∏è frozen bottom layers\n",
    "######################\n",
    "if False:\n",
    "    if TRAIN:\n",
    "      test_gen = make_generator(umtm, test_indices, BATCH_SIZE)\n",
    "      train_gen = make_generator(umtm, train_indices, BATCH_SIZE, augment_samples=True) \n",
    "\n",
    "      ctx.EPOCHS = 10\n",
    "      ctx.EVALUATE_ONLY = False\n",
    "      ctx.train_and_evaluate_model(umodel, generator=train_gen, test_generator=test_gen, retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076aeab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Phase II finetuning\n",
    "all unfrozen, entire trainset, low LR\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b4f2d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJAH6Fjn9abL",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "## Phase II finetuning\n",
    "#  all unfrozen, entire trainset, low LR\n",
    "######################\n",
    "if TRAIN and False:\n",
    "  ctx.unfreezeModel(umodel)\n",
    "  umodel.compile(loss=super_contract_model.losses, optimizer='Adam', metrics=super_contract_model.metrics)\n",
    "  umodel.summary()\n",
    "  \n",
    "  ctx.EPOCHS = 30\n",
    "  ctx.EVALUATE_ONLY = False\n",
    "  ctx.EPOCHS *= 2\n",
    "\n",
    "  test_gen = make_generator(umtm, train_indices + test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices + test_indices, BATCH_SIZE, augment_samples=True) \n",
    "  \n",
    "  ctx.train_and_evaluate_model(umodel, train_gen, test_generator=test_gen, retrain=True, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0758a8",
   "metadata": {},
   "source": [
    "## ü•∞ Att model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a290e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from analyser.headers_detector import TOKEN_FEATURES\n",
    "from tf_support.super_contract_model import EMB, CLASSES, DEFAULT_TRAIN_CTX, FEATURES\n",
    "from tf_support.tools import KerasTrainingContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "  # MAX_LEN = 256\n",
    "  # BATCH_SIZE = 32\n",
    "  LR = 0.001\n",
    "\n",
    "  EMBED_DIM = EMB\n",
    "  NUM_HEAD = 4  # used in bert model\n",
    "  FF_DIM = 128  # used in bert model\n",
    "  NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "def bert_module(query, key, value, i, height):\n",
    "  # Multi headed self-attention\n",
    "  attention_output = layers.MultiHeadAttention(\n",
    "    num_heads=config.NUM_HEAD,\n",
    "    key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "    name=\"encoder_{}/multiheadattention\".format(i),\n",
    "  )(query, key, value)\n",
    "  attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "    attention_output\n",
    "  )\n",
    "  attention_output = layers.LayerNormalization(\n",
    "    epsilon=1e-6, name=f\"encoder_{i}/att_layernormalization\"\n",
    "  )(query + attention_output)\n",
    "\n",
    "  # Feed-forward layer\n",
    "  ffn = keras.Sequential(\n",
    "    [\n",
    "      layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "      layers.Dense(height),\n",
    "    ],\n",
    "    name=f\"encoder_{i}/ffn\",\n",
    "  )\n",
    "  ffn_output = ffn(attention_output)\n",
    "  ffn_output = layers.Dropout(0.1, name=f\"encoder_{i}/ffn_dropout\")(\n",
    "    ffn_output\n",
    "  )\n",
    "  sequence_output = layers.LayerNormalization(\n",
    "    epsilon=1e-6, name=f\"encoder_{i}/ffn_layernormalization\"\n",
    "  )(attention_output + ffn_output)\n",
    "  return sequence_output\n",
    "\n",
    "\n",
    "metrics = ['mse', 'binary_crossentropy']\n",
    "\n",
    "losses = {\n",
    "  \"O1_tagging\": \"binary_crossentropy\",\n",
    "  \"O2_subject\": \"binary_crossentropy\",\n",
    "}\n",
    "\n",
    "class ThresholdLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ThresholdLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name=\"threshold\", shape=(1,), initializer=\"uniform\",\n",
    "                                      trainable=True)\n",
    "        super(ThresholdLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return keras.backend.sigmoid(100*(x-self.kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def make_att_model(name='make_att_model', ctx: KerasTrainingContext = DEFAULT_TRAIN_CTX, trained=False):\n",
    "  input_text_emb = layers.Input(shape=[None, config.EMBED_DIM], dtype='float32', name=\"input_text_emb\")\n",
    "  _out = layers.BatchNormalization(name=\"bn1\")(input_text_emb)\n",
    "  _out = layers.Dropout(0.2, name=\"drops\")(_out)  # small_drops_of_poison\n",
    "\n",
    "  token_features = layers.Input(shape=[None, TOKEN_FEATURES], dtype='float32', name=\"token_features\")\n",
    "  token_features_n = layers.BatchNormalization(name=\"bn2\")(token_features)\n",
    "\n",
    "  _out = layers.concatenate([input_text_emb, token_features_n], axis=-1)\n",
    "\n",
    "  for i in range(config.NUM_LAYERS):\n",
    "    _out = bert_module(_out, _out, _out, i, height=config.EMBED_DIM + TOKEN_FEATURES)\n",
    "\n",
    "  _out = layers.BatchNormalization(name=\"bn1\")(_out)\n",
    "  _out = layers.LSTM(FEATURES, return_sequences=True, activation='tanh', name='O1_tagging_tanh')(_out)\n",
    "#   _out1 = layers.ReLU(name='O1_tagging')(_out)\n",
    "  _out1 = ThresholdLayer(name='O1_tagging')(_out)\n",
    "  \n",
    "\n",
    "  #   _out = Conv1D(filters=FEATURES * 4, kernel_size=(2), padding='same', activation='relu' , name='embedding_reduced')(_out)\n",
    "  _out = layers.Bidirectional(layers.LSTM(16, return_sequences=False, name='narcissisism'), name='embedding_reduced')(\n",
    "    _out)\n",
    "  _out = layers.BatchNormalization(name=\"bn_bi_2\")(_out)\n",
    "  _out = layers.Dropout(0.1)(_out)\n",
    "\n",
    "  _out2 = layers.Dense(CLASSES, activation='softmax', name='O2_subject')(_out)\n",
    "\n",
    "  base_model_inputs = [input_text_emb, token_features]\n",
    "  model = Model(inputs=base_model_inputs, outputs=[_out1, _out2], name=name)\n",
    "  model.compile(loss=losses, optimizer='Adam', metrics=metrics)\n",
    "  return model\n",
    "\n",
    "\n",
    "umodel = make_att_model() \n",
    "\n",
    "model_factory_fn=make_att_model      \n",
    "weights = get_weights_filename(model_factory_fn)\n",
    "# model_name = make_att_model.__name__\n",
    "\n",
    "\n",
    "umodel.load_weights(weights, by_name=True, skip_mismatch=True)\n",
    "\n",
    "# ctx.init_model(make_att_model, trained=True, trainable=True, weights=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a72eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN:\n",
    "#   ctx.unfreezeModel(umodel)\n",
    "  umodel.summary()\n",
    "  \n",
    "  ctx.EPOCHS = 100\n",
    "  ctx.EVALUATE_ONLY = False\n",
    "#   ctx.EPOCHS *= 2\n",
    "\n",
    "  test_gen = make_generator(umtm, train_indices + test_indices, BATCH_SIZE)\n",
    "  train_gen = make_generator(umtm, train_indices + test_indices, BATCH_SIZE, augment_samples=True) \n",
    "  \n",
    "  ctx.train_and_evaluate_model(umodel, train_gen, test_generator=test_gen, retrain=True, lr=config.LR)\n",
    "\n",
    "threshold = umodel.get_layer('O1_tagging').get_weights()\n",
    "print('threshold=',threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c22e6e",
   "metadata": {
    "colab_type": "text",
    "id": "JUum89Tdhg-9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c2826",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_checkpoint(ctx, model_factory_fn):\n",
    "    \n",
    "#     model_name = model_factory_fn.__name__    \n",
    "#     model = ctx.init_model(model_fn, trained=True, trainable=False, weights=ctx.model_checkpoint_path / f'{model_factory_fn.__name__}.weights')\n",
    "#     model.summary()    \n",
    "\n",
    "\n",
    "#     ch_fn = os.path.join(ctx.model_checkpoint_path, f\"{model_name}-{keras.__version__}.h5\")\n",
    "\n",
    "#     if not os.path.isfile(ch_fn):\n",
    "#       model.save_weights(ch_fn)\n",
    "#       print(f\"model weights saved to {ch_fn}\")\n",
    "\n",
    "#     else:\n",
    "#       print(f\"model weights NOT saved, because file exists {ch_fn}\")\n",
    "\n",
    "# save_checkpoint(ctx, uber_detection_model_005_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3e2ac",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NJ9uhDBaJlO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# /Users/artem/work/nemo/work/uber_detection_model_005_1_1.h5\n",
    "\n",
    "#######################################\n",
    "#######################################\n",
    "model_fn = make_att_model\n",
    "# model_fn = uber_detection_model_003\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "umodel = ctx.init_model(model_fn, trained=True, trainable=False, weights=ctx.model_checkpoint_path / f'{model_fn.__name__}.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: remove next 2 lines\n",
    "ctx.trained_models[umodel.name] = umodel.name\n",
    "models = ctx.trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6948a18",
   "metadata": {
    "colab_type": "text",
    "id": "NLRR4qmKImgQ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb535fbb",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me1LdIP5Ik9z",
    "outputId": "1bf81b8f-1f09-4760-a0f3-868a523652d9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_compare_models(\n",
    "    models: [str],\n",
    "    metrics, \n",
    "    title=\"metric/epoch\",\n",
    "    image_save_path = umtm.reports_dir):\n",
    "    \n",
    "  _metrics = [m for m in metrics if not m.startswith('val_')]\n",
    "\n",
    "  for i, m in enumerate(models):\n",
    "\n",
    "    data: pd.DataFrame = ctx.get_log(m)\n",
    "\n",
    "    if data is not None:\n",
    "      data.set_index('epoch')\n",
    "\n",
    "      for metric in _metrics:\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.grid(True)\n",
    "        plt.title(f'{metric} [{m}]')\n",
    "        for metric_variant in ['', 'val_']:\n",
    "          key = metric_variant + metric\n",
    "          if key in data:\n",
    "\n",
    "            x = data['epoch'][-100:]\n",
    "            y = data[key][-100:]\n",
    "\n",
    "\n",
    "            c = 'red'  # plt.cm.jet_r(i * colorstep)\n",
    "            if metric_variant == '':\n",
    "              c = 'blue'\n",
    "            plt.plot(x, y, label=f'{key}', alpha=0.2, color=c)\n",
    "\n",
    "            y = y.rolling(4, win_type='gaussian').mean(std=4)\n",
    "            plt.plot(x, y, label=f'{key} SMOOTH', color=c)\n",
    "\n",
    "            plt.legend(loc='upper right')\n",
    "\n",
    "        \n",
    "        plt.title(f'{[m]} {title}')\n",
    "        plt.grid(True)\n",
    "        img_path = os.path.join(image_save_path, f'{m}-{metric}.png')\n",
    "        \n",
    "        plt.savefig(img_path, bbox_inches='tight')        \n",
    "        plt.show()\n",
    "    else:\n",
    "      logger.error('cannot plot')\n",
    "    \n",
    "\n",
    "models = list(ctx.trained_models.keys())\n",
    "\n",
    "\n",
    "plot_compare_models(models, ['loss'], 'Loss')\n",
    "\n",
    "# plot_compare_models(models, ['O1_tagging_kullback_leibler_divergence'], 'TAGS: Kullback Leibler divergence')\n",
    "# plot_compare_models(models, ['O1_tagging_mse'], 'TAGS: MSE')\n",
    "# plot_compare_models(models, ['O2_subject_kullback_leibler_divergence'], 'Subj: Kullback Leibler divergence')\n",
    "# plot_compare_models(models, ['O2_subject_mse'],  'Subjects: MSE')\n",
    "\n",
    "plot_compare_models(models, ['O1_tagging_loss', 'O2_subject_loss'], 'Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404112e",
   "metadata": {
    "colab_type": "text",
    "id": "v1p8Mqpi32Bf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Contract subj Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e5af7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# umtm.stats\n",
    "# semantic_map_keys_contract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fc554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_support.super_contract_model import make_xyw\n",
    "\n",
    "sample_index = umtm.stats [umtm.stats['value']>0].index[2]\n",
    "print(sample_index)\n",
    "\n",
    "x, y, _ = make_xyw(sample_index, umtm.stats)\n",
    "\n",
    "prediction = umodel.predict(x=[np.expand_dims(x[0], axis=0), np.expand_dims(x[1], axis=0)], batch_size=1)\n",
    "\n",
    "tagsmap = pd.DataFrame(prediction[0][0], columns=semantic_map_keys_contract)\n",
    "# .T\n",
    "plot_embedding(tagsmap[:500], f'Predicted Semantic Map {tagsmap.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e598f9f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analyser.contract_parser import nn_get_tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb73e96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "agent_tags = ['org-1-name',\n",
    "              'org-1-type',\n",
    "              'org-1-alias',\n",
    "              'org-2-name',\n",
    "              'org-2-type',\n",
    "              'org-2-alias']\n",
    "solo_tags = [\n",
    "  'date',\n",
    "  'number',\n",
    "  'sign_value_currency/value',\n",
    "  'sign_value_currency/currency',\n",
    "  'sign_value_currency/sign'\n",
    "]\n",
    "\n",
    "# seq_labels_contract[-3:]\n",
    "\n",
    "tagnames = solo_tags + agent_tags\n",
    "\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from analyser.contract_agents import ContractAgent, normalize_contract_agent\n",
    "from analyser.documents import TextMap\n",
    "from analyser.ml_tools import SemanticTag\n",
    "from analyser.persistence import DbJsonDoc\n",
    "from analyser.text_tools import find_top_spans\n",
    "# from tf_support.super_contract_model import seq_labels_contract\n",
    "from tf_support.tf_subject_model import decode_subj_prediction\n",
    "\n",
    "from analyser.contract_parser import nn_find_org_names, nn_get_tag_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_tags_from_predicted_semantic_map(_id: str, tagsmap: DataFrame):\n",
    "  jdoc = get_doc(_id)\n",
    "  _map = jdoc.get_tokens_map_unchaged()\n",
    "\n",
    "  results = {}\n",
    "  for key in tagnames:\n",
    "    t = nn_get_tag_value(key, _map, tagsmap )\n",
    "    results[key] = t\n",
    "    # print(t)\n",
    "\n",
    "#   ca = ContractAgent()\n",
    "#   ca.name =  results['org-1-name'] #TODO: check for NONE\n",
    "#   ca.type =  results['org-1-type']\n",
    "#   ca.alias = results['org-1-alias']\n",
    "   \n",
    "\n",
    "#   ca2 = ContractAgent()\n",
    "#   ca2.name =  results['org-2-name'] #TODO: check for NONE\n",
    "#   ca2.type =  results['org-2-type']\n",
    "#   ca2.alias = results['org-2-alias']\n",
    "#   try:\n",
    "#     normalize_contract_agent(ca)\n",
    "#     normalize_contract_agent(ca2)\n",
    "#   except Exception as e:\n",
    "#         # TODO:\n",
    "#     logger.error(f'{_id} {e}')\n",
    "\n",
    "  if results['number'] is not None:\n",
    "    results['number'].value = results['number'].value.strip().lstrip('‚Ññ').lstrip('N ').lstrip()\n",
    "\n",
    "  return results, jdoc\n",
    "\n",
    "\n",
    "def put_results_into_df(id_, results, df, jdoc: DbJsonDoc):\n",
    "  org_atribs = ['name', 'alias', 'type']\n",
    "\n",
    "  def v(x):\n",
    "    if results[x] is not None:\n",
    "      return results[x].value\n",
    "\n",
    "  def swap(a, b):\n",
    "    ab = [a, b]\n",
    "    try:\n",
    "      ab = sorted(ab)\n",
    "    except:\n",
    "      pass\n",
    "    return ab\n",
    "\n",
    "  def s(a, b):\n",
    "    ab = swap(v(a), v(b))\n",
    "    df.at[id_, f'p-{a}'] = ab[0]\n",
    "    df.at[id_, f'p-{b}'] = ab[1]\n",
    "    return ab\n",
    "\n",
    "  for key in org_atribs:\n",
    "    arr = s(f'org-1-{key}', f'org-2-{key}')\n",
    "\n",
    "  def p(key):\n",
    "    df.at[id_, f'p-{key}'] = v(key)\n",
    "\n",
    "  p('sign_value_currency/value')\n",
    "  p('sign_value_currency/currency')\n",
    "  p('sign_value_currency/sign')\n",
    "\n",
    "  p('date')\n",
    "  p('number')\n",
    "\n",
    "  # get_expected values\n",
    "  for key in solo_tags:\n",
    "    t = jdoc.get_attribute_value(key)\n",
    "    df.at[id_, f'{key}'] = t\n",
    "\n",
    "  for key in org_atribs:\n",
    "    orgs = swap(jdoc.get_attribute_value(f'org-1-{key}'), jdoc.get_attribute_value(f'org-2-{key}'))\n",
    "    df.at[id_, f'org-1-{key}'] = orgs[0]\n",
    "    df.at[id_, f'org-2-{key}'] = orgs[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e8266",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_subj_predictions(umodel, indices):\n",
    "  ev = umtm.stats.copy()\n",
    "  tags = pd.DataFrame()\n",
    "  for t in tagnames:\n",
    "    tags['p-' + t] = ''\n",
    "    tags[t] = ''\n",
    "\n",
    "  errors_report = pd.DataFrame()\n",
    "  errors_report['expected'] = ''\n",
    "  errors_report['predicted'] = ''\n",
    "\n",
    "  for i, _id in enumerate(indices):\n",
    "    logger.debug(f'validating {_id} {i} of {len(indices)}')\n",
    "    \n",
    "    x, y, _ = make_xyw(_id, umtm.stats)\n",
    "\n",
    "    prediction = umodel.predict(x=[np.expand_dims(x[0], axis=0), np.expand_dims(x[1], axis=0)], batch_size=1)\n",
    "\n",
    "    tagsmap = pd.DataFrame(prediction[0][0], columns=seq_labels_contract)\n",
    "  \n",
    "    r, jdoc = fetch_tags_from_predicted_semantic_map(_id, tagsmap)\n",
    "    put_results_into_df(_id, r, tags, jdoc)\n",
    "    \n",
    "\n",
    "    subj_1hot = prediction[1][0]\n",
    "\n",
    "    expected = decode_subj_prediction(y[1])[0]\n",
    "    predicted = decode_subj_prediction(subj_1hot)[0]\n",
    "\n",
    "    ev.at[_id, 'expected_subj'] = expected.name\n",
    "    ev.at[_id, 'predicted_subj'] = predicted.name\n",
    "\n",
    "    ev.at[_id, 'wrong'] = False\n",
    "    if expected != predicted:\n",
    "      ev.at[_id, 'wrong'] = True\n",
    "\n",
    "  return ev, tags\n",
    "\n",
    "ev, tags = make_subj_predictions(umodel, [sample_index])\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25748e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJLgtkpo-NUY",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def fetch_tag_value(tagname: str, textmap: TextMap, tagging: DataFrame, threshold=0.3) -> SemanticTag or None:\n",
    "#   att = tagging[tagname].values\n",
    "#   slices = find_top_spans(att, threshold=threshold, limit=1) #TODO: estimate per-tag thresholds\n",
    "  \n",
    "#   if len(slices) > 0:\n",
    "#     span = slices[0].start, slices[0].stop\n",
    "#     value = textmap.text_range(span)\n",
    "#     tag = SemanticTag(tagname, value, span)\n",
    "#     tag.confidence = att[slices[0]].mean()\n",
    "#     return tag\n",
    "#   return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subset = umtm.stats #umtm.stats[~pd.isna(umtm.stats['user_correction_date'])].sort_values('analyze_date')\n",
    "_indices =  subset.index\n",
    "ev, tags = make_subj_predictions(umodel, _indices)\n",
    "tags.to_csv('all_contracts_predicstions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef7d76",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edc196",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subset = umtm.stats[~pd.isna(umtm.stats['user_correction_date'])]\n",
    "# pd.isna(umtm.stats['user_correction_date'])\n",
    "ev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92527cf1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh1WFsjprUr1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _cols = [  'wrong' ]\n",
    "# _tmp = ev[cols]\n",
    "# errors_report = _tmp[ _tmp.wrong == True] #.sort_values('subject')\n",
    "# print(len(errors_report), 'wrong subjects of', len(tags))\n",
    "# errors_report \n",
    "\n",
    "ev['predicted_subj']\n",
    "subj_df = ev[['predicted_subj', 'expected_subj']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36f953",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995aef14",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFh8SxP9J081",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_report(umodel, ev):\n",
    "  plot_cm(ev['expected_subj'], ev['predicted_subj'])\n",
    "  \n",
    "  img_path = os.path.join(umtm.work_dir, f'subjects-confusion-matrix-{umodel.name}.png')\n",
    "  plt.savefig(img_path, bbox_inches='tight')\n",
    "\n",
    "  report = classification_report(ev['expected_subj'], ev['predicted_subj'], digits=3)\n",
    "  print(umodel.name)\n",
    "  print(report)\n",
    "  \n",
    "  with open(os.path.join(umtm.work_dir, f'subjects-classification_report-{umodel.name}.txt'), \"w\") as text_file:\n",
    "    text_file.write(report)\n",
    "\n",
    "\n",
    "subj_df = ev[['predicted_subj', 'expected_subj']].copy() #ev[~pd.isna(ev['predicted_subj'])]\n",
    "make_report(umodel, subj_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d0864",
   "metadata": {
    "colab_type": "text",
    "id": "mX5RUserhy0m",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate tags detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb9c95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags.fillna('-', inplace=True)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947199d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_csv(d, f):\n",
    "    fn = os.path.join(umtm.work_dir, f)\n",
    "    d.to_csv(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3d68f",
   "metadata": {
    "colab_type": "text",
    "id": "_SUQNtsxVBp9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Contract number validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e51734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "fGnSN-Bp87VW",
    "outputId": "0a23b5c6-91a3-44f7-dd1c-60f567a8f326",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_numbers = tags [ tags['number'] != tags['p-number']].sort_values('number')\n",
    "print( f'Contract numbers: {len(wrong_numbers)} of {len(tags)}  ({100. * len(wrong_numbers) / len(tags) :0.1f}%) were detected wronggly')\n",
    "\n",
    "save_csv( wrong_numbers[['p-number', 'number']], 'wrong_numbers.csv')\n",
    "\n",
    "# wrong_numbers[['p-number', 'number']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1f95b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags ['sign_value_currency/currency'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ed7d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv(x):\n",
    "    if type(x) is str:\n",
    "        v = x.replace(',','.').replace(' ','')\n",
    "    else: \n",
    "        v=x\n",
    "    try:\n",
    "        v=float(v)\n",
    "    except:\n",
    "        v=np.nan\n",
    "    return v \n",
    "\n",
    "tags['n-p-sign_value_currency/value'] = pd.to_numeric( tags['p-sign_value_currency/value'].apply(conv) )\n",
    "tags['n-sign_value_currency/value']   = pd.to_numeric( tags['sign_value_currency/value'].apply(conv) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc99c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_values = tags [  tags['n-p-sign_value_currency/value']  != tags['n-sign_value_currency/value']]\n",
    "cols = ['n-p-sign_value_currency/value', 'n-sign_value_currency/value']\n",
    "wrong_values = wrong_values[cols]\n",
    "\n",
    "wrong_values ['val_err'] = \\\n",
    "    np.log1p( np.abs(wrong_values['n-p-sign_value_currency/value'] - wrong_values['n-sign_value_currency/value']))\n",
    "wrong_values = wrong_values.sort_values('val_err', ascending=False)\n",
    "\n",
    "print(len(wrong_values))\n",
    "wrong_values.tail(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62552ad",
   "metadata": {
    "colab_type": "text",
    "id": "U8qtdCl3Zdt7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Contract Org-1 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff2ad2",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCUtMUEZZduA",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wrong_orgs1 = tags [ (tags['org-1-name'] != tags['p-org-1-name']) | (tags['org-2-name'] != tags['p-org-2-name']) ]\n",
    "print( f'Org-1 name: {len(wrong_orgs1)} of {len(tags)}  ({100. * len(wrong_orgs1) / len(tags):0.1f}%) were detected incorrectly')\n",
    "\n",
    "cols=['p-org-1-name', 'org-1-name', 'p-org-2-name', 'org-2-name']\n",
    "save_csv( wrong_orgs1[cols], 'wrong_orgs1.csv')\n",
    "\n",
    "wrong_orgs1[cols].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cfe16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "id": "HZrAdEP0iwjK",
    "outputId": "73b4e39b-6313-47a8-854e-7f4af1c76ee6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_aliases = tags [ (tags['org-1-alias'] != tags['p-org-1-alias']) | (tags['org-2-alias'] != tags['p-org-2-alias']) ]\n",
    "print( f'Aliases: {len(wrong_aliases)} of {len(tags)}  ({100. * len(wrong_aliases) / len(tags) : 0.1f}%) were detected incorrectly')\n",
    "\n",
    "cols=['p-org-1-alias', 'org-1-alias', 'p-org-2-alias', 'org-2-alias']\n",
    "save_csv( wrong_aliases[cols], 'wrong_aliases.csv')\n",
    "# wrong_aliases[cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c1bc6",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6j8omO8Sk2Nv",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrong_types = tags [ (tags['org-1-type'] != tags['p-org-1-type']) | (tags['org-2-type'] != tags['p-org-2-type'])]\n",
    "print( f'Types: {len(wrong_types)} of {len(tags)}  ({100. * len(wrong_types) / len(tags) : 0.1f}%) were detected incorrectly')\n",
    "cols=['p-org-1-type', 'p-org-2-type', 'org-1-type', 'org-2-type']\n",
    "save_csv( wrong_types[cols], 'wrong_types.csv')\n",
    "wrong_types[cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfbf88",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLXWndqih-oE",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "arrays = [ wrong_orgs1, wrong_types, wrong_numbers, wrong_aliases]\n",
    "counter = Counter()\n",
    "for a in arrays:\n",
    "  for i in a.index:\n",
    "   counter[i]+=1\n",
    " \n",
    "\n",
    "print('–°–∞–º—ã–π —Å–ª–æ–∂–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: ', counter.most_common()[0][0])\n",
    "print(\"–í—Å–µ–≥–æ –Ω–µ–¥–æ—á–µ—Ç–æ–≤:\", len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d507ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "colab_type": "code",
    "id": "uvG2pH1rt6G_",
    "outputId": "5b9173d2-c781-49bf-cccb-be2b481edee4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "umtm.stats['errors'] = 0\n",
    "for c in counter:\n",
    "  umtm.stats.at[c, 'errors'] = counter[c]\n",
    "\n",
    "\n",
    "calculate_samples_weights(umtm)\n",
    "umtm._save_stats()\n",
    "umtm.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218860e",
   "metadata": {
    "colab_type": "text",
    "id": "_CR9W8NDH8B-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Single doc eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fec69",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFzr4k-cpOzX",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !wget https://raw.githubusercontent.com/nemoware/analyser/uber-models/tests/contract_db_1.json\n",
    "\n",
    "  with open('contract_db_1.json', 'rb') as handle:    \n",
    "    jdata = json.load(handle, object_hook=json_util.object_hook)\n",
    "\n",
    "  jdoc = DbJsonDoc(jdata)\n",
    "\n",
    "else:\n",
    "  from integration.db import get_mongodb_connection\n",
    "  from bson.objectid import ObjectId\n",
    "\n",
    "  def get_doc(objid):\n",
    "    logger.debug(f'fetching {objid}')\n",
    "    db = get_mongodb_connection()\n",
    "    documents_collection = db['documents']\n",
    "    jdata =  documents_collection.find_one({'_id': ObjectId(objid)})\n",
    "    return DbJsonDoc(jdata)\n",
    "\n",
    "  SAMPLE_DOC_ID = counter.most_common()[0][0] #umtm.stats.index[10]\n",
    "    \n",
    "    \n",
    "    \n",
    "  SAMPLE_DOC_ID = '5eea27adc28b75807f3dae66'\n",
    "  print('SAMPLE_DOC_ID:', SAMPLE_DOC_ID)\n",
    "  dp = umtm.make_xyw(SAMPLE_DOC_ID)\n",
    "  (emb, tok_f), (sm, subj), (sample_weight, subject_weight) = dp\n",
    "\n",
    "  jdoc = get_doc(SAMPLE_DOC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf0186",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfHQOFhw3yuO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from analyser.legal_docs import embedd_tokens\n",
    "\n",
    "if IN_COLAB:\n",
    "  embedder = ElmoEmbedder.get_instance('elmo')  # lazy init\n",
    "  emb = embedd_tokens(jdoc.get_tokens_for_embedding(),\n",
    "                             embedder,\n",
    "                             verbosity=2,\n",
    "                             log_key='tmp')\n",
    "\n",
    "  tok_f = get_tokens_features(jdoc.get_tokens_map_unchaged().tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e96d6",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yF9kgac_hZr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############\n",
    "prediction = umodel.predict(   x=[  np.expand_dims(emb, axis=0), np.expand_dims(tok_f, axis=0)] , batch_size=1)\n",
    "##############\n",
    "print(len(prediction), umodel.name)\n",
    "subj_1hot = prediction[1][0]\n",
    "print('Subject:', decode_subj_prediction(subj_1hot))\n",
    "\n",
    "\n",
    "tagging = pd.DataFrame( prediction[0][0], columns=seq_labels_contract)\n",
    "plot_embedding(tagging, title = f'Predictions of {umodel.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa8a57",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBRtwOj5QvNf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render_slices(slices, tokens, attention_v, ht='') -> str:\n",
    "  ht += '<ol>'\n",
    "  for _s in slices:\n",
    "    ht += '<li>'\n",
    "    t = tokens[_s]\n",
    "    l = attention_v[_s]\n",
    "    ht += to_color_text(t, l, _range=(0, 1.2))\n",
    "    ht += '<br><hr>'\n",
    "    ht += '</li>'\n",
    "  ht += '</ol>'\n",
    "\n",
    "  return ht\n",
    "\n",
    "for t in seq_labels_contract:\n",
    "  spans = list( find_top_spans( tagging[t].values, threshold=0.3))  \n",
    "  display(HTML(render_slices(spans, jdoc.get_tokens_map_unchaged().tokens, tagging[t].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1896e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# mean_ = tagging.values.max(-1)*0.5\n",
    "# print (mean_.shape)\n",
    "# display(HTML( to_color_text (jdoc.get_tokens_map_unchaged().tokens[:24000],  mean_[:24000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9183707",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = '5edbadd7da3678279fbcaabf\n",
    "5edbc660da3678279fbcaeac\n",
    "5edbc668da3678279fbcaf6e\n",
    "5edbc65dda3678279fbcae56\n",
    "5edbc66bda3678279fbcafe6\n",
    "5edbc615da3678279fbcadc9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff550c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF-UGHyh-C9",
    "7X_zYCYEdlPM",
    "lyI4hbTRFjyM"
   ],
   "name": "Local: structure keras uber model - clean train, evaluate, test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.612159,
   "end_time": "2023-01-26T10:09:40.188121",
   "environment_variables": {},
   "exception": null,
   "input_path": "trainsets/retrain_contract_uber_model.ipynb",
   "output_path": "trainsets/retrain_contract_uber_model.ipynb",
   "parameters": {},
   "start_time": "2023-01-26T10:09:20.575962",
   "version": "2.4.0"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
